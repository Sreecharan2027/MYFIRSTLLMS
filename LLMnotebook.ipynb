{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mamid\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mamid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mamid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mamid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "c:\\Users\\mamid\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: what is large language model\n",
      "Generated Answer:\n",
      "+ large language models, A single large language model is a jack of all trades (and also master of none). It can perform a wide range of tasks and is capable of emergent behavior such as in-context learning. Behavior of large language models: We will start at the outer layer where we only have blackbox API access to the model (as weâ€™ve had so far). Our goal is to understand the behavior of these objects called large language models, as if we were a biologist studying an organism. Many questions about capabilities and harms can be answered at this level. In this lecture, we will begin our exploration of the harms of large language models. In this course, we What is a language model?\n",
      "\n",
      "\n",
      "Query: what is llm\n",
      "Generated Answer:\n",
      "LLMs are upstream foundation models. * LLMs have the potential to cause harm in a variety of ways, including through performance In this lecture, we will focus on fairly concrete and lower-level concerns regarding the harms of LLMs. Examples of Performance Disparities in LLMs + Existing benchmarks for LLMs have been the subject of significant critiques (Blodgett et al., 2021)\n",
      "\n",
      "\n",
      "Query: what is pren tree bank\n",
      "Generated Answer:\n",
      "Penn Tree Bank The Penn Tree Bank is a classic dataset in NLP, originally annotated for syntactic parsing. Beginning Bown LLMs are upstream foundation models. word screeg is: We screeged the tree with our swords\n",
      "\n",
      "\n",
      "Query: \n",
      "Generated Answer:\n",
      ". . . . .\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import nltk\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "# Set environment variables to suppress warnings\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename='app.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Preprocess text\n",
    "def preprocess_text(text):\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token.lower() for token in tokens if token.isalpha()]\n",
    "    filtered_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "    \n",
    "    preprocessed_text = ' '.join(filtered_tokens)\n",
    "    return preprocessed_text\n",
    "\n",
    "# Vectorize text chunks\n",
    "def vectorize_text_chunks(text_chunks, model):\n",
    "    vectors = [model.encode(chunk, show_progress_bar=False) for chunk in text_chunks]\n",
    "    return np.array(vectors)\n",
    "\n",
    "# Vectorize query\n",
    "def vectorize_query(query, model):\n",
    "    return model.encode([query], show_progress_bar=False)\n",
    "\n",
    "# Generate answer with citations\n",
    "def generate_answer_with_citations(retrieved_chunks):\n",
    "    answer = \" \".join(retrieved_chunks)\n",
    "    return answer.strip()\n",
    "\n",
    "# Multi-class conversational agent\n",
    "class MultiClassConversationalAgent:\n",
    "    def __init__(self):\n",
    "        self.indices = {}\n",
    "        self.text_chunks = {}\n",
    "\n",
    "    def add_class_data(self, class_name, text_chunks, model):\n",
    "        vectors = vectorize_text_chunks(text_chunks, model)\n",
    "        dimension = vectors.shape[1]\n",
    "        index = faiss.IndexFlatL2(dimension)\n",
    "        index.add(vectors)\n",
    "        self.indices[class_name] = index\n",
    "        self.text_chunks[class_name] = text_chunks\n",
    "\n",
    "    def search_class(self, class_name, query_vector, k=5):\n",
    "        try:\n",
    "            index = self.indices[class_name]\n",
    "            distances, indices = index.search(query_vector, k)\n",
    "            return distances, indices\n",
    "        except KeyError as e:\n",
    "            logging.error(f\"Class {class_name} not found: {e}\")\n",
    "            return None, None\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in search_class: {e}\")\n",
    "            return None, None\n",
    "\n",
    "    def retrieve_text_chunks(self, class_name, indices):\n",
    "        try:\n",
    "            text_chunks = self.text_chunks[class_name]\n",
    "            retrieved_chunks = [text_chunks[idx] for idx in indices[0]]\n",
    "            return retrieved_chunks\n",
    "        except KeyError as e:\n",
    "            logging.error(f\"Class {class_name} not found: {e}\")\n",
    "            return []\n",
    "        except IndexError as e:\n",
    "            logging.error(f\"Index error: {e}\")\n",
    "            return []\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in retrieve_text_chunks: {e}\")\n",
    "            return []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load model\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "    # Read data from files\n",
    "    data_files = ['lecture_notes/intro.txt', 'lecture_notes/capabilities.txt', 'lecture_notes/Harms I.txt', 'lecture_notes/Harms II.txt']\n",
    "    text_chunks = []\n",
    "    for file_name in data_files:\n",
    "        with open(file_name, 'r', encoding='utf-8') as file:\n",
    "            text_chunks.extend([line.strip() for line in file.readlines() if line.strip()])\n",
    "\n",
    "    # Initialize the multi-class agent\n",
    "    multi_agent = MultiClassConversationalAgent()\n",
    "\n",
    "    # Add lecture data to the agent\n",
    "    multi_agent.add_class_data('CS324', text_chunks, model)\n",
    "\n",
    "    while True:\n",
    "        # Get user input\n",
    "        query = input(\"Enter your query (or type 'exit' to quit): \")\n",
    "        if query.lower() == 'exit':\n",
    "            break\n",
    "\n",
    "        query_vector = vectorize_query(query, model)\n",
    "        distances, indices = multi_agent.search_class('CS324', query_vector, k=5)\n",
    "        retrieved_chunks = multi_agent.retrieve_text_chunks('CS324', indices)\n",
    "        generated_answer = generate_answer_with_citations(retrieved_chunks)\n",
    "\n",
    "        print(f\"Query: {query}\")\n",
    "        print(\"Generated Answer:\")\n",
    "        print(generated_answer)\n",
    "        print(\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
