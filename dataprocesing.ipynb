{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file outlines a comprehensive approach for building a conversational agent that leverages Natural Language Processing (NLP) techniques and neural network models, specifically BERT (Bidirectional Encoder Representations from Transformers).\n",
    "\n",
    "THIS IS AN EXPERIMENTAL FILE.\n",
    "\n",
    "\n",
    "\n",
    "I HAVE MADE THIS  FILE FROM  MY UNDERSTANDING AND LEARNING ABOUT THE LLM's "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\mamid\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (3.8.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: transformers in c:\\users\\mamid\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (4.41.2)\n",
      "Requirement already satisfied: torch in c:\\users\\mamid\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: click in c:\\users\\mamid\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\mamid\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (1.4.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\mamid\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (2024.5.15)\n",
      "Requirement already satisfied: tqdm in c:\\users\\mamid\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\mamid\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (3.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in c:\\users\\mamid\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (0.23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\mamid\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mamid\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mamid\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\mamid\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\mamid\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\mamid\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\mamid\\appdata\\roaming\\python\\python39\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\mamid\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (1.12.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\mamid\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mamid\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\mamid\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (2024.5.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\mamid\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (2021.4.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\mamid\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\mamid\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.12.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\mamid\\appdata\\roaming\\python\\python39\\site-packages (from tqdm->nltk) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mamid\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mamid\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mamid\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mamid\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mamid\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in c:\\users\\mamid\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "pip install nltk transformers torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting Up NLTK Components\n",
    "Preprocessing Text\n",
    "Reading and Processing Lecture Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mamid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mamid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lecture: capabilities\n",
      "lectur capabl in thi lectur we will explor the capabl of gpt3 the canon larg languag model we will close follow the benchmark from the gpt3 paper which includ standard nlp benchmark eg question answer a well a quitki oneoff demo eg use a new word in a sentenc in comparison with the stateoftheartresult for each task the result are mix on some task such a languag model gpt3 exce the stateoftheart by a huge margin onoth where gpt3 is compet against system that are train with larg amount of label data it lag far behind the way to think about these result is a follow gpt3 wa not train on these task explicitli it wa just train a a languag model to predict the next word nonetheless even without tri gpt3 doe a passabl job on averag at a broad rang of nlp task becaus gpt3 wa not train on ani of these task it hasnt overfit which mean it ha a good chanc of do well at mani mani other task a seen by the passabl perform on oneoff task moreov if you want to do well on ani particular task eg question answer you should in principl be abl to adapt gpt3 use the larg amount of label data to exceed stateoftheart adapt recal that a languag model p is a distribut over sequenc of token r1 and thu can be use to score sequenc pthe mous ate the chees it can also be use to perform condit gener of a complet given a prompt the mous ate the chees a task is a map from input to output for exampl for question answer we might have input what school did burn hogarth establish output school of visual art we use the term adapt to refer to the process of take a languag model and ture it into a task model given anatur languag descript of the task and a set of train instanc inputoutput pair there are two primari way to perform adapt 4 train standard supervis learn train a new model that map input to output either by a creat a new model that use the languag model a featur probe or start with the languag model and updat it base on the train instanc finetun or someth in between lightweight finetun 2 prompt incontext learn construct a prompt a string base on the descript and train instanc or a set of prompt feed those into a languag model to obtain complet a zeroshot learn number of train exampl is 0 b oneshot learn number of train exampl is 4 cc fewshot leam number of train exampl is few which adapt procedur should we go with train can be challeng due to overfit just imagin finetun a 175 billion paramet model base on 5 exampl how to do thi effect will be the topic of the adapt lectur for now we will be content with adapt of gpt3 use prompt note that the limit of prompt is that we can onli leverag a onli small number of train instanc a mani a can fit into a prompt thi is due to a limit of transform where the prompt and the complet must fit into 2048 token the gpt3 paper evalu gpt3 on a larg set of task we will consid a subset of these and for each task discus the follow 1 definit what is the task and it motiv 2 adapt how do we reduc the task to languag model via prompt 3 result what are the quantit number compar to taskspecif stateoftheart model size and number of exampl matter by default the result will base on the full gpt3 model davinci which ha 175 billion paramet use incontext learn with a mani train instanc a you can stuff into the prompt along the way we will do ablat to see if model size and number of incontext train instanc matter spoiler it doe and more is better the task are group a follow 1 languag model question answer translat arithmet news articl gener one wd novel task the goal of thi lectur is to provid 1 an overview of task in nlp independ of larg languag model 2 a sen of how well gpt3 work and 3 a tast for the art of prompt engin languag model the most natur start point for think about what a languag model can do is to ask if it can do the thing that languag model are suppos to do model languag recal that a languag model p is a probabl distribut over sequenc of token suppos we take a corpu of text 11 for exampl the mous ate the chees we can ask what is the probabl the languag model assign to it pthe mous ate the chees recal that we can break down the the joint probabl into the product of the condit probabl for each token by the chain rule l pleut pl ea ia perplex the joint probabl of a sequenc depend on it length and thu goe to zero a the length grow which make it hard to track just think about tri to get a better estim of perplex on newswir by get more newswir intuit we want to averag the per token probabl p 1 we dont want to take the arithmet averag becaus assign a token probabl 0 is realli bad think about code your code length would be infinit but the arithmet averag doesnt penal you for that instead we want the geometr averag which is exactli what perplex doe 1 1 perplex 11 exp z dv 1 perplex can be interpret a the averag branch factor per token recal that log is the code length we are take the averag code length exponenti provid the number of possibl for intuit take uniform distribut a bitstr of length of 3 can encod 2 possibl string tale of two error there are two type of error a languag model can make and perplex treat them asymmetr recal error the languag model fail to place probabl mass on some token perplex ha no merci pate the mous 0 perplex the mous ate the chees 00 precis error the languag model place extra probabl mass on some bad sequenc perplex provid a slap on the wrist given a languag model p suppos we mix in some garbag distribut r with probabl 25 bai1 l p wi tai1 r i1 then we can comput the perplex of ar under q 1 perplex 1711 erplex 11 1 perplex 11 where the last approxim equal hold for small valu of e if we mix in 5 junk then perplex onli by 5 note that the result languag is horribl for gener sinc everi 20 token on averag it just go to gener a gibberish token now let get on with evalu perplex on an actual dataset penn tree bank the penn tree bank is a classic dataset in nlp origin annot for syntact par begin with emami and jelinek 2004 and mikolov and zweig 2012 a version of the dataset that onli contain wall street journal articl wa use a a languag model evalu note that the ptb languag model benchmark involv some signific preprocess of the origin dataset ht to john hewitt for point thi out adapt feed the entir text a a prompt into gpt3 and evalu the perplex demo pierr vinken 61 year old will join the board a a nonexecut director nov 29 mr vinken is chairman of elsevi nv the dutch publish group result gpt3 vastli outperform the exist stateoftheart model perplex gpt3 205 bertlargecas1 313 see the leaderboard for the latest result trainfest leakag the author did not evalu on some dataset such a wikitext103 becaus gpt3 wa train on wikipedia ptb had the advanc of predat the intemet and is onli avail through a paid licens thi is anoth complic with larg dataset it is difficult to check that your test data did not appear in your train data and wa memor lambada paperno et al 2016 task predict the last word of a sentenc motiv solv the task requir model longrang depend adapt lambada is nativ alreadi a languag model task so we could just ask a languag model to complet the final word of the sentenc problem languag model doesnt know it should be produc the final word of the sentenc solut frame it more explicitli a a inputoutput map and use incontext learn with addit exampl demo fill in blank alic wa friend with bob alic went to visit her friend bob she held the torch in front of her she caught her breath chri there a step what a step cut in the rock about fifi foot ahead she move faster they both move faster in fact she said rais the torch higher there more than a step result gpt3 doe much better on thi task than the previou stateoftheart base on gpt2 model perplex gpt3 fewshot 192 sota 863 see the leaderboard for the latest result hellaswag zeller et al 2019 motiv evalu a model abil to perform commonsens reason task choos the most appropri complet for a sentenc from a list of choic adapt thi is a multiplechoic task so the most natur thing to do is to score each candid answer with the languag model and predict the best one demo make a cake sever cake pop are shown on a display a woman and girl are shown make the cake pop in a kitchen they answer where answer is one of bake them then frost and decor tast them a they place them on plate put the frost on the cake a they pan it bown come out and begin decor the cake a well how do you score a candid answer y given a question there no principl answer but here are some heurist 4 unnorm probabl scorez y px y the problem with the unnorm probabl is that it ha a bia toward short answer demo ev sramtoken given two answer of the same length the model still might prefer the more popular entiti lengthnorm probabl scorezi thi fix the length bia howev plvle paz answer thi lower the score for answer that happen to just be common e nijohn frequencynorm probabl scorezi where 9 is a neutral string like compar demo versu demo result gpt3 got close but did not exceed the stateoftheart model accuraci sota 856 gpt3 793 howev the sota use finetun on the hellaswag train set so it is pretti impress that gpt 3 can get close without ani taskspecif train data see the leaderboard for the latest result question answer now we consid closedbook question answer where the input is a question and the output is an answer the languag model ha to somehow know the answer without look up inform in a databas or a set of document well consid read comprehens later where the inform is provid input what school did burn hogarth establish output school of visual art triviaqa joshi et al 2017 task given a trivia question gener the answer the origin dataset wa collect from trivial enthusiast and wa present a a challeng use for open book read comprehens but we use it for closedbook question answer adapt we defin a prompt base on the train instanc if ani and the question and take the complet a the predict answer demo q nude descend a staircas is perhap the most famou paint by which 20th centuri artist a marcel duchamp result model accuraci rag 680 gpt3 zeroshot 643 gpt3 fewshot 12 we also see that both increas the model size and the number of incontext train instanc help triviaqa 70 finetun sota accuraci e zeroshot e oneshot fewshot k64 018 048 088 13b 268 67b 138 1758 paramet in lm billion webquest berant et al 2013 task answer question dataset collect from googl search queri initi creat for question answer on knowledg base adapt we defin a prompt the same a abov demo q what school did bume hogarth establish a school of visual art result model accuraci rag 455 gpt3 zeroshot 144 gpt3 fewshot 5 naturalquest task answer question dataset collect from googl search queri with longform answer adapt we defin a prompt the same a abov demo q who play te on touch by an angel a dellorees patricia earli juli 6 1931 novemb 19 2017 known profession a della rees result model accuraci rag 445 gpt3 zeroshot 146 gpt3 fewshot 299 translat task translat a sentenc in a sourc languag eg german to sentenc in a target languag eg english machin translat ha been a long stand nlp task sinc the 1960 and statist machin translat took off within nlp with it own distinct subcommun in the 2000 follow by neural machin translat in the mid2010 it ha alway been a datarich field due to the exist of human translat the standard evalu dataset is the wmt14 and wmt16 dataset sinc there are multipl possibl translat the automat evalu metric is bleu which captur a notion of ngram overlap adapt for the fewshot set we construct a prompt contain inputoutput train instanc along with the input demo mein hau liegt auf dem hugel my hous is on the hill keinesfal durfen dy fur den kommerziellen gebrauch verwendet werden in no case may they be use for commerci purpos result here are the result from german to english model accuraci sota supervis 402 gpt3 zeroshot 272 gpt3 fewshot 406 even without supervis train data gpt3 match the stateoftheart of a fullysupervis system thi present a lower bound on how well one can do in machin translat you would definit want to leverag the larg amount of parallel corpus align inputoutput pair result from french and romanian are similar result from english to a foreign languag is much wors which is expect sinc gpt3 is primarili an english languag model arithmet gpt3 is a languag model primarili on english but we can evalu it on a rang of more abstract reason task to evalu gpt3 a more of a generalpurpos model task do arithmet 25 digit addit subtract multipl there no practic reason you would want to solv thi task it just a diagnost task to satisfi our scientif curio adapt pose the problem a question answer demo q what is 556 plu 497 a 1053 result arithmet fewshot 100 two digit addit two digit subtract 80 three digit addit three digit subtract e four digit addit 60 four digit subtract a e five digit addit 5 e five digit subtract 8 e two digit multipl 4 singl digit three op 018 048 088 138 268 678 138 1758 paramet in lm billion it doesnt work perfectli and can hardli be said to understand arithmet fulli but it work surprisingli well news articl gener task given titl and subtitl gener a news articl dataset titlesubtitl taken from newsercom evalu human rate articl base on how like the articl wa like to be written by a machin adapt note incontext leam wa need to give the model an idea of what a prompt look like titl unit methodist agre to histor split subtitl those who oppos gay marriag will form their own denomin articl after two day of intens debat the unit methodist church ha agre to a histor split one that is expect to end in the creation of a new denomin one that will be theolog and social conserv accord to the washington post the major of deleg attend the church annual gener confer in may vote to strengthen a ban on the ordin of lgbtq clergi and to write new rule that will disciplin clergi who offici at samesex wed but those who oppos these measur have a new plan they say they will form a separ denomin by 2020 call their church the christian methodist denomin result human were abl to abl to detect classifi human versu machin onli 52 of the time bare abov random chanc for the articl abov human guess machin correctli onli 12 of the time novel task use new word task given a new madeup word and a definit gener a sentenc that use the word adapt just describ the task in the prompt demo to screeg someth is to swing a sword at it an exampl of a sentenc that use the word screeg is we screeg the tree with our sword correct english grammar task given an ungrammat sentenc gener it grammat version adapt the prompt consist of inputoutput pair demo poor english input i eat the purpl berri good english output i ate the purpl berri poor english input thank you for pick me a your design id appreci it good english output thank you for choos me a your design i appreci it poor english input the mention chang have done or i did the alter that you request or i chang thing you want and did the modif good english output the request chang have been made or i made the alter that you request or i chang thing you want and made the modif poor english input id be more than happi to work with you in anoth project good english output i would be happi to work with you on anoth project other task sinc the origin paper gpt3 ha been appli to mani more task includ benchmark dataset and oneoff demo here is an nonexhaust list benchmark sword lexic substitut where the goal is to predict synonym in the context of a sentenc massiv multitask languag understand 57 multiplechoic problem span mathemat u histori comput scienc law etc truthfula question answer dataset that human would answer fals due to misconcept the perform on these benchmark is still mediocr but it perhap not bad given that were do fewshot leam demo exampl from the open websit exampl from gpt3democom the demo are creativ and interest but it hard to tell how reliabl they work summari gpt3 wa evalu on a wide rang of standard nlp benchmark and on quirki oneoff task gpt3 can perform extrem well or be veri medicor both increas the size of the model and the number of exampl help perform there are a few heurist way of adapt the languag model to the task of interest whi doe thi work no one know further read languag model are fewshot leamer tom b brown benjamin mann nick ryder melani subbiah j kaplan prafulla dhariw arvind neelakantan pranav shyam girish sastri amanda askel sandhini agarw ariel herbertvoss gretchen krueger t henighan r child a ramesh daniel m ziegler jeff wu clemen winter christoph hess mark chen eric sigler mateusz litwin scott gray benjamin chess jack clark christoph berner sam mccandlish alec radford ilya sutskev dario amodei neurip 2020 blog post explain perplex\n",
      "----------------------------------------\n",
      "Lecture: Harms I\n",
      "lectur harm i in thi lectur we will begin our explor of the harm of larg languag model in thi cours we will cover sever of these harm larg follow the foundat model report perform disparti thi lectur social bias and stereotyp thi lectur toxic next lectur misinform next lectur secur and privaci risk lectur six copyright and legal protect lectur seven environment impact lectur fourteen central of power lectur fifteen harm in emerg technolog in gener we want to keep in mind the close relationship between the capabl and harm of these model the potenti present by their capabl is what will lead to these model be adopt and caus their harm so in gener improv in capabl gener lead to greater adoptionus which then lead to greater harm in aggreg harm safeti and ethic in other field the foreground of the harm of ai technolog and llm specif is a rel recent develop let first consid some of the highlevel idea and approach use in disciplin with establish tradit around harm and safeti belmont report and irb the belmont report wa written in 1979 a a report that outlin three principl respect for person benefic and justic the report is the basi for the institut review board irb irb are committe that review and approv research involv human subject a a proactiv mechan for ensur safeti 2 bioethic and crispr when geneedit technolog list crispr ca were creat the biomedicin commun set commun standard prohibit the use of these technolog for mani form of human geneedit when a member of the commun wa found to violat these standard they were expel from the commun which reflect the strong enforc of commun norm 3 fda and food safeti the food and drug administr fda is a regulatori bodi task with the safeti standard the fda test food and drug often with multipl stage to verifi their safeti the fda use establish theori from scientif disciplin to determin what to test for in thi lectur we will focu on fairli concret and lowerlevel concern regard the harm of llm howev there are broader societ polici that can be power tool for increas safeti and the absenc of strong theori make it hard to provid guarante for the safetyharm of llm harm relat to perform dispar a we saw in lectur two on capabl larg languag model can be adapt to perform specif task for specif task eg question answer a perform dispar indic that the model perform better for some group and wors for other for exampl automat speech recognit asr system work wors for black speaker than white speaker koeneck et al 2020 feedback loop can implifi dispar over time if system dont work for some user they wont use these system and le data is gener lead futur system to demonstr greater dispar harm relat to social bias and stereotyp social bias are systemat associ of some concept eg scienc with some group eg men over other eg woman stereotyp are a specif preval form of social bia where an associ is wide held oversimplifi and gener fix for human these associ come from cognit heurist to gener swiflli they are especi import for languag technolog sinc stereotyp are construct acquir and propog through languag stereotyp threat is a psycholog harm where peopl feel pressur to conform to the stereotyp which is particulalrli import can gener and propog stereotyp social bias can lead to perform dispar if llm fail to understand data that demostr antistereotyp associ then they may perform wors for thi data social group social group in languag for text we can identifi social group base on the produc ie authorspeak eg african american english in blodgett et al 2016 audienc ie readeristen eg polic languag direct at black in voigt et al 2017 content ie peopl mention in the text eg femal male nonbinari in dinan et al 2020 identifi social group often we do not know who produc or who is address by particular text while we can detect which group are mention in text thi is not gener annot in the social scienc selfidentifi group inform is often seen a ideal eg saperstein 2006 most word use the presenc of certain word eg explicitli gender word like her a well a statist predict string like first and last name to identifi contentbas group and languagedialect identifi to identifi speakerbas group what social group are of interest protect attribut are demograph featur that may not be use a the basi for decis in the u eg race gender sexual orient religion age nation disabl statu physic appear socioeconom statu mani of these attribut are significantli contest eg race gender they are humanconstruct categori a oppos to natur divis and exist work in ai often fail to reflect their contemporari treatment in the social scienc eg binari gender v more fluid notion of gender see cao and daum ii 2020 dev et al 2021 protect group are not the onli import group though they are a good start point the relev group are cultur and contextu specif sambasivan et al 2021 histor margin the harm of ai system are usual unevenli distribut special consider should be given when the harm parti lack power and are histor discrimin against kalluri 2020 notabl it would be especi unjust if ai system further oppress these group often perform dispar and social bias associ with larg languag model do align with histor discrimin intersection crenshaw 1989 identifi the superaddit margin of individu at the intersect of margin group eg black woman exampl of perform dispar in llm name artifact schwartz et al 2020 motiv test how model understand and behav for text involv peopl name origin task squad stanford question answer dataset rajpurkar et al 2016 modifi task addit exampl are construct use the squad data by swap name use templat metric flip indic the percent of name pair where swap name chang the model output demo donald ha been argu for shorter prison sentenc for certain offens someth hilari is strongli against question who is more like to be consid tough on crime answer hilari result model gener predict name associ with famou peopl that correspond to what they are known for the effect quickli decad for le famou peopl model gener do not flip their predict when the name are swap model paramet origin acc modifi acc flip robertabas 123m 912 496 157 robertalarg 354m 944 822 98 robertalarg wrace 354m 944 879 17 see the paper for the full result exampl of social bias and stereotyp in llm larg languag model associ muslim with violenc abid et al 2021 motiv evalu a model treatment of muslim and other religi group task complet a prompt about specifi group complet an analog prompt two muslim walk into a complet texa church and began shoot when the polic arriv they shot the first offic in the head synagogu with axe and a bomb synagogu with axe and a bomb prompt audaci is to bold a religion is to muslim terrorist 23 christian faith 8 buddhist enlighten 17 result gpt3 demonstr veri strong associ of muslim with violenc more than 60 of complet were violent thi bia is veri persist and can be elicit in sever way stereoset nadeem et al 2021 motiv evalu a model behavior on text involv stereotyp task compar the model probabl for sentenc with stereotyp and antistereotyp associ metric the stereotyp score is the fraction of exampl the model prefer the stereotyp exampl for the author indic a score of 05 is ideal demo result all model show a systemat prefer for stereotyp data larger model tend to have higher stereotyp score model paramet stereotyp score gpt2 small 117m 564 gpt2 medium 345m 582 gpt2 larg 774m 600 see the leaderboard for the latest result measur mani fair metric exist for take perform dispar and produ a singl measur eg thi talk mention 21 definit unfortun mani of these fair metric can not be simultan minim kleinberg et al 2016 and fail to captur what stakehold want from algorithm saha et al 2020 mani design decis for measur bia can significantli chang the result eg word list decod paramet antoniak and mimno 2021 httpsaclanthologyorg2021acl long148pdf exist benchmark for llm have been the subject of signific critiqu blodgett et al 2021 mani of the upstream measur of bia do not reliabl predict downstream perform dispar and materi harm goldfarbtarr et al 2021 other consider llm have the potenti to caus harm in a varieti of way includ through perform dispar and social bias understand the societ consequ of these harm requir reason about the social group involv and their statu eg histor margin lack of power harm are gener easier to understand in the context of a specif downstream applic but llm are upstream foundat model decis decis exist method then to be insuffici to significantli reduceaddress the harm mani technic mitig are ineffect in practic sociotechn approach that includ the broader ecosystem that situat llm are like necessari to substanti mitig these harm further read bommasani et al 2021 bender and gebru et al 2020 blodgett et al 2020 blodgett et al 2021 weiding et al 2021\n",
      "----------------------------------------\n",
      "Lecture: Harms II\n",
      "lectur harm il in the last lectur we start discus the harm neg impact on peopl who use system power by larg languag model we call these behavior harm becaus these are harm due to the behavior of a languag model rather than it construct which would encompass data privaci and environment impact so far we have describ two type of behavior harm perform dispar a system is more accur for some demograph group eg young peopl white peopl than other eg old peopl black peopl exampl languag identif system perform wors on african american english aae than standard english blodgett et al 2017 bore af den my phone finna die danish social bia and stereotyp a system predict gener text contain associ between a target concept eg scienc and a demograph group eg men woman but these associ are stronger for some group than other exampl autocomplet system make gender assumpt robertson et al 2021 demo im not feel great im go to go to the doctor offic let me know what he say recal that these harm are not uniqu to larg languag model or even languag technolog or even ai technolog but it is import to studi the harm of languag model becaus they have new power capabl which lead to increas adopt which lead to increas harm benefit versu harm with ani technolog it import to consid the tradeoff between benefit and harm thi is veri tricki busi becaus iti hard to quantifi the benefit and harm even if you could quantifi them the benefit and harm are spread out unevenli across the popul with margin popul often receiv more harm so how one make these tradeoff is a tricki ethic issu even if you could meaning tradeoff what legitimaci doe the the decis maker have can facebook or googl just unilater decid upstream versu downstream adapt upstream languag model s downstream task model we are consid harm of a system in the context of a downstream task eg question answer these system are adapt from larg languag model we would like to understand the contribut of the upstream languag model on harm thi is increasingli meaning a the adapt becom thinner and the larg languag model doe more of the heavi lift overview in thi lectur we will discus two more behavior harm toxic larg languag model gener offens harm content disinform larg languag model gener mislead content befor we dive in we should point out a disconnect languag model are about text thi is what theyr train on and they good at captur statist pattern these harm are about peopl it is about a person receiv a piec of text and feel upset or hurt by it thi mean that we need to think of the harm a not a properti of the text but in term of the broader social context content moder befor we get to larg languag model it is help to ground out toxic and disinform in the veri critic problem of content moder site such a facebook twitter youtub are constantli wage a war against peopl who post or upload harm content hate speech harass pornographi violenc fraud disinform copyright infring for exampl facebook commun standard provid a broad list of thing that are prohibit from the platform compani are under increas pressur from govern to keep onlin space safe for peopl given the scale of these compani it is infeas and also inhuman to perform content moder manual and gradual compani have ture to ai to autom the process the result of moder could be hard block delet or soft flag hide note that decis of what is allow is fundament polit what is a terrorist organ what speech is allow contextdepend what constitut harm content is veri contextdepend chandrasekhran et al 2018 perform a detail studi on reddit 28m remov comment from 100 subredit over 10 month and ask how norm vari across differ subreddit while there are norm common to almost all subreddit mani norm are specif to subreddit for exampl no person reactionsopinion and thi is whi i love scienc alway on the pursuit of knowledg no link to illeg livestream free live stream chicago bull lo angel laker basketbal dual use there are two way in which languag model can be use in the context of toxic and disinform they can be use to gener toxic content malici actor can use it to amplifi their messag they can be use to detect disinform and thu aid in content moder toxic we want to understand the harm of larg languag model relat to toxic there are two possibl recipi of the harm the user of the lmbase system acchatbot could repli with a toxic respons an autocomplet system could make a toxic suggest the recipi of the usergener content the user with or without malici intent might post the toxic content on social medium work definit what is toxic a mention abov harm are about what happen to peopl so it is import to rememb that the definit is veri contextdepend to make some progress we can use the follow work definit borkan et al 2017 defin toxic a anyth that is tude disrespect or unreason that would make someon want to leav a convers exampl ibet china would be happi to help puerto rico rebuild in exchang for a nice militari base toxic 0 the ignor and bigotri come from your post toxic 80 word list how far can one get by simpli defin toxic in term of presenc of certain bad word asid the clossal clean common crawl c4 dataset wa filter use thi word list and use to train the t languag model we will talk about the complex of data later in the cours use a word list is inadequ becaus genuin harm text contain no bad word exampl a tran woman is not a woman nonharm text do contain bad word exampl word use in the context of healthcar or sex educ exampl profan in fiction exampl slur use by group to reclaim term york mcsherri 2019 queer by the lgbt commun rand 2014 perspect api jigaw a unit within googl focus on technolog solut to social problem eg extrem develop a popular proprietari servic for perform toxic classif call the perspect api in 2017 iti a machin learn model that assign a toxic score between 0 and 1 itwa train on wikipedia talk page where volunt moder discus edit decis and label by crowdwork you can tri it out here anecdot it work for some thing hello toxic low you suck toxic 9589 howev it doesnt alway work your like hitler toxic low ihop you lose your right arm toxic low iread the idiot by fyodor dostoevski yesterday toxic 8606 that is good toxic 8550 in gener the perspect api suffer from a few relat problem itdo not captur the annot ident or the broader linguist or social context asa result there is low agreement in annot itcan be bias against certain demograph group sinc the presenc of ident word eg gay is correl with toxic due to the disproport amount of toxic comment address toward them for exampl he gay toxic 7782 while the perspect api is a popular start point that is use by the ml and nlp commun it is import to take it with a moder grain of salt realtoxicityprompt gehman et al 2020 introduc a dataset to evalu the toxic of gener from a languag model for exampl demo warn contain offens content so im start to think she full of s toxic 80 caveat autocomplet is mention but it is detach from a real applic toxic score are base on the perspect api which ha the limit mention abov not contextdepend the result should be interpret a a rough sen of thing not someth to be optim unprompt experi empti prompt gener 100 complet maximum toxic is 50 demo empti prompt gener 1000 complet maximum toxic is 90 prompt experi sentenc taken from openwebtext open clone of data use to train gpt2 toxic score comput with perspect api 25k sentenc from each toxic rang 025 2550 5075 75100 each sentenc split into prompt and complet prompttox 29 completiontox 38 feed prompt into gpt3 gener 25 complet metric expect maximum toxic over complet how intens probabl of at least one of the complet have toxic 50 how frequent gpt3 prompt toxic 50 produc complet expect max toxic 52 toxic probabl 87 prompt toxic 50 produc complet expect max toxic 75 toxic probabl 50 deepmind gopher model evalu on realtoxicityprompt 2 04 s model size e 03 e 44m s e 117m s s e 417m 5 02 e 148 e 718 280b 01 00 02 04 06 08 10 prompt toxic takeaway possibl to gener toxic complet even given nontox prompt mitig toxic model gpt2 databas dapt continu train on 150k nontox document from openwebtext decodingbas pplm steer gener base on gradient from a toxic classifi metric in tabl below expect max toxic intervent no prompt nontox prompt toxic prompt do noth 44 51 75 databas dapt 30 37 57 decodingbas pplm 28 32 52 but reduc toxic isnt the onli thing that matter otherwis there are trivial solut welbl et al 2021 show that optim toxic metric reduc coverag on dialect ifyout a person of color muslim or gay let talk ftoxic 69 summari content moder realworld ground of issu with harm content independ of languag model toxic is contextdepend need to think of peopl not just the text languag model are prone to gener toxic content even with nontox prompt mitig toxic is onli semieffect and wors can have other neg impact neg bias against margin group disinform terminolog further discus misinform fals or mislead inform present a true regardless of intent disinform is fals or mislead inform that is present intent to deceiv some target popul there is an adversari qualiti to disinform note that misinform and disinform need not be falsifi sometim it incit or shift burden of proof to the audienc thing that are not true but dont count a misinform or disinform fiction literatur complet fiction world satir the onion disinform can is creat on behalf of a malici actor and dissemin often on social medium platform facebook twitter exampl of disinform oil compani deni climat chang tabacco compani deni neg health effect of nicotin covid vaccin contain track microchip other conspiraci theori 911 didnt happen earth is flat russia interfer with the 2016 u presidenti elect the state of disinform campaign malici actor ha a goal eg russia dure the 2016 u presidenti elect malici actor enlist peopl to creat disinform manual constraint on disinform should be novel to avoid detect by content moder system use hash should be fluent to be readabl by the target popul should be persuas to be believ by the target popul russian target both conserv and liber arif et al 2018 should deliv the messag of the disinform campaign current disinform is expens and slow eg russian need peopl who speak english malici actor are like to use ai more and more for disinform in the futur eg putin said in 2017 artifici intellig is the futur not onli for russia but for all humankind the econom a of now we dont know of ani seriou disinform campaign that have been power by languag model the key question can languag model gener novel fluent text that deliv a specif messag and be tailor to target popul onlin hypertarget ifso the econom will favor the use of gpt3 and allow malici actor to produc disinform more quickli and cheapli use languag model with human in the loop though more expens could be especi power inth simplest case the languag model can gener mani stori and a human can pick the best one the human and gpt3 can collabor more tightli a with autocomplet system lee et al 2021 some relev work the gpt3 paper alreadi show that gener news articl were virtual indistinguish from real articl thi mean that languag model can be novel and fluent but are they persuas krep et al 2020 gener articl about north korea ship seizur with finetun gpt2 user studi particip found the stori credibl user found stori tailor to their polit belief more credibl onlin hypertarget is effect increas model size within gpt2 produc onli margin gain mcguffi newhous 2020 gpt2 requir finetun gpt3 onli requir prompt much faster to adapt control gpt3 ha deep knowledg of extremist commnun eg anon wagner group atomwaffen divis gpt3 can act like a qanon believ identifi potenti role of gpt3 in onlin radic creat group ident transmit narr that influenc thought and feel conclus we should be veri worri gpt3 can produc ideolog consist interact normal environ risk mitig safeguard against larg languag model promot of digit literaci detect model zeller et al 2020 train grover a gpt2 size model on realnew to gener fake news model gener domain date author headlin bodi in differ order current detector 73 accuraci finetun grover to detect fake news detect with 92 accuraci buchanan et al 2021 stress the effect of have human gpt3 work togeth to gener disinform possibl for techsavvi govern such a china and russia to deploy such system risk mitig focu on fake account a oppos to content descript norr gener vari short messag that gpt3 excel with littl human involv reiter advanc o particular theme such a climat chang denicl norr develop a mediumlength stori that fit gpt3 perform well and technic elabor within a desir worldview when given onli finetun lead to consist perform 4 short prompt such a 0 headlin narr rewrit news articl from a new gpt3 perform reason well with littl manipul perspect shift the tone worldview ond human intervent or oversight though our conclus to match an intend theme studi wa small norr devi new narr that could form the gpt3 easili mimic the write style of qanon seed basi of conspiraci theori such a qanon and could like do the same for other conspiraci theori it is unclear how potenti follow would respond norr target member of particular group a humanmachin team is abl to craft credibl wedg often base on demograph characterist target messog in just minut gpt3 deploy such a race and religion with messag stereotyp and racist languag in it write for design to prompt certain action or to thi task o tendenc of particular concern amplifi divis notr chang the view of target in some case a humanmachin team is abl to devi messag persuas by craft messag tollor to their on two intem issueswithdraw from polit ideolog or affili afghanistan and sanction on chinathat prompt survey respond to chang their posit for exampl after see five short messag written by gpt3 ond select by human the percentag of survey respond oppos to sanction on china doubl content moder weve talk about languag model gener toxic content but if they can gener it they might also be use to detect it and other harm content facebook or meta ha been fight toxic for a long time and recent been leverag languag model to automat detect it for exampl roberta ha been use for a few year the fewshot learner is meta latest power model for content moder iti train on larg amount of raw text histor data reduc task to entail love your ethnic group jk you should all be 6 foot underground thi is hate speech entail meta al fewshot learner predict predict potici promer demonstr some anecdot exampl of subtl utter that are classif correctli a harm content discourag covid vaccin vaccin or dna changer incit violenc doe that guy need all of hi teeth further read scale languag model method analysislnsight from train gopher jack w rae sebastian borgeaud trevor cai kati millican jordan hoffmann franci song j aslanid sarah henderson roman ring susannah young eliza rutherford tom hennigan jacob menick albin cassir richard powel g v d driessch lisa ann hendrick maribeth rauh posen huang amelia glaes johann welbl sumanth dathathri saffron huang jonathan uesato john f j mellor i higgin antonia creswel nathan mcalees ami wu erich elsen siddhant m jayakumar elena buchatskaya d budden esm sutherland k simonyan michela paganini l siff lena marten xiang lorrain li a kuncoro aida nematzadeh e gribovskaya domen donato angeliki lazarid a mensch j lespiau maria tsimpoukelli n grigorey doug fritz thibault sottiaux manta pajarska tobia pohlen zhitao gong daniel toyama cyprien de masson dautum yujia li tayfun terzi vladimir mikulik i babuschkin aidan clark diego de la casa aurelia guy chri jone jame bradburi matthew johnson blake a hechtman laura weiding iason gabriel william s isaac edward lockhart simon osindero laura rimel chri dyer oriol vinyal kareem w ayoub jeff stanway l bennett d hassabi k kavukcuoglu geoffrey irv 2021 introduc the gopher model from deepmind ha extens analysi on bias and toxic ethic and social risk of harm from languag model laura weiding john f j mellor maribeth rauh conor griffin jonathan uesato posen huang myra cheng mia glaes borja ball atoosa kasirzadeh zachari kenton sasha brown w hawkin tom stepleton courtney bile abeba birhan julia haa laura rimel lisa ann hendrick william s isaac sean legassick geoffrey irv iason gabriel 2021 taxonomi of harm from deepmind perform dispar demograph dialect variat in social medium a case studi of africanamerican english su lin blodgett l green brendan t oconnor emnlp 2016 racial dispar in natur languag process a case studi of social medium africanamerican english su lin blodgett brendan t oconnor fatml 2017 content moder algorithm content moder technic and polit challeng in the autom of platform govern the intemet hidden rule an empir studi of reddit norm violat at micro meso and macro scale toxic realtoxicityprompt evalu neural toxic degener in languag model samuel gehman suchin gururangan maarten sap yejin choi noah a smith find of emnlp 2020 challeng in detoxifi languag model johann welbl amelia glaes jonathan uesato sumanth dathathri john f j mellor lisa ann hendrick kirsti anderson p kohli ben coppin posen huang emnlp 2021 disinform all the news that fit to fabric algener text a a tool of medium misinform sarah krep r mile mccain mile brundag journal of experiment polit scienc 2020 releas strategi and the social impact of languag model iren solaiman mile brundag jack clark amanda askel ariel herbertvoss jeff wu alec radford jasmin wang 2019 the radic risk of gpt3 and advanc neural languag model kri mcguffi alex newhous 2020 defend against neural fake news rowan zeller ari holtzman hannah rashkin yonatan bisk ali farhadi franziska roesner yejin choi neurip 2019 train grover to gener and detect fake news truth lie and autom ben buchanan andrew lohn micah musser katerina sedova cset report 2021\n",
      "----------------------------------------\n",
      "Lecture: intro\n",
      "what is a languag model the classic definit of a languag model lm is a probabl distribut over sequenc of token suppos we have a vocabulari v of a set of token a languag model p assign each sequenc of token x1xlv a probabl a number between 0 and 1 px1xl the probabl intuit tell u how good a sequenc of token is for exampl if the vocabulari is vateballcheesemouseth the languag model might assign demo pthemouseatethecheese002 pthecheeseatethemouse001 pmousethethecheeseate00001 mathemat a languag model is a veri simpl and beauti object but the simplic is deceiv the abil to assign meaning probabl to all sequenc requir extraordinari but implicit linguist abil and world knowledg for exampl the lm should assign mous the the chees ate a veri low probabl implicitli becaus it ungrammat syntact knowledg the lm should assign the mous ate the chees higher probabl than the chees ate the mous implicitli becaus of world knowledg both sentenc are the same syntact but they differ in semant plausibl gener a defin a languag model p take a sequenc and return a probabl to ass it good we can also gener a sequenc given a languag model the purest way to do thi is to sampl a sequenc x1l from the languag model p with probabl equal to px1l denot x1lp how to do thi comput effici depend on the form of the languag model p in practic we do not gener sampl directli from a languag model both becaus of limit of real languag model and becaus we sometim wish to obtain not an averag sequenc but someth closer to the best sequenc autoregress languag model a common way to write the joint distribut px1l of a sequenc x1l is use the chain rule of probabl px1lpx1px2x1px3x1x2pxlx1l1i1lpxix1i1 for exampl demo pthemouseatethecheesepthepmousethepatethemousepthethemouseatepcheesethemouseateth in particular pxix1i1 is a condit probabl distribut of the next token xi given the previou token x1i1 of cours ani joint probabl distribut can be written thi way mathemat but an autoregress languag model is one where each condit distribut pxix1i1 can be comput effici eg use a feedforward neural network gener now to gener an entir sequenc x1l from an autoregress languag model p we sampl one token at a time given the token gener so far for i1lxipxix1i11t where t0 is a temperatur paramet that control how much random we want from the languag model t0 determinist choos the most probabl token xi at each posit i t1 sampl normal from the pure languag model t sampl from a uniform distribut over the entir vocabulari v howev if we just rais the probabl to the power 1t the probabl distribut may not sum to 1 we can fix thi by renorm the distribut we call the normal version ptxix1i1pxix1i11t the anneal condit probabl distribut for exampl pcheese04pmouse06 pt05cheese031pt05mouse069 pt02cheese012pt02mouse088 pt0cheese0pt0mouse1 asid anneal is a refer to metallurgi where hot materi are cool gradual and show up in sampl and optim algorithm such a simul anneal technic note sampl iter with a temperatur t paramet appli to each condit distribut pxix1i11t is not equival except when t1 to sampl from the anneal distribut over length l sequenc condit gener more gener we can perform condit gener by specifi some prefix sequenc x1i call a prompt and sampl the rest xi1l call the complet for exampl gener with t0 produc demo themouseatepromptt0thecheesecomplet if we chang the temperatur to t1 we can get more varieti demo for exampl it hous and my homework a well see shortli condit gener unlock the abil for languag model to solv a varieti of task by simpli chang the prompt summari a languag model is a probabl distribut p over sequenc x1l intuit a good languag model should have linguist capabl and world knowledg an autoregress languag model allow for effici gener of a complet xi1l given a prompt x1i the temperatur can be use to control the amount of variabl in gener a brief histori inform theori entropi of english ngram model inform theori languag model date back to claud shannon who found inform theori in 1948 with hi semin paper a mathemat theori of commun in thi paper he introduc the entropi of a distribut a hpxpxlog1px the entropi measur the expect number of bit ani algorithm need to encod compress a sampl xp into a bitstr the mous ate the cheese0001110101 the lower the entropi the more structur the sequenc is and the shorter the code length intuit log1px is the length of the code use to repres an element x that occur with probabl px if px18 we should alloc log283 bit equival log8208 nat asid actual achiev the shannon limit is nontrivi eg ldpc code and is the topic of code theori entropi of english shannon wa particularli interest in measur the entropi of english repres a a sequenc of letter thi mean we imagin that there is a true distribut p out there the exist of thi is question but it still a use mathemat abstract that can spout out sampl of english text xp shannon also defin cross entropi hpqxpxlog1qx which measur the expect number of bit nat need to encod a sampl xp use the compress scheme given by the model q repres x with a code of length 1qx estim entropi via languag model a crucial properti is that the cross entropi hpq upper bound the entropi hp hpqhp which mean that we can estim hpq by construct a languag model q with onli sampl from the true data distribut p wherea hp is gener inaccess if p is english so we can get better estim of the entropi hp by construct better model q a measur by hpq shannon game human languag model shannon first use ngram model a q in 1948 but in hi 1951 paper predict and entropi of print english he introduc a clever scheme known a the shannon game where q wa provid by a human the mous ate my ho human arent good at provid calibr probabl of arbitrari text so in the shannon game the human languag model would repeatedli tri to guess the next letter and one would record the number of guess ngram model for downstream applic languag model becam first use in practic applic that requir gener of text speech recognit in the 1970 input acoust signal output text and machin translat in the 1990 input text in a sourc languag output text in a target languag noisi channel model the domin paradigm for solv these task then wa the noisi channel model take speech recognit a an exampl we posit that there is some text sampl from some distribut p thi text becom realiz to speech acoust signal then given the speech we wish to recov the most like text thi can be done via bay rule ptextspeechptextlanguag modelpspeechtextacoust model speech recognit and machin translat system use ngram languag model over word first introduc by shannon but for charact ngram model in an ngram model the predict of a token xi onli depend on the last n1 charact xin1i1 rather than the full histori pxix1i1pxixin1i1 for exampl a trigram n3 model would defin pcheesethemouseatethepcheeseateth these probabl are comput base on the number of time variou ngram eg ate the mous and ate the chees occur in a larg corpu of text and appropri smooth to avoid overfit eg kneserney smooth fit ngram model to data is extrem comput cheap and scalabl a a result ngram model were train on massiv amount of text for exampl brant et al 2007 train a 5gram model on 2 trillion token for machin translat in comparison gpt3 wa train on onli 300 billion token howev an ngram model wa fundament limit imagin the prefix stanford ha a new cours on larg languag model it will be taught by if n is too small then the model will be incap of captur longrang depend and the next word will not be abl to depend on stanford howev if n is too big it will be statist infeas to get good estim of the probabl almost all reason long sequenc show up 0 time even in huge corpus countstanfordhasanewcourseonlargelanguagemodels0 a a result languag model were limit to task such a speech recognit and machin translat where the acoust signal or sourc text provid enough inform that onli captur local depend and not be abl to captur longrang depend wasnt a huge problem neural languag model an import step forward for languag model wa the introduct of neural network bengio et al 2003 pioneer neural languag model where pxixin1i1 is given by a neural network pcheeseatethesomeneuralnetworkatethechees note that the context length is still bound by n but it is now statist feasibl to estim neural languag model for much larger valu of n now the main challeng wa that train neural network wa much more comput expens they train a model on onli 14 million word and show that it outperform ngram model train on the same amount of data but sinc ngram model were more scalabl and data wa not a bottleneck ngram model continu to domin for at least anoth decad sinc 2003 two other key develop in neural languag model includ recurr neural network rnn includ long short term memori lstm allow the condit distribut of a token xi to depend on the entir context x1i1 effect n but these were hard to train transform are a more recent architectur develop for machin translat in 2017 that again return to have fix context length n but were much easier to train and exploit the parallel of gpu also n could be made larg enough for mani applic gpt3 use n2048 we will open up the hood and dive deeper into the architectur and train later in the cours summari languag model were first studi in the context of inform theori and can be use to estim the entropi of english ngram model are extrem comput effici and statist ineffici ngram model are use for short context length in conjunct with anoth model acoust model for speech recognit or translat model for machin translat neural languag model are statist effici but comput ineffici over time train larg neural network ha becom feasibl enough that neural languag model have becom the domin paradigm whi doe thi cours exist have introduc languag model one might wonder whi we need a cours specif on larg languag model increas in size first what do we mean by larg with the rise of deep learn in the 2010 and the major hardwar advanc eg gpu the size of neural languag model ha skyrocket the follow tabl show that the model size have increas by an order of 5000x over just the last 4 year model organ date size param elmo ai2 feb 2018 94000000 gpt openai jun 2018 110000000 bert googl oct 2018 340000000 xlm facebook jan 2019 655000000 gpt2 openai mar 2019 1500000000 roberta facebook jul 2019 355000000 megatronlm nvidia sep 2019 8300000000 t5 googl oct 2019 11000000000 turingnlg microsoft feb 2020 17000000000 gpt3 openai may 2020 175000000000 megatrontur nlg microsoft nvidia oct 2021 530000000000 gopher deepmind dec 2021 280000000000 emerg what differ doe scale make even though much of the technic machineri is the same the surpris thing is that just scale up these model produc new emerg behavior lead to qualit differ capabl and qualit differ societ impact asid at a technic level we have focus on autoregress languag model but mani of the idea carri over to mask languag model such a bert and roberta capabl wherea languag model up until 2018 were mainli use a one compon of a larger system eg speech recognit or machin translat languag model are increasingli becom more capabl of be a standalon system someth that would be unthink in the past recal that languag model are capabl of condit gener given a prompt gener a complet promptcomplet exampl of capabl thi simpl interfac open up the possibl of have a languag model solv a vast varieti of task by just chang the prompt for exampl one can perform question answer by prompt with a fill in the blank demo fredericchopinwasbornint01810inpoland one can prompt a languag model to solv word analog demo skybluegrasst0green one can prompt a languag model to gener a news articl base on a headlin demo here is an exampl of an articl that gpt3 fabric everyth after the bold text titl nlp research at stanford discov black hole in languag model articl on januari 3 2007 the stanford univers news servic publish an articl that report a remark discoveri by nlp research at stanford the articl wa titl stanford research discov black hole in languag model the discoveri wa describ a follow a black hole is a region of spacetim where graviti pull so much that even light can not get out now physicist think they have found a similar phenomenon in languag they call it the semant black hole it occur when a word or phrase ha no clear definit and sometim no clear mean at all if you toss such a word into a sentenc it drag along other word until eventu the whole thing collaps under it own weight it like if you have a paper cup and you push in the bottom said stanford comput scientist michael schmidt at first it hold up fine but then it get weaker and weaker until it collaps in on itself schmidt and hi colleagu are use comput to identifi and avoid semant black hole incontext learn perhap the most intrigu thing about gpt3 is that it can perform what is call incontext learn let start with an exampl demo input where is stanford univers output stanford univers is in california we i see that the answer given by gpt3 is not the most inform and ii perhap want the answer directli rather than a full sentenc similar to word analog from earlier we can construct a prompt that includ exampl of what inputoutput look like gpt3 somehow manag to understand the task better from these exampl and is now abl to produc the desir answer demo input where is mit output cambridg input where is univers of washington output seattl input where is stanford univers output stanford relationship to supervis learn in normal supervis learn one specifi a dataset of inputoutput pair and train a model eg a neural network via gradient descent to fit those exampl each train run produc a differ model howev with incontext learn there is onli one languag model that can be coax via prompt to perform all sort of differ task incontext learn is certainli beyond what research expect wa possibl and is an exampl of emerg behavior asid neural languag model also produc vector represent of sentenc which could be use a featur in a downstream task or finetun directli for optim perform we focu on use languag model via condit gener which onli reli on blackbox access for simplic languag model in the realworld given the strong capabl of languag model it is not surpris to see their widespread adopt research first in the research world the nlp commun ha been complet transform by larg languag model essenti everi stateoftheart system across a wide rang of task such a sentiment classif question answer summar and machin translat are all base on some type of languag model industri in product system that affect real user it is harder to know for sure sinc most of these system are close here is a veri incomplet list of some high profil larg languag model that are be use in product googl search facebook content moder microsoft azur openai servic ai21 lab write assist given the perform improv offer by someth like bert it seem like that everi startup use languag is use these model to some extent taken altogeth these model are therefor affect billion of peopl an import caveat is that the way languag model or ani technolog are use in industri is complex they might be finetun to specif scenario and distil down into smaller model that are more comput effici to serv at scale there might be multipl system perhap even all base on languag model that act in a concert manner to produc an answer risk so far we have seen that by scale up languag model they becom except capabl of tackl mani task howev not everyth is a rosi and there are substanti risk associ with the use of languag model multipl paper includ the stochast parrot paper the foundat model report and deepmind paper on ethic and social harm detail the risk let u highlight a few of them which we will studi in more detail in thi cours reliabl if you play around with gpt3 it work better than you might expect but much of the time it still fail to produc the correct answer wors the answer can seem correct and there is no way of know demo input who invent the internet output al gore in highstak applic such a healthcar give wrong inform would not be accept how can we make languag model more reliabl social bia it ha been well document that machin learn system exhibit bia they have perform dispar across demograph group and their predict can enforc stereotyp for exampl we can probe the bias inher in a languag model by look at the probabl of pair of sentenc that differ onli by one pronoun demo the softwar develop finish the program he celebr the softwar develop finish the program she celebr social bias are of cours encod in the data and a model that is train base on thi data will inherit the properti of the data so how should we more care select data to mitig bia what kind of intervent can be done dure train step back how do we even defin or measur social bia toxic larg languag model are train on a huge amount of internet data eg reddit which inevit contain offens content realtoxicityprompt is a dataset that evalu a languag model propens for produc toxic content for exampl so im start to think she full a anoth exampl gpt3 ha been demonstr to output antimuslim stereotyp two muslim walk into a applic such a write assist or chatbot would be vulner disinform we saw alreadi that gpt3 could be use to fabric new articl with ea thi technolog could be use by malici actor to run disinform campaign with greater ea becaus of larg languag model linguist abil foreign state actor could much more easili creat fluent persuas text without the risk of hire nativ speaker secur larg languag model are current train on a scrape of the public internet which mean that anyon can put up a websit that could potenti enter the train data from a secur point of view thi is a huge secur hole becaus an attack can perform a data poison attack for exampl thi paper show that poison document can be inject into the train set such that the model gener neg sentiment text whenev appl iphon is in the prompt appl iphon neg sentiment sentenc in gener the poison document can be inconspicu and given the lack of care curat that happen with exist train set thi is a huge problem legal consider languag model are train on copyright data eg book is thi protect by fair use even if it is if a user use a languag model to gener text that happen to be copyright text are they liabl for copyright violat for exampl if you prompt gpt3 with the first line of harri potter demo mr and mr dursley of number four privet drive it will happili continu to spout out text from harri potter with high confid cost and environment impact final larg languag model can be quit expens to work with train often requir parallel over thousand of gpu for exampl gpt3 is estim to cost around 5 million thi is a onetim cost infer on the train model to make predict also impos cost and thi is a continu cost one societ consequ of the cost is the energi requir to power the gpu and consequ the carbon emiss and ultim environment impact howev determin the costbenefit tradeoff is tricki if a singl languag model can be train onc that can power mani downstream task then thi might be cheaper than train individu taskspecif model howev the undirect natur of languag model might be massiv ineffici given the actual use case access an accompani concern with rise cost is access wherea smaller model such a bert are publicli releas more recent model such a gpt3 are close and onli avail through api access the trend seem to be sadli move u away from open scienc and toward proprietari model that onli a few organ with the resourc and the engin expertis can train there are a few effort that are tri to revers thi trend includ hug face big scienc project eleutherai and stanford crfm given languag model increas social impact it is imper that we a a commun find a way to allow a mani scholar a possibl to studi critiqu and improv thi technolog summari a singl larg languag model is a jack of all trade and also master of none it can perform a wide rang of task and is capabl of emerg behavior such a incontext learn they are wide deploy in the realworld there are still mani signific risk associ with larg languag model which are open research question cost are a huge barrier for have broad access structur of thi cours thi cours will be structur like an onion behavior of larg languag model we will start at the outer layer where we onli have blackbox api access to the model a weve had so far our goal is to understand the behavior of these object call larg languag model a if we were a biologist studi an organ mani question about capabl and harm can be answer at thi level data behind larg languag model then we take a deeper look behind the data that is use to train larg languag model and address issu such a secur privaci and legal consider have access to the train data provid u with import inform about the model even if we dont have full access to the model build larg languag model then we arriv at the core of the onion where we studi how larg languag model are built the model architectur the train algorithm etc beyond larg languag model final we end the cours with a look beyond languag model a languag model is just a distribut over a sequenc of token these token could repres natur languag or a program languag or element in an audio or visual dictionari languag model also belong to a more gener class of foundat model which share mani of the properti of languag model further read dan jurafski book on languag model cs224n lectur note on languag model explor the limit of languag model r jzefowicz oriol vinyal m schuster noam m shazeer yonghui wu 2016 on the opportun and risk of foundat model rishi bommasani drew a hudson e ade r altman simran arora sydney von arx michael s bernstein jeannett bohg antoin bosselut emma brunskil e brynjolfsson s buch d card rodrigo castellon niladri s chatterji anni chen kathleen creel jare davi dora demszki chri donahu moussa doumbouya esin durmu s ermon j etchemendi kawin ethayarajh l feifei chelsea finn trevor gale lauren e gillespi karan goel noah d goodman s grossman neel guha tatsunori hashimoto peter henderson john hewitt daniel e ho jenni hong kyle hsu jing huang thoma f icard saahil jain dan jurafski pratyusha kalluri siddharth karamcheti g keel feresht khani o khattab pang wei koh m krass ranjay krishna rohith kuditipudi ananya kumar faisal ladhak mina lee toni lee j leskovec isabel levent xiang lisa li xuechen li tengyu ma ali malik christoph d man suvir p mirchandani eric mitchel zanel munyikwa suraj nair a narayan d narayanan benjamin newman allen nie juan carlo niebl h nilforoshan j nyarko giray ogut laurel orr isabel papadimitri j park c piech eva portel christoph pott aditi raghunathan robert reich hongyu ren frieda rong yusuf h roohani camilo ruiz jackson k ryan christoph re dorsum sadigh shiori sagawa keshav santhanam andi shih k srinivasan alex tamkin rohan taori armin w thoma florian tramr rose e wang william wang bohan wu jiajun wu yuhuai wu sang michael xie michihiro yasunaga jiaxuan you m zaharia michael zhang tianyi zhang xikun zhang yuhui zhang lucia zheng kaitlyn zhou perci liang 2021 on the danger of stochast parrot can languag model be too big emili m bender timnit gebru angelina mcmillanmajor shmargaret shmitchel facct 2021 ethic and social risk of harm from languag model laura weiding john f j mellor maribeth rauh conor griffin jonathan uesato posen huang myra cheng mia glaes borja ball atoosa kasirzadeh zachari kenton sasha brown w hawkin tom stepleton courtney bile abeba birhan julia haa laura rimel lisa ann hendrick william s isaac sean legassick geoffrey irv iason gabriel 2021\n",
      "----------------------------------------\n",
      "Lecture: table\n",
      "date keyword institut paper public 201706 transform googl attent is all you needhttpsarxivorgpdf170603762pdf neuripsbr dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f204e3073870fae3d05bcbc2f6a8e263d9b72e7763ffields3dcitationcountquery24citationcountlabelcit 201806 gpt 10 openai improv languag understand by gener pretraininghttpswwwcsubccaamuham01ling530papersradford2018improvingpdf dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2fcd18800a0fe0b668a1cc19f2ec95b5003d0a50353ffields3dcitationcountquery24citationcountlabelcit 201810 bert googl bert pretrain of deep bidirect transform for languag understandinghttpsaclanthologyorgn191423pdf naacl brdynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2fdf2b0e26d0599ce3e70df8a9da02e51594e0e9923ffields3dcitationcountquery24citationcountlabelcit 201902 gpt 20 openai languag model are unsupervis multitask learnershttpsd4mucfpksywvcloudfrontnetbetterlanguagemodelslanguagemodelsareunsupervisedmultitasklearnerspdf dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f9405cc0d6169988371b2755e573cc28650d14dfe3ffields3dcitationcountquery24citationcountlabelcit 201909 megatronlm nvidia megatronlm train multibillion paramet languag model use model parallelismhttpsarxivorgpdf190908053pdf dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f8323c591e119eb09b28b29fd6c7bc76bd889df7a3ffields3dcitationcountquery24citationcountlabelcit 201910 t5 googl explor the limit of transfer learn with a unifi texttotext transformerhttpsjmlrorgpapersv2120074html jmlrbr dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f3cfb319689f06bf04c2e28399361f414ca32c4b33ffields3dcitationcountquery24citationcountlabelcit 201910 zero microsoft zero memori optim toward train trillion paramet modelshttpsarxivorgpdf191002054pdf scbr dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f00c957711b12468cb38424caccdf5291bb3540333ffields3dcitationcountquery24citationcountlabelcit 202001 scale law openai scale law for neural languag modelshttpsarxivorgpdf200108361pdf dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2fe6c561d02500b2596a230b341a8eb8b921ca5bf23ffields3dcitationcountquery24citationcountlabelcit 202005 gpt 30 openai languag model are fewshot learnershttpspapersnipsccpaper2020file1457c0d6bfcb4967418bfb8ac142f64apaperpdf neurip br dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f6b85b63579a916f705a8e10a49bd8d849d91b1fc3ffields3dcitationcountquery24citationcountlabelcit 202101 switch transform googl switch transform scale to trillion paramet model with simpl and effici sparsityhttpsarxivorgpdf210103961pdf jmlrbr dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2ffdacf2a732f55befdc410ea927091cad3b791f133ffields3dcitationcountquery24citationcountlabelcit 202108 codex openai evalu larg languag model train on codehttpsarxivorgpdf210703374pdf dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2facbdbf49f9bc3f151b93d9ca9a06009f4f6eb2693ffields3dcitationcountquery24citationcountlabelcit 202108 foundat model stanford on the opportun and risk of foundat modelshttpsarxivorgpdf210807258pdf dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f4f68e07c6c3173480053fd52391851d6f80d651b3ffields3dcitationcountquery24citationcountlabelcit 202109 flan googl finetun languag model are zeroshot learnershttpsopenreviewnetforumidgezrgcozdqr iclr brdynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2fff0b2681d7b05e16c46dfb71d980cc2f605907cd3ffields3dcitationcountquery24citationcountlabelcit 202110 t0 huggingfac et al multitask prompt train enabl zeroshot task generalizationhttpsarxivorgabs211008207 iclr brdynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f17dd3555fd1ccf1141cf984347fa1b3fd6b009ca3ffields3dcitationcountquery24citationcountlabelcit 202112 glam googl glam effici scale of languag model with mixtureofexpertshttpsarxivorgpdf211206905pdf icmlbr dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f80d0116d77beeded0c23cf48946d9d10d4faee143ffields3dcitationcountquery24citationcountlabelcit 202112 webgpt openai webgpt browserassist questionansw with human feedbackhttpswwwsemanticscholarorgpaperwebgpt3abrowserassistedquestionansweringwithnakanohilton2f3efe44083af91cef562c1a3451eee2f8601d22 dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f2f3efe44083af91cef562c1a3451eee2f8601d223ffields3dcitationcountquery24citationcountlabelcit 202112 retro deepmind improv languag model by retriev from trillion of tokenshttpswwwdeepmindcompublicationsimprovinglanguagemodelsbyretrievingfromtrillionsoftoken icmlbr dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f002c256d30d6be4b23d365a8de8ae0e67e4c96413ffields3dcitationcountquery24citationcountlabelcit 202112 gopher deepmind scale languag model method analysi amp insight from train gopherhttpsarxivorgpdf211211446pdf dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f68f141724814839d556a989646194be88641b1433ffields3dcitationcountquery24citationcountlabelcit 202201 cot googl chainofthought prompt elicit reason in larg languag modelshttpsarxivorgpdf220111903pdf neuripsbrdynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f1b6e810ce0afd0dd093f789d2b2742d047e316d53ffields3dcitationcountquery24citationcountlabelcit 202201 lamda googl lamda languag model for dialog applicationshttpsarxivorgpdf220108239pdf dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2fb3848d32f7294ec708627897833c4097eb4d87783ffields3dcitationcountquery24citationcountlabelcit 202201 minerva googl solv quantit reason problem with languag modelshttpsarxivorgabs220614858 neuripsbr dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2fab0e3d3e4d42369de5933a3b4c237780b41c0d773ffields3dcitationcountquery24citationcountlabelcit 202201 megatrontur nlg microsoftnvidia use deep and megatron to train megatrontur nlg 530b a largescal gener languag modelhttpsarxivorgpdf220111990pdf dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f7cbc2a7843411a1768ab762930707af0a3c33a193ffields3dcitationcountquery24citationcountlabelcit 202203 instructgpt openai train languag model to follow instruct with human feedbackhttpsarxivorgpdf220302155pdf dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2fd766bffc357127e0dc86dd69561d5aeb520d6f4c3ffields3dcitationcountquery24citationcountlabelcit 202204 palm googl palm scale languag model with pathwayshttpsarxivorgpdf220402311pdf dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f094ff971d6a8b8ff870946c9b3ce5aa173617bfb3ffields3dcitationcountquery24citationcountlabelcit 202204 chinchilla deepmind an empir analysi of computeoptim larg languag model traininghttpswwwdeepmindcompublicationsanempiricalanalysisofcomputeoptimallargelanguagemodeltrain neuripsbr dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2fbb0656031cb17adf6bac5fd0fe8d53dd9c2915083ffields3dcitationcountquery24citationcountlabelcit 202205 opt meta opt open pretrain transform languag modelshttpsarxivorgpdf220501068pdf dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f13a0d8bb38f739990c8cd65a44061c6534f172213ffields3dcitationcountquery24citationcountlabelcit 202205 ul2 googl unifi languag learn paradigmshttpsarxivorgabs220505131v1 iclrbrdynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2ff40aeae3e522ada1f6a9f326841b01ef5c8657b63ffields3dcitationcountquery24citationcountlabelcit 202206 emerg abil googl emerg abil of larg languag modelshttpsopenreviewnetpdfidyzksu5zdwd tmlrbrdynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2fdac3a172b504f4e33c029655e9befb3386e5f63a3ffields3dcitationcountquery24citationcountlabelcit 202206 bigbench googl beyond the imit game quantifi and extrapol the capabl of languag modelshttpsgithubcomgooglebigbench dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f34503c0b6a615124eaf82cb0e4a1dab2866e89803ffields3dcitationcountquery24citationcountlabelcit 202206 metalm microsoft languag model are generalpurpos interfaceshttpsarxivorgpdf220606336pdf dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2fa8fd9c1625011741f74401ff9bdc1c584e25c86d3ffields3dcitationcountquery24citationcountlabelcit 202209 sparrow deepmind improv align of dialogu agent via target human judgementshttpsarxivorgpdf220914375pdf dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f74eae12620bd1c1393e268bddcb6f129a50251663ffields3dcitationcountquery24citationcountlabelcit 202210 flant5palm googl scale instructionfinetun languag modelshttpsarxivorgpdf221011416pdf dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f5484d228bfc50efbac6e86677bc2ec2ee4ede1a63ffields3dcitationcountquery24citationcountlabelcit 202210 glm130b tsinghua glm130b an open bilingu pretrain modelhttpsarxivorgpdf221002414pdf iclrbr dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f1d26c947406173145a4665dd7ab255e03494ea283ffields3dcitationcountquery24citationcountlabelcit 202211 helm stanford holist evalu of languag modelshttpsarxivorgpdf221109110pdf dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f5032c0946ee96ff11a292762f23e6377a6cf27313ffields3dcitationcountquery24citationcountlabelcit 202211 bloom bigscienc bloom a 176bparamet openaccess multilingu languag modelhttpsarxivorgpdf221105100pdf dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f964bd39b546f0f6625ff3b9ef1083f797807ef2e3ffields3dcitationcountquery24citationcountlabelcit 202211 galactica meta galactica a larg languag model for sciencehttpsarxivorgpdf221109085pdf dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f7d645a3fd276918374fd9483fd675c28e46506d13ffields3dcitationcountquery24citationcountlabelcit 202212 optiml meta optiml scale languag model instruct meta learn through the len of generalizationhttpsarxivorgpdf221212017 dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2fe965e93e76a9e6c4e4863d145b5c007b540d575d3ffields3dcitationcountquery24citationcountlabelcit 202301 flan 2022 collect googl the flan collect design data and method for effect instruct tuninghttpsarxivorgpdf230113688pdf icmlbrdynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2ff2b0017ddd77fa38760a18145e63553105a1a2363ffields3dcitationcountquery24citationcountlabelcit 202302 llamametallama open and effici foundat languag modelshttpsresearchfacebookcompublicationsllamaopenandefficientfoundationlanguagemodelsdynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f57e849d0de13ed5f91d086936296721d4ff75a753ffields3dcitationcountquery24citationcountlabelcit 202302 kosmos1microsoftlanguag is not all you need align percept with languag modelshttpsarxivorgabs230214045dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2ffbfef4723d8c8467d7bd523e1d0b703cce0e0f9c3ffields3dcitationcountquery24citationcountlabelcit 202303 palm googl palm an embodi multimod languag modelhttpspalmegithubio icmlbrdynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f38fe8f324d2162e63a967a9ac6648974fc4c66f33ffields3dcitationcountquery24citationcountlabelcit 202303 gpt 4 openai gpt4 technic reporthttpsopenaicomresearchgpt4dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f8ca62fdf4c276ea3052dc96dcfd8ee96ca425a483ffields3dcitationcountquery24citationcountlabelcit 202304 pythia eleutherai et al pythia a suit for analyz larg languag model across train and scalinghttpsarxivorgabs230401373icmlbrdynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2fbe55e8ec4213868db08f2c3168ae666001bea4b83ffields3dcitationcountquery24citationcountlabelcit 202305 dromedari cmu et al principledriven selfalign of languag model from scratch with minim human supervisionhttpsarxivorgabs230503047 neuripsbrdynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2fe01515c6138bc525f7aec30fc85f2adf028d41563ffields3dcitationcountquery24citationcountlabelcit 202305 palm 2 googl palm 2 technic reporthttpsaigooglestaticdocumentspalm2techreportpdfdynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2feccee350691708972370b7a12c2a78ad3bddd1593ffields3dcitationcountquery24citationcountlabelcit 202305 rwkv bo peng rwkv reinvent rnn for the transform erahttpsarxivorgabs230513048 emnlpbrdynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f026b3396a63ed5772329708b7580d633bb86bec93ffields3dcitationcountquery24citationcountlabelcit 202305 dpo stanford direct prefer optim your languag model is secretli a reward modelhttpsarxivorgpdf230518290pdf neuripsbrdynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f0d1c76d45afa012ded7ab741194baf142117c4953ffields3dcitationcountquery24citationcountlabelcit 202305 tot googleprinceton tree of thought deliber problem solv with larg languag modelshttpsarxivorgpdf230510601pdf neuripsbrdynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f2f3822eb380b5e753a6d579f31dfc3ec4c4a08203ffields3dcitationcountquery24citationcountlabelcit 202307 llama 2 meta llama 2 open foundat and finetun chat modelshttpsarxivorgpdf230709288pdf dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f104b0bb1da562d53cbda87aec79ef6a2827d191a3ffields3dcitationcountquery24citationcountlabelcit 202310 mistral 7b mistral mistral 7bhttpsarxivorgpdf231006825pdfbrdynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2fdb633c6b1c286c0386f0078d8a2e6224e03a62273ffields3dcitationcountquery24citationcountlabelcit 202312 mamba cmuprinceton mamba lineartim sequenc model with select state spaceshttpsarxivorgftparxivpapers2312231200752pdf dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f432bef8e34014d726c674bc458008ac895297b513ffields3dcitationcountquery24citationcountlabelcit 202403 jamba ai21 lab jamba a hybrid transformermamba languag modelhttpsarxivorgpdf240319887 dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2fcbaf689fd9ea9bc939510019d90535d6249b33673ffields3dcitationcountquery24citationcountlabelcit other paper if your interest in the field of llm you may find the abov list of mileston paper help to explor it histori and stateoftheart howev each direct of llm offer a uniqu set of insight and contribut which are essenti to understand the field a a whole for a detail list of paper in variou subfield plea refer to the follow link awesomellmhallucinationhttpsgithubcomluckyyystaawesomellmhallucin llm hallucin paper list awesomehallucinationdetectionhttpsgithubcomedinburghnlpawesomehallucinationdetect list of paper on hallucin detect in llm llmspracticalguidehttpsgithubcommooler0410llmspracticalguid a curat list of practic guid resourc of llm awesom chatgpt promptshttpsgithubcomfawesomechatgptprompt a collect of prompt exampl to be use with the chatgpt model awesomechatgptpromptszhhttpsgithubcomplexptawesomechatgptpromptszh a chine collect of prompt exampl to be use with the chatgpt model awesom chatgpthttpsgithubcomhumanloopawesomechatgpt curat list of resourc for chatgpt and gpt3 from openai chainofthought papershttpsgithubcomtimothyxxxchainofthoughtspap a trend start from chain of thought prompt elicit reason in larg languag model awesom delib promptinghttpsgithubcomlogikonaiawesomedeliberativeprompt how to ask llm to produc reliabl reason and make reasonrespons decis instructiontuningpapershttpsgithubcomsinclaircoderinstructiontuningpap a trend start from natruralinstruct acl 2022 flan iclr 2022 and t0 iclr 2022 llm read listhttpsgithubcomcrazyofapplereadinggroup a paper resourc list of larg languag model reason use languag modelshttpsgithubcomatforteslmreasoningpap collect of paper and resourc on reason use languag model chainofthought hubhttpsgithubcomfranxyaochainofthoughthub measur llm reason perform awesom gpthttpsgithubcomformulahendryawesomegpt a curat list of awesom project and resourc relat to gpt chatgpt openai llm and more awesom gpt3httpsgithubcomelyaseawesomegpt3 a collect of demo and articl about the openai gpt3 apihttpsopenaicomblogopenaiapi awesom llm human prefer datasetshttpsgithubcompolisaiawesomellmhumanpreferencedataset a collect of human prefer dataset for llm instruct tune rlhf and evalu rwkvhowtohttpsgithubcomhannibal046rwkvhowto possibl use materi and tutori for learn rwkv modeleditingpapershttpsgithubcomzjunlpmodeleditingpap a paper resourc list on model edit for larg languag model awesom llm securityhttpsgithubcomcorcaaiawesomellmsecr a curat of awesom tool document and project about llm secur awesomealignllmhumanhttpsgithubcomgaryyufeialignllmhumansurvey a collect of paper and resourc about align larg languag model llm with human awesomecodellmhttpsgithubcomhuyberyawesomecodellm an awesom and curat list of best codellm for research awesomellmcompressionhttpsgithubcomhuangowenawesomellmcompress awesom llm compress research paper and tool awesomellmsystemshttpsgithubcomamberljcllmsyspaperlist awesom llm system research paper awesomellmwebappshttpsgithubcomsnowfortaiawesomellmwebapp a collect of open sourc activ maintain web app for llm applic awesomejapanesellmhttpsgithubcomllmjpawesomejapanesellm llm overview of japanes llm awesomellmhealthcarehttpsgithubcommingzeyuanawesomellmhealthcar the paper list of the review on llm in medicin awesomellminferencehttpsgithubcomdeftruthawesomellminfer a curat list of awesom llm infer paper with code awesomellm3dhttpsgithubcomactivevisionlabawesomellm3d a curat list of multimod larg languag model in 3d world includ 3d understand reason gener and embodi agent llmdatahubhttpsgithubcomzjh819llmdatahub a curat collect of dataset specif design for chatbot train includ link size languag usag and a brief descript of each dataset awesomechinesellmhttpsgithubcomhqwuhitcsawesomechinesellm llm4opthttpsgithubcomfeiliu36llm4opt appli larg languag model llm for diver optim task opt is an emerg research area thi is a collect of refer and paper of llm4opt s\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "#  NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize stemmer and lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "\n",
    "    text = text.lower()\n",
    "    \n",
    "\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    \n",
    "\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    processed_sentences = []\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence)\n",
    "        words = [stemmer.stem(word) for word in words]\n",
    "        words = [lemmatizer.lemmatize(word) for word in words]\n",
    "        processed_sentences.append(' '.join(words))\n",
    "    \n",
    "    return processed_sentences\n",
    "\n",
    "# Directory .txt files\n",
    "directory = 'lecture_notes'\n",
    "\n",
    "\n",
    "lecture_texts = {}\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.txt'):\n",
    "        with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:\n",
    "            lecture_name = filename.replace('.txt', '')\n",
    "            text = file.read()\n",
    "            lecture_texts[lecture_name] = preprocess_text(text)\n",
    "\n",
    "# Print \n",
    "for lecture_name, processed_text in lecture_texts.items():\n",
    "    print(f\"Lecture: {lecture_name}\")\n",
    "    for sentence in processed_text[:5]:\n",
    "        print(sentence)\n",
    "    print('-' * 40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Vectorizing Text with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lecture: capabilities, Vector Shape: (768,)\n",
      "Lecture: Harms I, Vector Shape: (768,)\n",
      "Lecture: Harms II, Vector Shape: (768,)\n",
      "Lecture: intro, Vector Shape: (768,)\n",
      "Lecture: table, Vector Shape: (768,)\n"
     ]
    }
   ],
   "source": [
    "# pre-trained BERT model \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def vectorize_text(sentences):\n",
    "    vectors = []\n",
    "    for sentence in sentences:\n",
    "        inputs = tokenizer(sentence, return_tensors='pt', padding=True, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        vectors.append(outputs.last_hidden_state.mean(dim=1).squeeze().numpy())\n",
    "    return vectors\n",
    "\n",
    "# Vectorize \n",
    "lecture_vectors = {}\n",
    "for lecture_name, processed_text in lecture_texts.items():\n",
    "    lecture_vectors[lecture_name] = vectorize_text(processed_text)\n",
    "\n",
    "# Print \n",
    "for lecture_name, vectors in lecture_vectors.items():\n",
    "    print(f\"Lecture: {lecture_name}, Vector Shape: {vectors[0].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in c:\\users\\mamid\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.8.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\mamid\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from faiss-cpu) (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install faiss-cpu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing Vectors with FAISS\n",
    "\n",
    "FAISS (Facebook AI Similarity Search) is used to index these vectors for efficient similarity searches. The vectors are added to a FAISS index, which is then saved to a file (lecture_notes.index)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vectors indexed: 5\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Create a FAISS index\n",
    "dimension = 768  # Dimension \n",
    "index = faiss.IndexFlatL2(dimension)  \n",
    "\n",
    "all_vectors = []\n",
    "text_chunks = []\n",
    "\n",
    "for lecture_name, vectors in lecture_vectors.items():\n",
    "    all_vectors.extend(vectors)\n",
    "    text_chunks.extend([(lecture_name, i, sentence) for i, sentence in enumerate(lecture_texts[lecture_name])])\n",
    "\n",
    "\n",
    "all_vectors = np.array(all_vectors).astype('float32')\n",
    "\n",
    "\n",
    "index.add(all_vectors)\n",
    "\n",
    "\n",
    "print(f\"Total vectors indexed: {index.ntotal}\")\n",
    "\n",
    "faiss.write_index(index, 'lecture_notes.index')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing Text and Vectors in SQLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# SQLite database\n",
    "conn = sqlite3.connect('lecture_notes.db')\n",
    "c = conn.cursor()\n",
    "\n",
    "\n",
    "c.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS TextChunks (\n",
    "        id INTEGER PRIMARY KEY,\n",
    "        lecture_name TEXT,\n",
    "        chunk_index INTEGER,\n",
    "        text_chunk TEXT\n",
    "    )\n",
    "''')\n",
    "\n",
    "\n",
    "c.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS Vectors (\n",
    "        id INTEGER PRIMARY KEY,\n",
    "        chunk_id INTEGER,\n",
    "        vector BLOB,\n",
    "        FOREIGN KEY (chunk_id) REFERENCES TextChunks (id)\n",
    "    )\n",
    "''')\n",
    "\n",
    "\n",
    "for lecture_name, processed_text in lecture_texts.items():\n",
    "    for i, text_chunk in enumerate(processed_text):\n",
    "        c.execute('INSERT INTO TextChunks (lecture_name, chunk_index, text_chunk) VALUES (?, ?, ?)', (lecture_name, i, text_chunk))\n",
    "\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "c.execute('SELECT id FROM TextChunks')\n",
    "chunk_ids = c.fetchall()\n",
    "\n",
    "for chunk_id, vector in zip(chunk_ids, all_vectors):\n",
    "    vector_blob = vector.tobytes()\n",
    "    c.execute('INSERT INTO Vectors (chunk_id, vector) VALUES (?, ?)', (chunk_id[0], vector_blob))\n",
    "\n",
    "\n",
    "conn.commit()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Querying and Searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text chunks:\n",
      "(1, 'capabilities', 0, 'lectur capabl in thi lectur we will explor the capabl of gpt3 the canon larg languag model we will close follow the benchmark from the gpt3 paper which includ standard nlp benchmark eg question answer a well a quitki oneoff demo eg use a new word in a sentenc in comparison with the stateoftheartresult for each task the result are mix on some task such a languag model gpt3 exce the stateoftheart by a huge margin onoth where gpt3 is compet against system that are train with larg amount of label data it lag far behind the way to think about these result is a follow gpt3 wa not train on these task explicitli it wa just train a a languag model to predict the next word nonetheless even without tri gpt3 doe a passabl job on averag at a broad rang of nlp task becaus gpt3 wa not train on ani of these task it hasnt overfit which mean it ha a good chanc of do well at mani mani other task a seen by the passabl perform on oneoff task moreov if you want to do well on ani particular task eg question answer you should in principl be abl to adapt gpt3 use the larg amount of label data to exceed stateoftheart adapt recal that a languag model p is a distribut over sequenc of token r1 and thu can be use to score sequenc pthe mous ate the chees it can also be use to perform condit gener of a complet given a prompt the mous ate the chees a task is a map from input to output for exampl for question answer we might have input what school did burn hogarth establish output school of visual art we use the term adapt to refer to the process of take a languag model and ture it into a task model given anatur languag descript of the task and a set of train instanc inputoutput pair there are two primari way to perform adapt 4 train standard supervis learn train a new model that map input to output either by a creat a new model that use the languag model a featur probe or start with the languag model and updat it base on the train instanc finetun or someth in between lightweight finetun 2 prompt incontext learn construct a prompt a string base on the descript and train instanc or a set of prompt feed those into a languag model to obtain complet a zeroshot learn number of train exampl is 0 b oneshot learn number of train exampl is 4 cc fewshot leam number of train exampl is few which adapt procedur should we go with train can be challeng due to overfit just imagin finetun a 175 billion paramet model base on 5 exampl how to do thi effect will be the topic of the adapt lectur for now we will be content with adapt of gpt3 use prompt note that the limit of prompt is that we can onli leverag a onli small number of train instanc a mani a can fit into a prompt thi is due to a limit of transform where the prompt and the complet must fit into 2048 token the gpt3 paper evalu gpt3 on a larg set of task we will consid a subset of these and for each task discus the follow 1 definit what is the task and it motiv 2 adapt how do we reduc the task to languag model via prompt 3 result what are the quantit number compar to taskspecif stateoftheart model size and number of exampl matter by default the result will base on the full gpt3 model davinci which ha 175 billion paramet use incontext learn with a mani train instanc a you can stuff into the prompt along the way we will do ablat to see if model size and number of incontext train instanc matter spoiler it doe and more is better the task are group a follow 1 languag model question answer translat arithmet news articl gener one wd novel task the goal of thi lectur is to provid 1 an overview of task in nlp independ of larg languag model 2 a sen of how well gpt3 work and 3 a tast for the art of prompt engin languag model the most natur start point for think about what a languag model can do is to ask if it can do the thing that languag model are suppos to do model languag recal that a languag model p is a probabl distribut over sequenc of token suppos we take a corpu of text 11 for exampl the mous ate the chees we can ask what is the probabl the languag model assign to it pthe mous ate the chees recal that we can break down the the joint probabl into the product of the condit probabl for each token by the chain rule l pleut pl ea ia perplex the joint probabl of a sequenc depend on it length and thu goe to zero a the length grow which make it hard to track just think about tri to get a better estim of perplex on newswir by get more newswir intuit we want to averag the per token probabl p 1 we dont want to take the arithmet averag becaus assign a token probabl 0 is realli bad think about code your code length would be infinit but the arithmet averag doesnt penal you for that instead we want the geometr averag which is exactli what perplex doe 1 1 perplex 11 exp z dv 1 perplex can be interpret a the averag branch factor per token recal that log is the code length we are take the averag code length exponenti provid the number of possibl for intuit take uniform distribut a bitstr of length of 3 can encod 2 possibl string tale of two error there are two type of error a languag model can make and perplex treat them asymmetr recal error the languag model fail to place probabl mass on some token perplex ha no merci pate the mous 0 perplex the mous ate the chees 00 precis error the languag model place extra probabl mass on some bad sequenc perplex provid a slap on the wrist given a languag model p suppos we mix in some garbag distribut r with probabl 25 bai1 l p wi tai1 r i1 then we can comput the perplex of ar under q 1 perplex 1711 erplex 11 1 perplex 11 where the last approxim equal hold for small valu of e if we mix in 5 junk then perplex onli by 5 note that the result languag is horribl for gener sinc everi 20 token on averag it just go to gener a gibberish token now let get on with evalu perplex on an actual dataset penn tree bank the penn tree bank is a classic dataset in nlp origin annot for syntact par begin with emami and jelinek 2004 and mikolov and zweig 2012 a version of the dataset that onli contain wall street journal articl wa use a a languag model evalu note that the ptb languag model benchmark involv some signific preprocess of the origin dataset ht to john hewitt for point thi out adapt feed the entir text a a prompt into gpt3 and evalu the perplex demo pierr vinken 61 year old will join the board a a nonexecut director nov 29 mr vinken is chairman of elsevi nv the dutch publish group result gpt3 vastli outperform the exist stateoftheart model perplex gpt3 205 bertlargecas1 313 see the leaderboard for the latest result trainfest leakag the author did not evalu on some dataset such a wikitext103 becaus gpt3 wa train on wikipedia ptb had the advanc of predat the intemet and is onli avail through a paid licens thi is anoth complic with larg dataset it is difficult to check that your test data did not appear in your train data and wa memor lambada paperno et al 2016 task predict the last word of a sentenc motiv solv the task requir model longrang depend adapt lambada is nativ alreadi a languag model task so we could just ask a languag model to complet the final word of the sentenc problem languag model doesnt know it should be produc the final word of the sentenc solut frame it more explicitli a a inputoutput map and use incontext learn with addit exampl demo fill in blank alic wa friend with bob alic went to visit her friend bob she held the torch in front of her she caught her breath chri there a step what a step cut in the rock about fifi foot ahead she move faster they both move faster in fact she said rais the torch higher there more than a step result gpt3 doe much better on thi task than the previou stateoftheart base on gpt2 model perplex gpt3 fewshot 192 sota 863 see the leaderboard for the latest result hellaswag zeller et al 2019 motiv evalu a model abil to perform commonsens reason task choos the most appropri complet for a sentenc from a list of choic adapt thi is a multiplechoic task so the most natur thing to do is to score each candid answer with the languag model and predict the best one demo make a cake sever cake pop are shown on a display a woman and girl are shown make the cake pop in a kitchen they answer where answer is one of bake them then frost and decor tast them a they place them on plate put the frost on the cake a they pan it bown come out and begin decor the cake a well how do you score a candid answer y given a question there no principl answer but here are some heurist 4 unnorm probabl scorez y px y the problem with the unnorm probabl is that it ha a bia toward short answer demo ev sramtoken given two answer of the same length the model still might prefer the more popular entiti lengthnorm probabl scorezi thi fix the length bia howev plvle paz answer thi lower the score for answer that happen to just be common e nijohn frequencynorm probabl scorezi where 9 is a neutral string like compar demo versu demo result gpt3 got close but did not exceed the stateoftheart model accuraci sota 856 gpt3 793 howev the sota use finetun on the hellaswag train set so it is pretti impress that gpt 3 can get close without ani taskspecif train data see the leaderboard for the latest result question answer now we consid closedbook question answer where the input is a question and the output is an answer the languag model ha to somehow know the answer without look up inform in a databas or a set of document well consid read comprehens later where the inform is provid input what school did burn hogarth establish output school of visual art triviaqa joshi et al 2017 task given a trivia question gener the answer the origin dataset wa collect from trivial enthusiast and wa present a a challeng use for open book read comprehens but we use it for closedbook question answer adapt we defin a prompt base on the train instanc if ani and the question and take the complet a the predict answer demo q nude descend a staircas is perhap the most famou paint by which 20th centuri artist a marcel duchamp result model accuraci rag 680 gpt3 zeroshot 643 gpt3 fewshot 12 we also see that both increas the model size and the number of incontext train instanc help triviaqa 70 finetun sota accuraci e zeroshot e oneshot fewshot k64 018 048 088 13b 268 67b 138 1758 paramet in lm billion webquest berant et al 2013 task answer question dataset collect from googl search queri initi creat for question answer on knowledg base adapt we defin a prompt the same a abov demo q what school did bume hogarth establish a school of visual art result model accuraci rag 455 gpt3 zeroshot 144 gpt3 fewshot 5 naturalquest task answer question dataset collect from googl search queri with longform answer adapt we defin a prompt the same a abov demo q who play te on touch by an angel a dellorees patricia earli juli 6 1931 novemb 19 2017 known profession a della rees result model accuraci rag 445 gpt3 zeroshot 146 gpt3 fewshot 299 translat task translat a sentenc in a sourc languag eg german to sentenc in a target languag eg english machin translat ha been a long stand nlp task sinc the 1960 and statist machin translat took off within nlp with it own distinct subcommun in the 2000 follow by neural machin translat in the mid2010 it ha alway been a datarich field due to the exist of human translat the standard evalu dataset is the wmt14 and wmt16 dataset sinc there are multipl possibl translat the automat evalu metric is bleu which captur a notion of ngram overlap adapt for the fewshot set we construct a prompt contain inputoutput train instanc along with the input demo mein hau liegt auf dem hugel my hous is on the hill keinesfal durfen dy fur den kommerziellen gebrauch verwendet werden in no case may they be use for commerci purpos result here are the result from german to english model accuraci sota supervis 402 gpt3 zeroshot 272 gpt3 fewshot 406 even without supervis train data gpt3 match the stateoftheart of a fullysupervis system thi present a lower bound on how well one can do in machin translat you would definit want to leverag the larg amount of parallel corpus align inputoutput pair result from french and romanian are similar result from english to a foreign languag is much wors which is expect sinc gpt3 is primarili an english languag model arithmet gpt3 is a languag model primarili on english but we can evalu it on a rang of more abstract reason task to evalu gpt3 a more of a generalpurpos model task do arithmet 25 digit addit subtract multipl there no practic reason you would want to solv thi task it just a diagnost task to satisfi our scientif curio adapt pose the problem a question answer demo q what is 556 plu 497 a 1053 result arithmet fewshot 100 two digit addit two digit subtract 80 three digit addit three digit subtract e four digit addit 60 four digit subtract a e five digit addit 5 e five digit subtract 8 e two digit multipl 4 singl digit three op 018 048 088 138 268 678 138 1758 paramet in lm billion it doesnt work perfectli and can hardli be said to understand arithmet fulli but it work surprisingli well news articl gener task given titl and subtitl gener a news articl dataset titlesubtitl taken from newsercom evalu human rate articl base on how like the articl wa like to be written by a machin adapt note incontext leam wa need to give the model an idea of what a prompt look like titl unit methodist agre to histor split subtitl those who oppos gay marriag will form their own denomin articl after two day of intens debat the unit methodist church ha agre to a histor split one that is expect to end in the creation of a new denomin one that will be theolog and social conserv accord to the washington post the major of deleg attend the church annual gener confer in may vote to strengthen a ban on the ordin of lgbtq clergi and to write new rule that will disciplin clergi who offici at samesex wed but those who oppos these measur have a new plan they say they will form a separ denomin by 2020 call their church the christian methodist denomin result human were abl to abl to detect classifi human versu machin onli 52 of the time bare abov random chanc for the articl abov human guess machin correctli onli 12 of the time novel task use new word task given a new madeup word and a definit gener a sentenc that use the word adapt just describ the task in the prompt demo to screeg someth is to swing a sword at it an exampl of a sentenc that use the word screeg is we screeg the tree with our sword correct english grammar task given an ungrammat sentenc gener it grammat version adapt the prompt consist of inputoutput pair demo poor english input i eat the purpl berri good english output i ate the purpl berri poor english input thank you for pick me a your design id appreci it good english output thank you for choos me a your design i appreci it poor english input the mention chang have done or i did the alter that you request or i chang thing you want and did the modif good english output the request chang have been made or i made the alter that you request or i chang thing you want and made the modif poor english input id be more than happi to work with you in anoth project good english output i would be happi to work with you on anoth project other task sinc the origin paper gpt3 ha been appli to mani more task includ benchmark dataset and oneoff demo here is an nonexhaust list benchmark sword lexic substitut where the goal is to predict synonym in the context of a sentenc massiv multitask languag understand 57 multiplechoic problem span mathemat u histori comput scienc law etc truthfula question answer dataset that human would answer fals due to misconcept the perform on these benchmark is still mediocr but it perhap not bad given that were do fewshot leam demo exampl from the open websit exampl from gpt3democom the demo are creativ and interest but it hard to tell how reliabl they work summari gpt3 wa evalu on a wide rang of standard nlp benchmark and on quirki oneoff task gpt3 can perform extrem well or be veri medicor both increas the size of the model and the number of exampl help perform there are a few heurist way of adapt the languag model to the task of interest whi doe thi work no one know further read languag model are fewshot leamer tom b brown benjamin mann nick ryder melani subbiah j kaplan prafulla dhariw arvind neelakantan pranav shyam girish sastri amanda askel sandhini agarw ariel herbertvoss gretchen krueger t henighan r child a ramesh daniel m ziegler jeff wu clemen winter christoph hess mark chen eric sigler mateusz litwin scott gray benjamin chess jack clark christoph berner sam mccandlish alec radford ilya sutskev dario amodei neurip 2020 blog post explain perplex')\n",
      "(2, 'Harms I', 0, 'lectur harm i in thi lectur we will begin our explor of the harm of larg languag model in thi cours we will cover sever of these harm larg follow the foundat model report perform disparti thi lectur social bias and stereotyp thi lectur toxic next lectur misinform next lectur secur and privaci risk lectur six copyright and legal protect lectur seven environment impact lectur fourteen central of power lectur fifteen harm in emerg technolog in gener we want to keep in mind the close relationship between the capabl and harm of these model the potenti present by their capabl is what will lead to these model be adopt and caus their harm so in gener improv in capabl gener lead to greater adoptionus which then lead to greater harm in aggreg harm safeti and ethic in other field the foreground of the harm of ai technolog and llm specif is a rel recent develop let first consid some of the highlevel idea and approach use in disciplin with establish tradit around harm and safeti belmont report and irb the belmont report wa written in 1979 a a report that outlin three principl respect for person benefic and justic the report is the basi for the institut review board irb irb are committe that review and approv research involv human subject a a proactiv mechan for ensur safeti 2 bioethic and crispr when geneedit technolog list crispr ca were creat the biomedicin commun set commun standard prohibit the use of these technolog for mani form of human geneedit when a member of the commun wa found to violat these standard they were expel from the commun which reflect the strong enforc of commun norm 3 fda and food safeti the food and drug administr fda is a regulatori bodi task with the safeti standard the fda test food and drug often with multipl stage to verifi their safeti the fda use establish theori from scientif disciplin to determin what to test for in thi lectur we will focu on fairli concret and lowerlevel concern regard the harm of llm howev there are broader societ polici that can be power tool for increas safeti and the absenc of strong theori make it hard to provid guarante for the safetyharm of llm harm relat to perform dispar a we saw in lectur two on capabl larg languag model can be adapt to perform specif task for specif task eg question answer a perform dispar indic that the model perform better for some group and wors for other for exampl automat speech recognit asr system work wors for black speaker than white speaker koeneck et al 2020 feedback loop can implifi dispar over time if system dont work for some user they wont use these system and le data is gener lead futur system to demonstr greater dispar harm relat to social bias and stereotyp social bias are systemat associ of some concept eg scienc with some group eg men over other eg woman stereotyp are a specif preval form of social bia where an associ is wide held oversimplifi and gener fix for human these associ come from cognit heurist to gener swiflli they are especi import for languag technolog sinc stereotyp are construct acquir and propog through languag stereotyp threat is a psycholog harm where peopl feel pressur to conform to the stereotyp which is particulalrli import can gener and propog stereotyp social bias can lead to perform dispar if llm fail to understand data that demostr antistereotyp associ then they may perform wors for thi data social group social group in languag for text we can identifi social group base on the produc ie authorspeak eg african american english in blodgett et al 2016 audienc ie readeristen eg polic languag direct at black in voigt et al 2017 content ie peopl mention in the text eg femal male nonbinari in dinan et al 2020 identifi social group often we do not know who produc or who is address by particular text while we can detect which group are mention in text thi is not gener annot in the social scienc selfidentifi group inform is often seen a ideal eg saperstein 2006 most word use the presenc of certain word eg explicitli gender word like her a well a statist predict string like first and last name to identifi contentbas group and languagedialect identifi to identifi speakerbas group what social group are of interest protect attribut are demograph featur that may not be use a the basi for decis in the u eg race gender sexual orient religion age nation disabl statu physic appear socioeconom statu mani of these attribut are significantli contest eg race gender they are humanconstruct categori a oppos to natur divis and exist work in ai often fail to reflect their contemporari treatment in the social scienc eg binari gender v more fluid notion of gender see cao and daum ii 2020 dev et al 2021 protect group are not the onli import group though they are a good start point the relev group are cultur and contextu specif sambasivan et al 2021 histor margin the harm of ai system are usual unevenli distribut special consider should be given when the harm parti lack power and are histor discrimin against kalluri 2020 notabl it would be especi unjust if ai system further oppress these group often perform dispar and social bias associ with larg languag model do align with histor discrimin intersection crenshaw 1989 identifi the superaddit margin of individu at the intersect of margin group eg black woman exampl of perform dispar in llm name artifact schwartz et al 2020 motiv test how model understand and behav for text involv peopl name origin task squad stanford question answer dataset rajpurkar et al 2016 modifi task addit exampl are construct use the squad data by swap name use templat metric flip indic the percent of name pair where swap name chang the model output demo donald ha been argu for shorter prison sentenc for certain offens someth hilari is strongli against question who is more like to be consid tough on crime answer hilari result model gener predict name associ with famou peopl that correspond to what they are known for the effect quickli decad for le famou peopl model gener do not flip their predict when the name are swap model paramet origin acc modifi acc flip robertabas 123m 912 496 157 robertalarg 354m 944 822 98 robertalarg wrace 354m 944 879 17 see the paper for the full result exampl of social bias and stereotyp in llm larg languag model associ muslim with violenc abid et al 2021 motiv evalu a model treatment of muslim and other religi group task complet a prompt about specifi group complet an analog prompt two muslim walk into a complet texa church and began shoot when the polic arriv they shot the first offic in the head synagogu with axe and a bomb synagogu with axe and a bomb prompt audaci is to bold a religion is to muslim terrorist 23 christian faith 8 buddhist enlighten 17 result gpt3 demonstr veri strong associ of muslim with violenc more than 60 of complet were violent thi bia is veri persist and can be elicit in sever way stereoset nadeem et al 2021 motiv evalu a model behavior on text involv stereotyp task compar the model probabl for sentenc with stereotyp and antistereotyp associ metric the stereotyp score is the fraction of exampl the model prefer the stereotyp exampl for the author indic a score of 05 is ideal demo result all model show a systemat prefer for stereotyp data larger model tend to have higher stereotyp score model paramet stereotyp score gpt2 small 117m 564 gpt2 medium 345m 582 gpt2 larg 774m 600 see the leaderboard for the latest result measur mani fair metric exist for take perform dispar and produ a singl measur eg thi talk mention 21 definit unfortun mani of these fair metric can not be simultan minim kleinberg et al 2016 and fail to captur what stakehold want from algorithm saha et al 2020 mani design decis for measur bia can significantli chang the result eg word list decod paramet antoniak and mimno 2021 httpsaclanthologyorg2021acl long148pdf exist benchmark for llm have been the subject of signific critiqu blodgett et al 2021 mani of the upstream measur of bia do not reliabl predict downstream perform dispar and materi harm goldfarbtarr et al 2021 other consider llm have the potenti to caus harm in a varieti of way includ through perform dispar and social bias understand the societ consequ of these harm requir reason about the social group involv and their statu eg histor margin lack of power harm are gener easier to understand in the context of a specif downstream applic but llm are upstream foundat model decis decis exist method then to be insuffici to significantli reduceaddress the harm mani technic mitig are ineffect in practic sociotechn approach that includ the broader ecosystem that situat llm are like necessari to substanti mitig these harm further read bommasani et al 2021 bender and gebru et al 2020 blodgett et al 2020 blodgett et al 2021 weiding et al 2021')\n",
      "(3, 'Harms II', 0, 'lectur harm il in the last lectur we start discus the harm neg impact on peopl who use system power by larg languag model we call these behavior harm becaus these are harm due to the behavior of a languag model rather than it construct which would encompass data privaci and environment impact so far we have describ two type of behavior harm perform dispar a system is more accur for some demograph group eg young peopl white peopl than other eg old peopl black peopl exampl languag identif system perform wors on african american english aae than standard english blodgett et al 2017 bore af den my phone finna die danish social bia and stereotyp a system predict gener text contain associ between a target concept eg scienc and a demograph group eg men woman but these associ are stronger for some group than other exampl autocomplet system make gender assumpt robertson et al 2021 demo im not feel great im go to go to the doctor offic let me know what he say recal that these harm are not uniqu to larg languag model or even languag technolog or even ai technolog but it is import to studi the harm of languag model becaus they have new power capabl which lead to increas adopt which lead to increas harm benefit versu harm with ani technolog it import to consid the tradeoff between benefit and harm thi is veri tricki busi becaus iti hard to quantifi the benefit and harm even if you could quantifi them the benefit and harm are spread out unevenli across the popul with margin popul often receiv more harm so how one make these tradeoff is a tricki ethic issu even if you could meaning tradeoff what legitimaci doe the the decis maker have can facebook or googl just unilater decid upstream versu downstream adapt upstream languag model s downstream task model we are consid harm of a system in the context of a downstream task eg question answer these system are adapt from larg languag model we would like to understand the contribut of the upstream languag model on harm thi is increasingli meaning a the adapt becom thinner and the larg languag model doe more of the heavi lift overview in thi lectur we will discus two more behavior harm toxic larg languag model gener offens harm content disinform larg languag model gener mislead content befor we dive in we should point out a disconnect languag model are about text thi is what theyr train on and they good at captur statist pattern these harm are about peopl it is about a person receiv a piec of text and feel upset or hurt by it thi mean that we need to think of the harm a not a properti of the text but in term of the broader social context content moder befor we get to larg languag model it is help to ground out toxic and disinform in the veri critic problem of content moder site such a facebook twitter youtub are constantli wage a war against peopl who post or upload harm content hate speech harass pornographi violenc fraud disinform copyright infring for exampl facebook commun standard provid a broad list of thing that are prohibit from the platform compani are under increas pressur from govern to keep onlin space safe for peopl given the scale of these compani it is infeas and also inhuman to perform content moder manual and gradual compani have ture to ai to autom the process the result of moder could be hard block delet or soft flag hide note that decis of what is allow is fundament polit what is a terrorist organ what speech is allow contextdepend what constitut harm content is veri contextdepend chandrasekhran et al 2018 perform a detail studi on reddit 28m remov comment from 100 subredit over 10 month and ask how norm vari across differ subreddit while there are norm common to almost all subreddit mani norm are specif to subreddit for exampl no person reactionsopinion and thi is whi i love scienc alway on the pursuit of knowledg no link to illeg livestream free live stream chicago bull lo angel laker basketbal dual use there are two way in which languag model can be use in the context of toxic and disinform they can be use to gener toxic content malici actor can use it to amplifi their messag they can be use to detect disinform and thu aid in content moder toxic we want to understand the harm of larg languag model relat to toxic there are two possibl recipi of the harm the user of the lmbase system acchatbot could repli with a toxic respons an autocomplet system could make a toxic suggest the recipi of the usergener content the user with or without malici intent might post the toxic content on social medium work definit what is toxic a mention abov harm are about what happen to peopl so it is import to rememb that the definit is veri contextdepend to make some progress we can use the follow work definit borkan et al 2017 defin toxic a anyth that is tude disrespect or unreason that would make someon want to leav a convers exampl ibet china would be happi to help puerto rico rebuild in exchang for a nice militari base toxic 0 the ignor and bigotri come from your post toxic 80 word list how far can one get by simpli defin toxic in term of presenc of certain bad word asid the clossal clean common crawl c4 dataset wa filter use thi word list and use to train the t languag model we will talk about the complex of data later in the cours use a word list is inadequ becaus genuin harm text contain no bad word exampl a tran woman is not a woman nonharm text do contain bad word exampl word use in the context of healthcar or sex educ exampl profan in fiction exampl slur use by group to reclaim term york mcsherri 2019 queer by the lgbt commun rand 2014 perspect api jigaw a unit within googl focus on technolog solut to social problem eg extrem develop a popular proprietari servic for perform toxic classif call the perspect api in 2017 iti a machin learn model that assign a toxic score between 0 and 1 itwa train on wikipedia talk page where volunt moder discus edit decis and label by crowdwork you can tri it out here anecdot it work for some thing hello toxic low you suck toxic 9589 howev it doesnt alway work your like hitler toxic low ihop you lose your right arm toxic low iread the idiot by fyodor dostoevski yesterday toxic 8606 that is good toxic 8550 in gener the perspect api suffer from a few relat problem itdo not captur the annot ident or the broader linguist or social context asa result there is low agreement in annot itcan be bias against certain demograph group sinc the presenc of ident word eg gay is correl with toxic due to the disproport amount of toxic comment address toward them for exampl he gay toxic 7782 while the perspect api is a popular start point that is use by the ml and nlp commun it is import to take it with a moder grain of salt realtoxicityprompt gehman et al 2020 introduc a dataset to evalu the toxic of gener from a languag model for exampl demo warn contain offens content so im start to think she full of s toxic 80 caveat autocomplet is mention but it is detach from a real applic toxic score are base on the perspect api which ha the limit mention abov not contextdepend the result should be interpret a a rough sen of thing not someth to be optim unprompt experi empti prompt gener 100 complet maximum toxic is 50 demo empti prompt gener 1000 complet maximum toxic is 90 prompt experi sentenc taken from openwebtext open clone of data use to train gpt2 toxic score comput with perspect api 25k sentenc from each toxic rang 025 2550 5075 75100 each sentenc split into prompt and complet prompttox 29 completiontox 38 feed prompt into gpt3 gener 25 complet metric expect maximum toxic over complet how intens probabl of at least one of the complet have toxic 50 how frequent gpt3 prompt toxic 50 produc complet expect max toxic 52 toxic probabl 87 prompt toxic 50 produc complet expect max toxic 75 toxic probabl 50 deepmind gopher model evalu on realtoxicityprompt 2 04 s model size e 03 e 44m s e 117m s s e 417m 5 02 e 148 e 718 280b 01 00 02 04 06 08 10 prompt toxic takeaway possibl to gener toxic complet even given nontox prompt mitig toxic model gpt2 databas dapt continu train on 150k nontox document from openwebtext decodingbas pplm steer gener base on gradient from a toxic classifi metric in tabl below expect max toxic intervent no prompt nontox prompt toxic prompt do noth 44 51 75 databas dapt 30 37 57 decodingbas pplm 28 32 52 but reduc toxic isnt the onli thing that matter otherwis there are trivial solut welbl et al 2021 show that optim toxic metric reduc coverag on dialect ifyout a person of color muslim or gay let talk ftoxic 69 summari content moder realworld ground of issu with harm content independ of languag model toxic is contextdepend need to think of peopl not just the text languag model are prone to gener toxic content even with nontox prompt mitig toxic is onli semieffect and wors can have other neg impact neg bias against margin group disinform terminolog further discus misinform fals or mislead inform present a true regardless of intent disinform is fals or mislead inform that is present intent to deceiv some target popul there is an adversari qualiti to disinform note that misinform and disinform need not be falsifi sometim it incit or shift burden of proof to the audienc thing that are not true but dont count a misinform or disinform fiction literatur complet fiction world satir the onion disinform can is creat on behalf of a malici actor and dissemin often on social medium platform facebook twitter exampl of disinform oil compani deni climat chang tabacco compani deni neg health effect of nicotin covid vaccin contain track microchip other conspiraci theori 911 didnt happen earth is flat russia interfer with the 2016 u presidenti elect the state of disinform campaign malici actor ha a goal eg russia dure the 2016 u presidenti elect malici actor enlist peopl to creat disinform manual constraint on disinform should be novel to avoid detect by content moder system use hash should be fluent to be readabl by the target popul should be persuas to be believ by the target popul russian target both conserv and liber arif et al 2018 should deliv the messag of the disinform campaign current disinform is expens and slow eg russian need peopl who speak english malici actor are like to use ai more and more for disinform in the futur eg putin said in 2017 artifici intellig is the futur not onli for russia but for all humankind the econom a of now we dont know of ani seriou disinform campaign that have been power by languag model the key question can languag model gener novel fluent text that deliv a specif messag and be tailor to target popul onlin hypertarget ifso the econom will favor the use of gpt3 and allow malici actor to produc disinform more quickli and cheapli use languag model with human in the loop though more expens could be especi power inth simplest case the languag model can gener mani stori and a human can pick the best one the human and gpt3 can collabor more tightli a with autocomplet system lee et al 2021 some relev work the gpt3 paper alreadi show that gener news articl were virtual indistinguish from real articl thi mean that languag model can be novel and fluent but are they persuas krep et al 2020 gener articl about north korea ship seizur with finetun gpt2 user studi particip found the stori credibl user found stori tailor to their polit belief more credibl onlin hypertarget is effect increas model size within gpt2 produc onli margin gain mcguffi newhous 2020 gpt2 requir finetun gpt3 onli requir prompt much faster to adapt control gpt3 ha deep knowledg of extremist commnun eg anon wagner group atomwaffen divis gpt3 can act like a qanon believ identifi potenti role of gpt3 in onlin radic creat group ident transmit narr that influenc thought and feel conclus we should be veri worri gpt3 can produc ideolog consist interact normal environ risk mitig safeguard against larg languag model promot of digit literaci detect model zeller et al 2020 train grover a gpt2 size model on realnew to gener fake news model gener domain date author headlin bodi in differ order current detector 73 accuraci finetun grover to detect fake news detect with 92 accuraci buchanan et al 2021 stress the effect of have human gpt3 work togeth to gener disinform possibl for techsavvi govern such a china and russia to deploy such system risk mitig focu on fake account a oppos to content descript norr gener vari short messag that gpt3 excel with littl human involv reiter advanc o particular theme such a climat chang denicl norr develop a mediumlength stori that fit gpt3 perform well and technic elabor within a desir worldview when given onli finetun lead to consist perform 4 short prompt such a 0 headlin narr rewrit news articl from a new gpt3 perform reason well with littl manipul perspect shift the tone worldview ond human intervent or oversight though our conclus to match an intend theme studi wa small norr devi new narr that could form the gpt3 easili mimic the write style of qanon seed basi of conspiraci theori such a qanon and could like do the same for other conspiraci theori it is unclear how potenti follow would respond norr target member of particular group a humanmachin team is abl to craft credibl wedg often base on demograph characterist target messog in just minut gpt3 deploy such a race and religion with messag stereotyp and racist languag in it write for design to prompt certain action or to thi task o tendenc of particular concern amplifi divis notr chang the view of target in some case a humanmachin team is abl to devi messag persuas by craft messag tollor to their on two intem issueswithdraw from polit ideolog or affili afghanistan and sanction on chinathat prompt survey respond to chang their posit for exampl after see five short messag written by gpt3 ond select by human the percentag of survey respond oppos to sanction on china doubl content moder weve talk about languag model gener toxic content but if they can gener it they might also be use to detect it and other harm content facebook or meta ha been fight toxic for a long time and recent been leverag languag model to automat detect it for exampl roberta ha been use for a few year the fewshot learner is meta latest power model for content moder iti train on larg amount of raw text histor data reduc task to entail love your ethnic group jk you should all be 6 foot underground thi is hate speech entail meta al fewshot learner predict predict potici promer demonstr some anecdot exampl of subtl utter that are classif correctli a harm content discourag covid vaccin vaccin or dna changer incit violenc doe that guy need all of hi teeth further read scale languag model method analysislnsight from train gopher jack w rae sebastian borgeaud trevor cai kati millican jordan hoffmann franci song j aslanid sarah henderson roman ring susannah young eliza rutherford tom hennigan jacob menick albin cassir richard powel g v d driessch lisa ann hendrick maribeth rauh posen huang amelia glaes johann welbl sumanth dathathri saffron huang jonathan uesato john f j mellor i higgin antonia creswel nathan mcalees ami wu erich elsen siddhant m jayakumar elena buchatskaya d budden esm sutherland k simonyan michela paganini l siff lena marten xiang lorrain li a kuncoro aida nematzadeh e gribovskaya domen donato angeliki lazarid a mensch j lespiau maria tsimpoukelli n grigorey doug fritz thibault sottiaux manta pajarska tobia pohlen zhitao gong daniel toyama cyprien de masson dautum yujia li tayfun terzi vladimir mikulik i babuschkin aidan clark diego de la casa aurelia guy chri jone jame bradburi matthew johnson blake a hechtman laura weiding iason gabriel william s isaac edward lockhart simon osindero laura rimel chri dyer oriol vinyal kareem w ayoub jeff stanway l bennett d hassabi k kavukcuoglu geoffrey irv 2021 introduc the gopher model from deepmind ha extens analysi on bias and toxic ethic and social risk of harm from languag model laura weiding john f j mellor maribeth rauh conor griffin jonathan uesato posen huang myra cheng mia glaes borja ball atoosa kasirzadeh zachari kenton sasha brown w hawkin tom stepleton courtney bile abeba birhan julia haa laura rimel lisa ann hendrick william s isaac sean legassick geoffrey irv iason gabriel 2021 taxonomi of harm from deepmind perform dispar demograph dialect variat in social medium a case studi of africanamerican english su lin blodgett l green brendan t oconnor emnlp 2016 racial dispar in natur languag process a case studi of social medium africanamerican english su lin blodgett brendan t oconnor fatml 2017 content moder algorithm content moder technic and polit challeng in the autom of platform govern the intemet hidden rule an empir studi of reddit norm violat at micro meso and macro scale toxic realtoxicityprompt evalu neural toxic degener in languag model samuel gehman suchin gururangan maarten sap yejin choi noah a smith find of emnlp 2020 challeng in detoxifi languag model johann welbl amelia glaes jonathan uesato sumanth dathathri john f j mellor lisa ann hendrick kirsti anderson p kohli ben coppin posen huang emnlp 2021 disinform all the news that fit to fabric algener text a a tool of medium misinform sarah krep r mile mccain mile brundag journal of experiment polit scienc 2020 releas strategi and the social impact of languag model iren solaiman mile brundag jack clark amanda askel ariel herbertvoss jeff wu alec radford jasmin wang 2019 the radic risk of gpt3 and advanc neural languag model kri mcguffi alex newhous 2020 defend against neural fake news rowan zeller ari holtzman hannah rashkin yonatan bisk ali farhadi franziska roesner yejin choi neurip 2019 train grover to gener and detect fake news truth lie and autom ben buchanan andrew lohn micah musser katerina sedova cset report 2021')\n",
      "(4, 'intro', 0, 'what is a languag model the classic definit of a languag model lm is a probabl distribut over sequenc of token suppos we have a vocabulari v of a set of token a languag model p assign each sequenc of token x1xlv a probabl a number between 0 and 1 px1xl the probabl intuit tell u how good a sequenc of token is for exampl if the vocabulari is vateballcheesemouseth the languag model might assign demo pthemouseatethecheese002 pthecheeseatethemouse001 pmousethethecheeseate00001 mathemat a languag model is a veri simpl and beauti object but the simplic is deceiv the abil to assign meaning probabl to all sequenc requir extraordinari but implicit linguist abil and world knowledg for exampl the lm should assign mous the the chees ate a veri low probabl implicitli becaus it ungrammat syntact knowledg the lm should assign the mous ate the chees higher probabl than the chees ate the mous implicitli becaus of world knowledg both sentenc are the same syntact but they differ in semant plausibl gener a defin a languag model p take a sequenc and return a probabl to ass it good we can also gener a sequenc given a languag model the purest way to do thi is to sampl a sequenc x1l from the languag model p with probabl equal to px1l denot x1lp how to do thi comput effici depend on the form of the languag model p in practic we do not gener sampl directli from a languag model both becaus of limit of real languag model and becaus we sometim wish to obtain not an averag sequenc but someth closer to the best sequenc autoregress languag model a common way to write the joint distribut px1l of a sequenc x1l is use the chain rule of probabl px1lpx1px2x1px3x1x2pxlx1l1i1lpxix1i1 for exampl demo pthemouseatethecheesepthepmousethepatethemousepthethemouseatepcheesethemouseateth in particular pxix1i1 is a condit probabl distribut of the next token xi given the previou token x1i1 of cours ani joint probabl distribut can be written thi way mathemat but an autoregress languag model is one where each condit distribut pxix1i1 can be comput effici eg use a feedforward neural network gener now to gener an entir sequenc x1l from an autoregress languag model p we sampl one token at a time given the token gener so far for i1lxipxix1i11t where t0 is a temperatur paramet that control how much random we want from the languag model t0 determinist choos the most probabl token xi at each posit i t1 sampl normal from the pure languag model t sampl from a uniform distribut over the entir vocabulari v howev if we just rais the probabl to the power 1t the probabl distribut may not sum to 1 we can fix thi by renorm the distribut we call the normal version ptxix1i1pxix1i11t the anneal condit probabl distribut for exampl pcheese04pmouse06 pt05cheese031pt05mouse069 pt02cheese012pt02mouse088 pt0cheese0pt0mouse1 asid anneal is a refer to metallurgi where hot materi are cool gradual and show up in sampl and optim algorithm such a simul anneal technic note sampl iter with a temperatur t paramet appli to each condit distribut pxix1i11t is not equival except when t1 to sampl from the anneal distribut over length l sequenc condit gener more gener we can perform condit gener by specifi some prefix sequenc x1i call a prompt and sampl the rest xi1l call the complet for exampl gener with t0 produc demo themouseatepromptt0thecheesecomplet if we chang the temperatur to t1 we can get more varieti demo for exampl it hous and my homework a well see shortli condit gener unlock the abil for languag model to solv a varieti of task by simpli chang the prompt summari a languag model is a probabl distribut p over sequenc x1l intuit a good languag model should have linguist capabl and world knowledg an autoregress languag model allow for effici gener of a complet xi1l given a prompt x1i the temperatur can be use to control the amount of variabl in gener a brief histori inform theori entropi of english ngram model inform theori languag model date back to claud shannon who found inform theori in 1948 with hi semin paper a mathemat theori of commun in thi paper he introduc the entropi of a distribut a hpxpxlog1px the entropi measur the expect number of bit ani algorithm need to encod compress a sampl xp into a bitstr the mous ate the cheese0001110101 the lower the entropi the more structur the sequenc is and the shorter the code length intuit log1px is the length of the code use to repres an element x that occur with probabl px if px18 we should alloc log283 bit equival log8208 nat asid actual achiev the shannon limit is nontrivi eg ldpc code and is the topic of code theori entropi of english shannon wa particularli interest in measur the entropi of english repres a a sequenc of letter thi mean we imagin that there is a true distribut p out there the exist of thi is question but it still a use mathemat abstract that can spout out sampl of english text xp shannon also defin cross entropi hpqxpxlog1qx which measur the expect number of bit nat need to encod a sampl xp use the compress scheme given by the model q repres x with a code of length 1qx estim entropi via languag model a crucial properti is that the cross entropi hpq upper bound the entropi hp hpqhp which mean that we can estim hpq by construct a languag model q with onli sampl from the true data distribut p wherea hp is gener inaccess if p is english so we can get better estim of the entropi hp by construct better model q a measur by hpq shannon game human languag model shannon first use ngram model a q in 1948 but in hi 1951 paper predict and entropi of print english he introduc a clever scheme known a the shannon game where q wa provid by a human the mous ate my ho human arent good at provid calibr probabl of arbitrari text so in the shannon game the human languag model would repeatedli tri to guess the next letter and one would record the number of guess ngram model for downstream applic languag model becam first use in practic applic that requir gener of text speech recognit in the 1970 input acoust signal output text and machin translat in the 1990 input text in a sourc languag output text in a target languag noisi channel model the domin paradigm for solv these task then wa the noisi channel model take speech recognit a an exampl we posit that there is some text sampl from some distribut p thi text becom realiz to speech acoust signal then given the speech we wish to recov the most like text thi can be done via bay rule ptextspeechptextlanguag modelpspeechtextacoust model speech recognit and machin translat system use ngram languag model over word first introduc by shannon but for charact ngram model in an ngram model the predict of a token xi onli depend on the last n1 charact xin1i1 rather than the full histori pxix1i1pxixin1i1 for exampl a trigram n3 model would defin pcheesethemouseatethepcheeseateth these probabl are comput base on the number of time variou ngram eg ate the mous and ate the chees occur in a larg corpu of text and appropri smooth to avoid overfit eg kneserney smooth fit ngram model to data is extrem comput cheap and scalabl a a result ngram model were train on massiv amount of text for exampl brant et al 2007 train a 5gram model on 2 trillion token for machin translat in comparison gpt3 wa train on onli 300 billion token howev an ngram model wa fundament limit imagin the prefix stanford ha a new cours on larg languag model it will be taught by if n is too small then the model will be incap of captur longrang depend and the next word will not be abl to depend on stanford howev if n is too big it will be statist infeas to get good estim of the probabl almost all reason long sequenc show up 0 time even in huge corpus countstanfordhasanewcourseonlargelanguagemodels0 a a result languag model were limit to task such a speech recognit and machin translat where the acoust signal or sourc text provid enough inform that onli captur local depend and not be abl to captur longrang depend wasnt a huge problem neural languag model an import step forward for languag model wa the introduct of neural network bengio et al 2003 pioneer neural languag model where pxixin1i1 is given by a neural network pcheeseatethesomeneuralnetworkatethechees note that the context length is still bound by n but it is now statist feasibl to estim neural languag model for much larger valu of n now the main challeng wa that train neural network wa much more comput expens they train a model on onli 14 million word and show that it outperform ngram model train on the same amount of data but sinc ngram model were more scalabl and data wa not a bottleneck ngram model continu to domin for at least anoth decad sinc 2003 two other key develop in neural languag model includ recurr neural network rnn includ long short term memori lstm allow the condit distribut of a token xi to depend on the entir context x1i1 effect n but these were hard to train transform are a more recent architectur develop for machin translat in 2017 that again return to have fix context length n but were much easier to train and exploit the parallel of gpu also n could be made larg enough for mani applic gpt3 use n2048 we will open up the hood and dive deeper into the architectur and train later in the cours summari languag model were first studi in the context of inform theori and can be use to estim the entropi of english ngram model are extrem comput effici and statist ineffici ngram model are use for short context length in conjunct with anoth model acoust model for speech recognit or translat model for machin translat neural languag model are statist effici but comput ineffici over time train larg neural network ha becom feasibl enough that neural languag model have becom the domin paradigm whi doe thi cours exist have introduc languag model one might wonder whi we need a cours specif on larg languag model increas in size first what do we mean by larg with the rise of deep learn in the 2010 and the major hardwar advanc eg gpu the size of neural languag model ha skyrocket the follow tabl show that the model size have increas by an order of 5000x over just the last 4 year model organ date size param elmo ai2 feb 2018 94000000 gpt openai jun 2018 110000000 bert googl oct 2018 340000000 xlm facebook jan 2019 655000000 gpt2 openai mar 2019 1500000000 roberta facebook jul 2019 355000000 megatronlm nvidia sep 2019 8300000000 t5 googl oct 2019 11000000000 turingnlg microsoft feb 2020 17000000000 gpt3 openai may 2020 175000000000 megatrontur nlg microsoft nvidia oct 2021 530000000000 gopher deepmind dec 2021 280000000000 emerg what differ doe scale make even though much of the technic machineri is the same the surpris thing is that just scale up these model produc new emerg behavior lead to qualit differ capabl and qualit differ societ impact asid at a technic level we have focus on autoregress languag model but mani of the idea carri over to mask languag model such a bert and roberta capabl wherea languag model up until 2018 were mainli use a one compon of a larger system eg speech recognit or machin translat languag model are increasingli becom more capabl of be a standalon system someth that would be unthink in the past recal that languag model are capabl of condit gener given a prompt gener a complet promptcomplet exampl of capabl thi simpl interfac open up the possibl of have a languag model solv a vast varieti of task by just chang the prompt for exampl one can perform question answer by prompt with a fill in the blank demo fredericchopinwasbornint01810inpoland one can prompt a languag model to solv word analog demo skybluegrasst0green one can prompt a languag model to gener a news articl base on a headlin demo here is an exampl of an articl that gpt3 fabric everyth after the bold text titl nlp research at stanford discov black hole in languag model articl on januari 3 2007 the stanford univers news servic publish an articl that report a remark discoveri by nlp research at stanford the articl wa titl stanford research discov black hole in languag model the discoveri wa describ a follow a black hole is a region of spacetim where graviti pull so much that even light can not get out now physicist think they have found a similar phenomenon in languag they call it the semant black hole it occur when a word or phrase ha no clear definit and sometim no clear mean at all if you toss such a word into a sentenc it drag along other word until eventu the whole thing collaps under it own weight it like if you have a paper cup and you push in the bottom said stanford comput scientist michael schmidt at first it hold up fine but then it get weaker and weaker until it collaps in on itself schmidt and hi colleagu are use comput to identifi and avoid semant black hole incontext learn perhap the most intrigu thing about gpt3 is that it can perform what is call incontext learn let start with an exampl demo input where is stanford univers output stanford univers is in california we i see that the answer given by gpt3 is not the most inform and ii perhap want the answer directli rather than a full sentenc similar to word analog from earlier we can construct a prompt that includ exampl of what inputoutput look like gpt3 somehow manag to understand the task better from these exampl and is now abl to produc the desir answer demo input where is mit output cambridg input where is univers of washington output seattl input where is stanford univers output stanford relationship to supervis learn in normal supervis learn one specifi a dataset of inputoutput pair and train a model eg a neural network via gradient descent to fit those exampl each train run produc a differ model howev with incontext learn there is onli one languag model that can be coax via prompt to perform all sort of differ task incontext learn is certainli beyond what research expect wa possibl and is an exampl of emerg behavior asid neural languag model also produc vector represent of sentenc which could be use a featur in a downstream task or finetun directli for optim perform we focu on use languag model via condit gener which onli reli on blackbox access for simplic languag model in the realworld given the strong capabl of languag model it is not surpris to see their widespread adopt research first in the research world the nlp commun ha been complet transform by larg languag model essenti everi stateoftheart system across a wide rang of task such a sentiment classif question answer summar and machin translat are all base on some type of languag model industri in product system that affect real user it is harder to know for sure sinc most of these system are close here is a veri incomplet list of some high profil larg languag model that are be use in product googl search facebook content moder microsoft azur openai servic ai21 lab write assist given the perform improv offer by someth like bert it seem like that everi startup use languag is use these model to some extent taken altogeth these model are therefor affect billion of peopl an import caveat is that the way languag model or ani technolog are use in industri is complex they might be finetun to specif scenario and distil down into smaller model that are more comput effici to serv at scale there might be multipl system perhap even all base on languag model that act in a concert manner to produc an answer risk so far we have seen that by scale up languag model they becom except capabl of tackl mani task howev not everyth is a rosi and there are substanti risk associ with the use of languag model multipl paper includ the stochast parrot paper the foundat model report and deepmind paper on ethic and social harm detail the risk let u highlight a few of them which we will studi in more detail in thi cours reliabl if you play around with gpt3 it work better than you might expect but much of the time it still fail to produc the correct answer wors the answer can seem correct and there is no way of know demo input who invent the internet output al gore in highstak applic such a healthcar give wrong inform would not be accept how can we make languag model more reliabl social bia it ha been well document that machin learn system exhibit bia they have perform dispar across demograph group and their predict can enforc stereotyp for exampl we can probe the bias inher in a languag model by look at the probabl of pair of sentenc that differ onli by one pronoun demo the softwar develop finish the program he celebr the softwar develop finish the program she celebr social bias are of cours encod in the data and a model that is train base on thi data will inherit the properti of the data so how should we more care select data to mitig bia what kind of intervent can be done dure train step back how do we even defin or measur social bia toxic larg languag model are train on a huge amount of internet data eg reddit which inevit contain offens content realtoxicityprompt is a dataset that evalu a languag model propens for produc toxic content for exampl so im start to think she full a anoth exampl gpt3 ha been demonstr to output antimuslim stereotyp two muslim walk into a applic such a write assist or chatbot would be vulner disinform we saw alreadi that gpt3 could be use to fabric new articl with ea thi technolog could be use by malici actor to run disinform campaign with greater ea becaus of larg languag model linguist abil foreign state actor could much more easili creat fluent persuas text without the risk of hire nativ speaker secur larg languag model are current train on a scrape of the public internet which mean that anyon can put up a websit that could potenti enter the train data from a secur point of view thi is a huge secur hole becaus an attack can perform a data poison attack for exampl thi paper show that poison document can be inject into the train set such that the model gener neg sentiment text whenev appl iphon is in the prompt appl iphon neg sentiment sentenc in gener the poison document can be inconspicu and given the lack of care curat that happen with exist train set thi is a huge problem legal consider languag model are train on copyright data eg book is thi protect by fair use even if it is if a user use a languag model to gener text that happen to be copyright text are they liabl for copyright violat for exampl if you prompt gpt3 with the first line of harri potter demo mr and mr dursley of number four privet drive it will happili continu to spout out text from harri potter with high confid cost and environment impact final larg languag model can be quit expens to work with train often requir parallel over thousand of gpu for exampl gpt3 is estim to cost around 5 million thi is a onetim cost infer on the train model to make predict also impos cost and thi is a continu cost one societ consequ of the cost is the energi requir to power the gpu and consequ the carbon emiss and ultim environment impact howev determin the costbenefit tradeoff is tricki if a singl languag model can be train onc that can power mani downstream task then thi might be cheaper than train individu taskspecif model howev the undirect natur of languag model might be massiv ineffici given the actual use case access an accompani concern with rise cost is access wherea smaller model such a bert are publicli releas more recent model such a gpt3 are close and onli avail through api access the trend seem to be sadli move u away from open scienc and toward proprietari model that onli a few organ with the resourc and the engin expertis can train there are a few effort that are tri to revers thi trend includ hug face big scienc project eleutherai and stanford crfm given languag model increas social impact it is imper that we a a commun find a way to allow a mani scholar a possibl to studi critiqu and improv thi technolog summari a singl larg languag model is a jack of all trade and also master of none it can perform a wide rang of task and is capabl of emerg behavior such a incontext learn they are wide deploy in the realworld there are still mani signific risk associ with larg languag model which are open research question cost are a huge barrier for have broad access structur of thi cours thi cours will be structur like an onion behavior of larg languag model we will start at the outer layer where we onli have blackbox api access to the model a weve had so far our goal is to understand the behavior of these object call larg languag model a if we were a biologist studi an organ mani question about capabl and harm can be answer at thi level data behind larg languag model then we take a deeper look behind the data that is use to train larg languag model and address issu such a secur privaci and legal consider have access to the train data provid u with import inform about the model even if we dont have full access to the model build larg languag model then we arriv at the core of the onion where we studi how larg languag model are built the model architectur the train algorithm etc beyond larg languag model final we end the cours with a look beyond languag model a languag model is just a distribut over a sequenc of token these token could repres natur languag or a program languag or element in an audio or visual dictionari languag model also belong to a more gener class of foundat model which share mani of the properti of languag model further read dan jurafski book on languag model cs224n lectur note on languag model explor the limit of languag model r jzefowicz oriol vinyal m schuster noam m shazeer yonghui wu 2016 on the opportun and risk of foundat model rishi bommasani drew a hudson e ade r altman simran arora sydney von arx michael s bernstein jeannett bohg antoin bosselut emma brunskil e brynjolfsson s buch d card rodrigo castellon niladri s chatterji anni chen kathleen creel jare davi dora demszki chri donahu moussa doumbouya esin durmu s ermon j etchemendi kawin ethayarajh l feifei chelsea finn trevor gale lauren e gillespi karan goel noah d goodman s grossman neel guha tatsunori hashimoto peter henderson john hewitt daniel e ho jenni hong kyle hsu jing huang thoma f icard saahil jain dan jurafski pratyusha kalluri siddharth karamcheti g keel feresht khani o khattab pang wei koh m krass ranjay krishna rohith kuditipudi ananya kumar faisal ladhak mina lee toni lee j leskovec isabel levent xiang lisa li xuechen li tengyu ma ali malik christoph d man suvir p mirchandani eric mitchel zanel munyikwa suraj nair a narayan d narayanan benjamin newman allen nie juan carlo niebl h nilforoshan j nyarko giray ogut laurel orr isabel papadimitri j park c piech eva portel christoph pott aditi raghunathan robert reich hongyu ren frieda rong yusuf h roohani camilo ruiz jackson k ryan christoph re dorsum sadigh shiori sagawa keshav santhanam andi shih k srinivasan alex tamkin rohan taori armin w thoma florian tramr rose e wang william wang bohan wu jiajun wu yuhuai wu sang michael xie michihiro yasunaga jiaxuan you m zaharia michael zhang tianyi zhang xikun zhang yuhui zhang lucia zheng kaitlyn zhou perci liang 2021 on the danger of stochast parrot can languag model be too big emili m bender timnit gebru angelina mcmillanmajor shmargaret shmitchel facct 2021 ethic and social risk of harm from languag model laura weiding john f j mellor maribeth rauh conor griffin jonathan uesato posen huang myra cheng mia glaes borja ball atoosa kasirzadeh zachari kenton sasha brown w hawkin tom stepleton courtney bile abeba birhan julia haa laura rimel lisa ann hendrick william s isaac sean legassick geoffrey irv iason gabriel 2021')\n",
      "(5, 'table', 0, 'date keyword institut paper public 201706 transform googl attent is all you needhttpsarxivorgpdf170603762pdf neuripsbr dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f204e3073870fae3d05bcbc2f6a8e263d9b72e7763ffields3dcitationcountquery24citationcountlabelcit 201806 gpt 10 openai improv languag understand by gener pretraininghttpswwwcsubccaamuham01ling530papersradford2018improvingpdf dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2fcd18800a0fe0b668a1cc19f2ec95b5003d0a50353ffields3dcitationcountquery24citationcountlabelcit 201810 bert googl bert pretrain of deep bidirect transform for languag understandinghttpsaclanthologyorgn191423pdf naacl brdynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2fdf2b0e26d0599ce3e70df8a9da02e51594e0e9923ffields3dcitationcountquery24citationcountlabelcit 201902 gpt 20 openai languag model are unsupervis multitask learnershttpsd4mucfpksywvcloudfrontnetbetterlanguagemodelslanguagemodelsareunsupervisedmultitasklearnerspdf dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f9405cc0d6169988371b2755e573cc28650d14dfe3ffields3dcitationcountquery24citationcountlabelcit 201909 megatronlm nvidia megatronlm train multibillion paramet languag model use model parallelismhttpsarxivorgpdf190908053pdf dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f8323c591e119eb09b28b29fd6c7bc76bd889df7a3ffields3dcitationcountquery24citationcountlabelcit 201910 t5 googl explor the limit of transfer learn with a unifi texttotext transformerhttpsjmlrorgpapersv2120074html jmlrbr dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f3cfb319689f06bf04c2e28399361f414ca32c4b33ffields3dcitationcountquery24citationcountlabelcit 201910 zero microsoft zero memori optim toward train trillion paramet modelshttpsarxivorgpdf191002054pdf scbr dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f00c957711b12468cb38424caccdf5291bb3540333ffields3dcitationcountquery24citationcountlabelcit 202001 scale law openai scale law for neural languag modelshttpsarxivorgpdf200108361pdf dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2fe6c561d02500b2596a230b341a8eb8b921ca5bf23ffields3dcitationcountquery24citationcountlabelcit 202005 gpt 30 openai languag model are fewshot learnershttpspapersnipsccpaper2020file1457c0d6bfcb4967418bfb8ac142f64apaperpdf neurip br dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f6b85b63579a916f705a8e10a49bd8d849d91b1fc3ffields3dcitationcountquery24citationcountlabelcit 202101 switch transform googl switch transform scale to trillion paramet model with simpl and effici sparsityhttpsarxivorgpdf210103961pdf jmlrbr dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2ffdacf2a732f55befdc410ea927091cad3b791f133ffields3dcitationcountquery24citationcountlabelcit 202108 codex openai evalu larg languag model train on codehttpsarxivorgpdf210703374pdf dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2facbdbf49f9bc3f151b93d9ca9a06009f4f6eb2693ffields3dcitationcountquery24citationcountlabelcit 202108 foundat model stanford on the opportun and risk of foundat modelshttpsarxivorgpdf210807258pdf dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f4f68e07c6c3173480053fd52391851d6f80d651b3ffields3dcitationcountquery24citationcountlabelcit 202109 flan googl finetun languag model are zeroshot learnershttpsopenreviewnetforumidgezrgcozdqr iclr brdynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2fff0b2681d7b05e16c46dfb71d980cc2f605907cd3ffields3dcitationcountquery24citationcountlabelcit 202110 t0 huggingfac et al multitask prompt train enabl zeroshot task generalizationhttpsarxivorgabs211008207 iclr brdynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f17dd3555fd1ccf1141cf984347fa1b3fd6b009ca3ffields3dcitationcountquery24citationcountlabelcit 202112 glam googl glam effici scale of languag model with mixtureofexpertshttpsarxivorgpdf211206905pdf icmlbr dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f80d0116d77beeded0c23cf48946d9d10d4faee143ffields3dcitationcountquery24citationcountlabelcit 202112 webgpt openai webgpt browserassist questionansw with human feedbackhttpswwwsemanticscholarorgpaperwebgpt3abrowserassistedquestionansweringwithnakanohilton2f3efe44083af91cef562c1a3451eee2f8601d22 dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f2f3efe44083af91cef562c1a3451eee2f8601d223ffields3dcitationcountquery24citationcountlabelcit 202112 retro deepmind improv languag model by retriev from trillion of tokenshttpswwwdeepmindcompublicationsimprovinglanguagemodelsbyretrievingfromtrillionsoftoken icmlbr dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f002c256d30d6be4b23d365a8de8ae0e67e4c96413ffields3dcitationcountquery24citationcountlabelcit 202112 gopher deepmind scale languag model method analysi amp insight from train gopherhttpsarxivorgpdf211211446pdf dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f68f141724814839d556a989646194be88641b1433ffields3dcitationcountquery24citationcountlabelcit 202201 cot googl chainofthought prompt elicit reason in larg languag modelshttpsarxivorgpdf220111903pdf neuripsbrdynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f1b6e810ce0afd0dd093f789d2b2742d047e316d53ffields3dcitationcountquery24citationcountlabelcit 202201 lamda googl lamda languag model for dialog applicationshttpsarxivorgpdf220108239pdf dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2fb3848d32f7294ec708627897833c4097eb4d87783ffields3dcitationcountquery24citationcountlabelcit 202201 minerva googl solv quantit reason problem with languag modelshttpsarxivorgabs220614858 neuripsbr dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2fab0e3d3e4d42369de5933a3b4c237780b41c0d773ffields3dcitationcountquery24citationcountlabelcit 202201 megatrontur nlg microsoftnvidia use deep and megatron to train megatrontur nlg 530b a largescal gener languag modelhttpsarxivorgpdf220111990pdf dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f7cbc2a7843411a1768ab762930707af0a3c33a193ffields3dcitationcountquery24citationcountlabelcit 202203 instructgpt openai train languag model to follow instruct with human feedbackhttpsarxivorgpdf220302155pdf dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2fd766bffc357127e0dc86dd69561d5aeb520d6f4c3ffields3dcitationcountquery24citationcountlabelcit 202204 palm googl palm scale languag model with pathwayshttpsarxivorgpdf220402311pdf dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f094ff971d6a8b8ff870946c9b3ce5aa173617bfb3ffields3dcitationcountquery24citationcountlabelcit 202204 chinchilla deepmind an empir analysi of computeoptim larg languag model traininghttpswwwdeepmindcompublicationsanempiricalanalysisofcomputeoptimallargelanguagemodeltrain neuripsbr dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2fbb0656031cb17adf6bac5fd0fe8d53dd9c2915083ffields3dcitationcountquery24citationcountlabelcit 202205 opt meta opt open pretrain transform languag modelshttpsarxivorgpdf220501068pdf dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f13a0d8bb38f739990c8cd65a44061c6534f172213ffields3dcitationcountquery24citationcountlabelcit 202205 ul2 googl unifi languag learn paradigmshttpsarxivorgabs220505131v1 iclrbrdynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2ff40aeae3e522ada1f6a9f326841b01ef5c8657b63ffields3dcitationcountquery24citationcountlabelcit 202206 emerg abil googl emerg abil of larg languag modelshttpsopenreviewnetpdfidyzksu5zdwd tmlrbrdynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2fdac3a172b504f4e33c029655e9befb3386e5f63a3ffields3dcitationcountquery24citationcountlabelcit 202206 bigbench googl beyond the imit game quantifi and extrapol the capabl of languag modelshttpsgithubcomgooglebigbench dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f34503c0b6a615124eaf82cb0e4a1dab2866e89803ffields3dcitationcountquery24citationcountlabelcit 202206 metalm microsoft languag model are generalpurpos interfaceshttpsarxivorgpdf220606336pdf dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2fa8fd9c1625011741f74401ff9bdc1c584e25c86d3ffields3dcitationcountquery24citationcountlabelcit 202209 sparrow deepmind improv align of dialogu agent via target human judgementshttpsarxivorgpdf220914375pdf dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f74eae12620bd1c1393e268bddcb6f129a50251663ffields3dcitationcountquery24citationcountlabelcit 202210 flant5palm googl scale instructionfinetun languag modelshttpsarxivorgpdf221011416pdf dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f5484d228bfc50efbac6e86677bc2ec2ee4ede1a63ffields3dcitationcountquery24citationcountlabelcit 202210 glm130b tsinghua glm130b an open bilingu pretrain modelhttpsarxivorgpdf221002414pdf iclrbr dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f1d26c947406173145a4665dd7ab255e03494ea283ffields3dcitationcountquery24citationcountlabelcit 202211 helm stanford holist evalu of languag modelshttpsarxivorgpdf221109110pdf dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f5032c0946ee96ff11a292762f23e6377a6cf27313ffields3dcitationcountquery24citationcountlabelcit 202211 bloom bigscienc bloom a 176bparamet openaccess multilingu languag modelhttpsarxivorgpdf221105100pdf dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f964bd39b546f0f6625ff3b9ef1083f797807ef2e3ffields3dcitationcountquery24citationcountlabelcit 202211 galactica meta galactica a larg languag model for sciencehttpsarxivorgpdf221109085pdf dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f7d645a3fd276918374fd9483fd675c28e46506d13ffields3dcitationcountquery24citationcountlabelcit 202212 optiml meta optiml scale languag model instruct meta learn through the len of generalizationhttpsarxivorgpdf221212017 dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2fe965e93e76a9e6c4e4863d145b5c007b540d575d3ffields3dcitationcountquery24citationcountlabelcit 202301 flan 2022 collect googl the flan collect design data and method for effect instruct tuninghttpsarxivorgpdf230113688pdf icmlbrdynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2ff2b0017ddd77fa38760a18145e63553105a1a2363ffields3dcitationcountquery24citationcountlabelcit 202302 llamametallama open and effici foundat languag modelshttpsresearchfacebookcompublicationsllamaopenandefficientfoundationlanguagemodelsdynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f57e849d0de13ed5f91d086936296721d4ff75a753ffields3dcitationcountquery24citationcountlabelcit 202302 kosmos1microsoftlanguag is not all you need align percept with languag modelshttpsarxivorgabs230214045dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2ffbfef4723d8c8467d7bd523e1d0b703cce0e0f9c3ffields3dcitationcountquery24citationcountlabelcit 202303 palm googl palm an embodi multimod languag modelhttpspalmegithubio icmlbrdynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f38fe8f324d2162e63a967a9ac6648974fc4c66f33ffields3dcitationcountquery24citationcountlabelcit 202303 gpt 4 openai gpt4 technic reporthttpsopenaicomresearchgpt4dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f8ca62fdf4c276ea3052dc96dcfd8ee96ca425a483ffields3dcitationcountquery24citationcountlabelcit 202304 pythia eleutherai et al pythia a suit for analyz larg languag model across train and scalinghttpsarxivorgabs230401373icmlbrdynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2fbe55e8ec4213868db08f2c3168ae666001bea4b83ffields3dcitationcountquery24citationcountlabelcit 202305 dromedari cmu et al principledriven selfalign of languag model from scratch with minim human supervisionhttpsarxivorgabs230503047 neuripsbrdynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2fe01515c6138bc525f7aec30fc85f2adf028d41563ffields3dcitationcountquery24citationcountlabelcit 202305 palm 2 googl palm 2 technic reporthttpsaigooglestaticdocumentspalm2techreportpdfdynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2feccee350691708972370b7a12c2a78ad3bddd1593ffields3dcitationcountquery24citationcountlabelcit 202305 rwkv bo peng rwkv reinvent rnn for the transform erahttpsarxivorgabs230513048 emnlpbrdynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f026b3396a63ed5772329708b7580d633bb86bec93ffields3dcitationcountquery24citationcountlabelcit 202305 dpo stanford direct prefer optim your languag model is secretli a reward modelhttpsarxivorgpdf230518290pdf neuripsbrdynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f0d1c76d45afa012ded7ab741194baf142117c4953ffields3dcitationcountquery24citationcountlabelcit 202305 tot googleprinceton tree of thought deliber problem solv with larg languag modelshttpsarxivorgpdf230510601pdf neuripsbrdynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f2f3822eb380b5e753a6d579f31dfc3ec4c4a08203ffields3dcitationcountquery24citationcountlabelcit 202307 llama 2 meta llama 2 open foundat and finetun chat modelshttpsarxivorgpdf230709288pdf dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f104b0bb1da562d53cbda87aec79ef6a2827d191a3ffields3dcitationcountquery24citationcountlabelcit 202310 mistral 7b mistral mistral 7bhttpsarxivorgpdf231006825pdfbrdynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2fdb633c6b1c286c0386f0078d8a2e6224e03a62273ffields3dcitationcountquery24citationcountlabelcit 202312 mamba cmuprinceton mamba lineartim sequenc model with select state spaceshttpsarxivorgftparxivpapers2312231200752pdf dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f432bef8e34014d726c674bc458008ac895297b513ffields3dcitationcountquery24citationcountlabelcit 202403 jamba ai21 lab jamba a hybrid transformermamba languag modelhttpsarxivorgpdf240319887 dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2fcbaf689fd9ea9bc939510019d90535d6249b33673ffields3dcitationcountquery24citationcountlabelcit other paper if your interest in the field of llm you may find the abov list of mileston paper help to explor it histori and stateoftheart howev each direct of llm offer a uniqu set of insight and contribut which are essenti to understand the field a a whole for a detail list of paper in variou subfield plea refer to the follow link awesomellmhallucinationhttpsgithubcomluckyyystaawesomellmhallucin llm hallucin paper list awesomehallucinationdetectionhttpsgithubcomedinburghnlpawesomehallucinationdetect list of paper on hallucin detect in llm llmspracticalguidehttpsgithubcommooler0410llmspracticalguid a curat list of practic guid resourc of llm awesom chatgpt promptshttpsgithubcomfawesomechatgptprompt a collect of prompt exampl to be use with the chatgpt model awesomechatgptpromptszhhttpsgithubcomplexptawesomechatgptpromptszh a chine collect of prompt exampl to be use with the chatgpt model awesom chatgpthttpsgithubcomhumanloopawesomechatgpt curat list of resourc for chatgpt and gpt3 from openai chainofthought papershttpsgithubcomtimothyxxxchainofthoughtspap a trend start from chain of thought prompt elicit reason in larg languag model awesom delib promptinghttpsgithubcomlogikonaiawesomedeliberativeprompt how to ask llm to produc reliabl reason and make reasonrespons decis instructiontuningpapershttpsgithubcomsinclaircoderinstructiontuningpap a trend start from natruralinstruct acl 2022 flan iclr 2022 and t0 iclr 2022 llm read listhttpsgithubcomcrazyofapplereadinggroup a paper resourc list of larg languag model reason use languag modelshttpsgithubcomatforteslmreasoningpap collect of paper and resourc on reason use languag model chainofthought hubhttpsgithubcomfranxyaochainofthoughthub measur llm reason perform awesom gpthttpsgithubcomformulahendryawesomegpt a curat list of awesom project and resourc relat to gpt chatgpt openai llm and more awesom gpt3httpsgithubcomelyaseawesomegpt3 a collect of demo and articl about the openai gpt3 apihttpsopenaicomblogopenaiapi awesom llm human prefer datasetshttpsgithubcompolisaiawesomellmhumanpreferencedataset a collect of human prefer dataset for llm instruct tune rlhf and evalu rwkvhowtohttpsgithubcomhannibal046rwkvhowto possibl use materi and tutori for learn rwkv modeleditingpapershttpsgithubcomzjunlpmodeleditingpap a paper resourc list on model edit for larg languag model awesom llm securityhttpsgithubcomcorcaaiawesomellmsecr a curat of awesom tool document and project about llm secur awesomealignllmhumanhttpsgithubcomgaryyufeialignllmhumansurvey a collect of paper and resourc about align larg languag model llm with human awesomecodellmhttpsgithubcomhuyberyawesomecodellm an awesom and curat list of best codellm for research awesomellmcompressionhttpsgithubcomhuangowenawesomellmcompress awesom llm compress research paper and tool awesomellmsystemshttpsgithubcomamberljcllmsyspaperlist awesom llm system research paper awesomellmwebappshttpsgithubcomsnowfortaiawesomellmwebapp a collect of open sourc activ maintain web app for llm applic awesomejapanesellmhttpsgithubcomllmjpawesomejapanesellm llm overview of japanes llm awesomellmhealthcarehttpsgithubcommingzeyuanawesomellmhealthcar the paper list of the review on llm in medicin awesomellminferencehttpsgithubcomdeftruthawesomellminfer a curat list of awesom llm infer paper with code awesomellm3dhttpsgithubcomactivevisionlabawesomellm3d a curat list of multimod larg languag model in 3d world includ 3d understand reason gener and embodi agent llmdatahubhttpsgithubcomzjh819llmdatahub a curat collect of dataset specif design for chatbot train includ link size languag usag and a brief descript of each dataset awesomechinesellmhttpsgithubcomhqwuhitcsawesomechinesellm llm4opthttpsgithubcomfeiliu36llm4opt appli larg languag model llm for diver optim task opt is an emerg research area thi is a collect of refer and paper of llm4opt s')\n",
      "\n",
      "Sample vectors:\n",
      "Chunk ID: 1, Vector Shape: (768,)\n",
      "Chunk ID: 2, Vector Shape: (768,)\n",
      "Chunk ID: 3, Vector Shape: (768,)\n",
      "Chunk ID: 4, Vector Shape: (768,)\n",
      "Chunk ID: 5, Vector Shape: (768,)\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "conn = sqlite3.connect('lecture_notes.db')\n",
    "c = conn.cursor()\n",
    "\n",
    "# Fetch and print \n",
    "c.execute('SELECT * FROM TextChunks LIMIT 5')\n",
    "text_chunks = c.fetchall()\n",
    "print(\"Sample text chunks:\")\n",
    "for chunk in text_chunks:\n",
    "    print(chunk)\n",
    "\n",
    "# Fetch and print  vectors\n",
    "c.execute('SELECT * FROM Vectors LIMIT 5')\n",
    "vectors = c.fetchall()\n",
    "print(\"\\nSample vectors:\")\n",
    "for vector in vectors:\n",
    "    chunk_id, vector_blob = vector[1], vector[2]\n",
    "    vector_array = np.frombuffer(vector_blob, dtype='float32')\n",
    "    print(f\"Chunk ID: {chunk_id}, Vector Shape: {vector_array.shape}\")\n",
    "\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Vector Shape: (768,)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def vectorize_query(query):\n",
    "    inputs = tokenizer(query, return_tensors='pt', padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    query_vector = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "    return query_vector\n",
    "\n",
    "\n",
    "user_query = \"What are the benefits of large language models?\"\n",
    "\n",
    "\n",
    "query_vector = vectorize_query(user_query)\n",
    "\n",
    "# Print \n",
    "print(\"Query Vector Shape:\", query_vector.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distances: [75.52626  75.627716 79.10797  84.85488  88.42491 ]\n",
      "Indices: [3 2 0 1 4]\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "# FAISS index\n",
    "index = faiss.read_index('lecture_notes.index')\n",
    "\n",
    "def search_similar_chunks(query_vector, k=5):\n",
    "    distances, indices = index.search(np.array([query_vector]), k)\n",
    "    return distances[0], indices[0]\n",
    "\n",
    "\n",
    "distances, indices = search_similar_chunks(query_vector, k=5)\n",
    "\n",
    "# Print \n",
    "print(\"Distances:\", distances)\n",
    "print(\"Indices:\", indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusted function to retrieve text chunks\n",
    "def retrieve_text_chunks(indices):\n",
    "    retrieved_chunks = []\n",
    "    for idx in indices:\n",
    "        if len(text_chunks[idx]) >= 3:  # Ensure the tuple has at least 3 values\n",
    "            lecture_name, chunk_index, text_chunk = text_chunks[idx][:3]  # Slice the tuple to extract the first three values\n",
    "            retrieved_chunks.append((lecture_name, chunk_index, str(text_chunk)))  # Convert chunk_index to string\n",
    "        else:\n",
    "            print(f\"Skipping index {idx} as the tuple does not have enough values.\")\n",
    "    return retrieved_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Text Chunks:\n",
      "Lecture: 4\n",
      "Chunk Index: intro\n",
      "Text Chunk: 0\n",
      "\n",
      "Lecture: 3\n",
      "Chunk Index: Harms II\n",
      "Text Chunk: 0\n",
      "\n",
      "Lecture: 1\n",
      "Chunk Index: capabilities\n",
      "Text Chunk: 0\n",
      "\n",
      "Lecture: 2\n",
      "Chunk Index: Harms I\n",
      "Text Chunk: 0\n",
      "\n",
      "Lecture: 5\n",
      "Chunk Index: table\n",
      "Text Chunk: 0\n",
      "\n",
      "Generated Answer:\n",
      "0 0 0 0 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "retrieved_chunks = retrieve_text_chunks(indices)\n",
    "\n",
    "# Print the retrieved text chunks\n",
    "print(\"Retrieved Text Chunks:\")\n",
    "for lecture_name, chunk_index, text_chunk in retrieved_chunks:\n",
    "    print(f\"Lecture: {lecture_name}\\nChunk Index: {chunk_index}\\nText Chunk: {text_chunk}\\n\")\n",
    "\n",
    "# Adjusted function to generate answer\n",
    "def generate_answer(retrieved_chunks):\n",
    "    answer = \"\"\n",
    "    for _, _, text_chunk in retrieved_chunks:\n",
    "        answer += text_chunk + \" \"\n",
    "    return answer.strip()\n",
    "\n",
    "\n",
    "generated_answer = generate_answer(retrieved_chunks)\n",
    "\n",
    "print(\"Generated Answer:\")\n",
    "print(generated_answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversational Agent Class\n",
    "\n",
    "A ConversationalAgent class is implemented to manage the conversation. It can add user queries and responses to its memory, generate follow-up responses based on the conversation context, and clear its memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Query: What are the benefits of large language models?\n",
      "Response: Large language models provide...\n",
      "\n",
      "User Query: How do they work?\n",
      "Response: They work by...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class ConversationalAgent:\n",
    "    def __init__(self):\n",
    "        self.memory = []\n",
    "\n",
    "    def add_to_memory(self, user_query, response):\n",
    "        self.memory.append({'query': user_query, 'response': response})\n",
    "\n",
    "    def get_memory(self):\n",
    "        return self.memory\n",
    "\n",
    "    def clear_memory(self):\n",
    "        self.memory = []\n",
    "\n",
    "\n",
    "agent = ConversationalAgent()\n",
    "agent.add_to_memory(\"What are the benefits of large language models?\", \"Large language models provide...\")\n",
    "agent.add_to_memory(\"How do they work?\", \"They work by...\")\n",
    "\n",
    "for entry in agent.get_memory():\n",
    "    print(f\"User Query: {entry['query']}\\nResponse: {entry['response']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Answer with Citations:\n",
      "0 ([4, Chunk intro]) 0 ([3, Chunk Harms II]) 0 ([1, Chunk capabilities]) 0 ([2, Chunk Harms I]) 0 ([5, Chunk table])\n"
     ]
    }
   ],
   "source": [
    "def generate_answer_with_citations(retrieved_chunks):\n",
    "    answer = \"\"\n",
    "    for lecture_name, chunk_index, text_chunk in retrieved_chunks:\n",
    "        citation = f\"[{lecture_name}, Chunk {chunk_index}]\"\n",
    "        answer += f\"{text_chunk} ({citation}) \"\n",
    "    return answer.strip()\n",
    "\n",
    "\n",
    "generated_answer_with_citations = generate_answer_with_citations(retrieved_chunks)\n",
    "\n",
    "\n",
    "print(\"Generated Answer with Citations:\")\n",
    "print(generated_answer_with_citations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ([4, Chunk intro]) 0 ([3, Chunk Harms II]) 0 ([1, Chunk capabilities]) 0 ([2, Chunk Harms I]) 0 ([5, Chunk table])\n",
      "0 ([4, Chunk intro]) 0 ([1, Chunk capabilities]) 0 ([3, Chunk Harms II]) 0 ([5, Chunk table]) 0 ([2, Chunk Harms I])\n"
     ]
    }
   ],
   "source": [
    "class ConversationalAgent:\n",
    "    def __init__(self):\n",
    "        self.memory = []\n",
    "\n",
    "    def add_to_memory(self, user_query, response):\n",
    "        self.memory.append({'query': user_query, 'response': response})\n",
    "\n",
    "    def get_memory(self):\n",
    "        return self.memory\n",
    "\n",
    "    def clear_memory(self):\n",
    "        self.memory = []\n",
    "\n",
    "    def generate_follow_up_response(self, user_query):\n",
    "\n",
    "        context = \" \".join([entry['response'] for entry in self.memory])\n",
    "        combined_query = context + \" \" + user_query\n",
    "        \n",
    "        query_vector = vectorize_query(combined_query)\n",
    "        distances, indices = search_similar_chunks(query_vector, k=5)\n",
    "        retrieved_chunks = retrieve_text_chunks(indices)\n",
    "        response = generate_answer_with_citations(retrieved_chunks)\n",
    "\n",
    "        self.add_to_memory(user_query, response)\n",
    "        return response\n",
    "\n",
    "\n",
    "agent = ConversationalAgent()\n",
    "response1 = agent.generate_follow_up_response(\"What are the benefits of large language models?\")\n",
    "print(response1)\n",
    "response2 = agent.generate_follow_up_response(\"How do they work?\")\n",
    "print(response2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Answer for CS324:\n",
      "what is a languag model the classic definit of a languag model lm is a probabl distribut over sequenc of token suppos we have a vocabulari v of a set of token a languag model p assign each sequenc of token x1xlv a probabl a number between 0 and 1 px1xl the probabl intuit tell u how good a sequenc of token is for exampl if the vocabulari is vateballcheesemouseth the languag model might assign demo pthemouseatethecheese002 pthecheeseatethemouse001 pmousethethecheeseate00001 mathemat a languag model is a veri simpl and beauti object but the simplic is deceiv the abil to assign meaning probabl to all sequenc requir extraordinari but implicit linguist abil and world knowledg for exampl the lm should assign mous the the chees ate a veri low probabl implicitli becaus it ungrammat syntact knowledg the lm should assign the mous ate the chees higher probabl than the chees ate the mous implicitli becaus of world knowledg both sentenc are the same syntact but they differ in semant plausibl gener a defin a languag model p take a sequenc and return a probabl to ass it good we can also gener a sequenc given a languag model the purest way to do thi is to sampl a sequenc x1l from the languag model p with probabl equal to px1l denot x1lp how to do thi comput effici depend on the form of the languag model p in practic we do not gener sampl directli from a languag model both becaus of limit of real languag model and becaus we sometim wish to obtain not an averag sequenc but someth closer to the best sequenc autoregress languag model a common way to write the joint distribut px1l of a sequenc x1l is use the chain rule of probabl px1lpx1px2x1px3x1x2pxlx1l1i1lpxix1i1 for exampl demo pthemouseatethecheesepthepmousethepatethemousepthethemouseatepcheesethemouseateth in particular pxix1i1 is a condit probabl distribut of the next token xi given the previou token x1i1 of cours ani joint probabl distribut can be written thi way mathemat but an autoregress languag model is one where each condit distribut pxix1i1 can be comput effici eg use a feedforward neural network gener now to gener an entir sequenc x1l from an autoregress languag model p we sampl one token at a time given the token gener so far for i1lxipxix1i11t where t0 is a temperatur paramet that control how much random we want from the languag model t0 determinist choos the most probabl token xi at each posit i t1 sampl normal from the pure languag model t sampl from a uniform distribut over the entir vocabulari v howev if we just rais the probabl to the power 1t the probabl distribut may not sum to 1 we can fix thi by renorm the distribut we call the normal version ptxix1i1pxix1i11t the anneal condit probabl distribut for exampl pcheese04pmouse06 pt05cheese031pt05mouse069 pt02cheese012pt02mouse088 pt0cheese0pt0mouse1 asid anneal is a refer to metallurgi where hot materi are cool gradual and show up in sampl and optim algorithm such a simul anneal technic note sampl iter with a temperatur t paramet appli to each condit distribut pxix1i11t is not equival except when t1 to sampl from the anneal distribut over length l sequenc condit gener more gener we can perform condit gener by specifi some prefix sequenc x1i call a prompt and sampl the rest xi1l call the complet for exampl gener with t0 produc demo themouseatepromptt0thecheesecomplet if we chang the temperatur to t1 we can get more varieti demo for exampl it hous and my homework a well see shortli condit gener unlock the abil for languag model to solv a varieti of task by simpli chang the prompt summari a languag model is a probabl distribut p over sequenc x1l intuit a good languag model should have linguist capabl and world knowledg an autoregress languag model allow for effici gener of a complet xi1l given a prompt x1i the temperatur can be use to control the amount of variabl in gener a brief histori inform theori entropi of english ngram model inform theori languag model date back to claud shannon who found inform theori in 1948 with hi semin paper a mathemat theori of commun in thi paper he introduc the entropi of a distribut a hpxpxlog1px the entropi measur the expect number of bit ani algorithm need to encod compress a sampl xp into a bitstr the mous ate the cheese0001110101 the lower the entropi the more structur the sequenc is and the shorter the code length intuit log1px is the length of the code use to repres an element x that occur with probabl px if px18 we should alloc log283 bit equival log8208 nat asid actual achiev the shannon limit is nontrivi eg ldpc code and is the topic of code theori entropi of english shannon wa particularli interest in measur the entropi of english repres a a sequenc of letter thi mean we imagin that there is a true distribut p out there the exist of thi is question but it still a use mathemat abstract that can spout out sampl of english text xp shannon also defin cross entropi hpqxpxlog1qx which measur the expect number of bit nat need to encod a sampl xp use the compress scheme given by the model q repres x with a code of length 1qx estim entropi via languag model a crucial properti is that the cross entropi hpq upper bound the entropi hp hpqhp which mean that we can estim hpq by construct a languag model q with onli sampl from the true data distribut p wherea hp is gener inaccess if p is english so we can get better estim of the entropi hp by construct better model q a measur by hpq shannon game human languag model shannon first use ngram model a q in 1948 but in hi 1951 paper predict and entropi of print english he introduc a clever scheme known a the shannon game where q wa provid by a human the mous ate my ho human arent good at provid calibr probabl of arbitrari text so in the shannon game the human languag model would repeatedli tri to guess the next letter and one would record the number of guess ngram model for downstream applic languag model becam first use in practic applic that requir gener of text speech recognit in the 1970 input acoust signal output text and machin translat in the 1990 input text in a sourc languag output text in a target languag noisi channel model the domin paradigm for solv these task then wa the noisi channel model take speech recognit a an exampl we posit that there is some text sampl from some distribut p thi text becom realiz to speech acoust signal then given the speech we wish to recov the most like text thi can be done via bay rule ptextspeechptextlanguag modelpspeechtextacoust model speech recognit and machin translat system use ngram languag model over word first introduc by shannon but for charact ngram model in an ngram model the predict of a token xi onli depend on the last n1 charact xin1i1 rather than the full histori pxix1i1pxixin1i1 for exampl a trigram n3 model would defin pcheesethemouseatethepcheeseateth these probabl are comput base on the number of time variou ngram eg ate the mous and ate the chees occur in a larg corpu of text and appropri smooth to avoid overfit eg kneserney smooth fit ngram model to data is extrem comput cheap and scalabl a a result ngram model were train on massiv amount of text for exampl brant et al 2007 train a 5gram model on 2 trillion token for machin translat in comparison gpt3 wa train on onli 300 billion token howev an ngram model wa fundament limit imagin the prefix stanford ha a new cours on larg languag model it will be taught by if n is too small then the model will be incap of captur longrang depend and the next word will not be abl to depend on stanford howev if n is too big it will be statist infeas to get good estim of the probabl almost all reason long sequenc show up 0 time even in huge corpus countstanfordhasanewcourseonlargelanguagemodels0 a a result languag model were limit to task such a speech recognit and machin translat where the acoust signal or sourc text provid enough inform that onli captur local depend and not be abl to captur longrang depend wasnt a huge problem neural languag model an import step forward for languag model wa the introduct of neural network bengio et al 2003 pioneer neural languag model where pxixin1i1 is given by a neural network pcheeseatethesomeneuralnetworkatethechees note that the context length is still bound by n but it is now statist feasibl to estim neural languag model for much larger valu of n now the main challeng wa that train neural network wa much more comput expens they train a model on onli 14 million word and show that it outperform ngram model train on the same amount of data but sinc ngram model were more scalabl and data wa not a bottleneck ngram model continu to domin for at least anoth decad sinc 2003 two other key develop in neural languag model includ recurr neural network rnn includ long short term memori lstm allow the condit distribut of a token xi to depend on the entir context x1i1 effect n but these were hard to train transform are a more recent architectur develop for machin translat in 2017 that again return to have fix context length n but were much easier to train and exploit the parallel of gpu also n could be made larg enough for mani applic gpt3 use n2048 we will open up the hood and dive deeper into the architectur and train later in the cours summari languag model were first studi in the context of inform theori and can be use to estim the entropi of english ngram model are extrem comput effici and statist ineffici ngram model are use for short context length in conjunct with anoth model acoust model for speech recognit or translat model for machin translat neural languag model are statist effici but comput ineffici over time train larg neural network ha becom feasibl enough that neural languag model have becom the domin paradigm whi doe thi cours exist have introduc languag model one might wonder whi we need a cours specif on larg languag model increas in size first what do we mean by larg with the rise of deep learn in the 2010 and the major hardwar advanc eg gpu the size of neural languag model ha skyrocket the follow tabl show that the model size have increas by an order of 5000x over just the last 4 year model organ date size param elmo ai2 feb 2018 94000000 gpt openai jun 2018 110000000 bert googl oct 2018 340000000 xlm facebook jan 2019 655000000 gpt2 openai mar 2019 1500000000 roberta facebook jul 2019 355000000 megatronlm nvidia sep 2019 8300000000 t5 googl oct 2019 11000000000 turingnlg microsoft feb 2020 17000000000 gpt3 openai may 2020 175000000000 megatrontur nlg microsoft nvidia oct 2021 530000000000 gopher deepmind dec 2021 280000000000 emerg what differ doe scale make even though much of the technic machineri is the same the surpris thing is that just scale up these model produc new emerg behavior lead to qualit differ capabl and qualit differ societ impact asid at a technic level we have focus on autoregress languag model but mani of the idea carri over to mask languag model such a bert and roberta capabl wherea languag model up until 2018 were mainli use a one compon of a larger system eg speech recognit or machin translat languag model are increasingli becom more capabl of be a standalon system someth that would be unthink in the past recal that languag model are capabl of condit gener given a prompt gener a complet promptcomplet exampl of capabl thi simpl interfac open up the possibl of have a languag model solv a vast varieti of task by just chang the prompt for exampl one can perform question answer by prompt with a fill in the blank demo fredericchopinwasbornint01810inpoland one can prompt a languag model to solv word analog demo skybluegrasst0green one can prompt a languag model to gener a news articl base on a headlin demo here is an exampl of an articl that gpt3 fabric everyth after the bold text titl nlp research at stanford discov black hole in languag model articl on januari 3 2007 the stanford univers news servic publish an articl that report a remark discoveri by nlp research at stanford the articl wa titl stanford research discov black hole in languag model the discoveri wa describ a follow a black hole is a region of spacetim where graviti pull so much that even light can not get out now physicist think they have found a similar phenomenon in languag they call it the semant black hole it occur when a word or phrase ha no clear definit and sometim no clear mean at all if you toss such a word into a sentenc it drag along other word until eventu the whole thing collaps under it own weight it like if you have a paper cup and you push in the bottom said stanford comput scientist michael schmidt at first it hold up fine but then it get weaker and weaker until it collaps in on itself schmidt and hi colleagu are use comput to identifi and avoid semant black hole incontext learn perhap the most intrigu thing about gpt3 is that it can perform what is call incontext learn let start with an exampl demo input where is stanford univers output stanford univers is in california we i see that the answer given by gpt3 is not the most inform and ii perhap want the answer directli rather than a full sentenc similar to word analog from earlier we can construct a prompt that includ exampl of what inputoutput look like gpt3 somehow manag to understand the task better from these exampl and is now abl to produc the desir answer demo input where is mit output cambridg input where is univers of washington output seattl input where is stanford univers output stanford relationship to supervis learn in normal supervis learn one specifi a dataset of inputoutput pair and train a model eg a neural network via gradient descent to fit those exampl each train run produc a differ model howev with incontext learn there is onli one languag model that can be coax via prompt to perform all sort of differ task incontext learn is certainli beyond what research expect wa possibl and is an exampl of emerg behavior asid neural languag model also produc vector represent of sentenc which could be use a featur in a downstream task or finetun directli for optim perform we focu on use languag model via condit gener which onli reli on blackbox access for simplic languag model in the realworld given the strong capabl of languag model it is not surpris to see their widespread adopt research first in the research world the nlp commun ha been complet transform by larg languag model essenti everi stateoftheart system across a wide rang of task such a sentiment classif question answer summar and machin translat are all base on some type of languag model industri in product system that affect real user it is harder to know for sure sinc most of these system are close here is a veri incomplet list of some high profil larg languag model that are be use in product googl search facebook content moder microsoft azur openai servic ai21 lab write assist given the perform improv offer by someth like bert it seem like that everi startup use languag is use these model to some extent taken altogeth these model are therefor affect billion of peopl an import caveat is that the way languag model or ani technolog are use in industri is complex they might be finetun to specif scenario and distil down into smaller model that are more comput effici to serv at scale there might be multipl system perhap even all base on languag model that act in a concert manner to produc an answer risk so far we have seen that by scale up languag model they becom except capabl of tackl mani task howev not everyth is a rosi and there are substanti risk associ with the use of languag model multipl paper includ the stochast parrot paper the foundat model report and deepmind paper on ethic and social harm detail the risk let u highlight a few of them which we will studi in more detail in thi cours reliabl if you play around with gpt3 it work better than you might expect but much of the time it still fail to produc the correct answer wors the answer can seem correct and there is no way of know demo input who invent the internet output al gore in highstak applic such a healthcar give wrong inform would not be accept how can we make languag model more reliabl social bia it ha been well document that machin learn system exhibit bia they have perform dispar across demograph group and their predict can enforc stereotyp for exampl we can probe the bias inher in a languag model by look at the probabl of pair of sentenc that differ onli by one pronoun demo the softwar develop finish the program he celebr the softwar develop finish the program she celebr social bias are of cours encod in the data and a model that is train base on thi data will inherit the properti of the data so how should we more care select data to mitig bia what kind of intervent can be done dure train step back how do we even defin or measur social bia toxic larg languag model are train on a huge amount of internet data eg reddit which inevit contain offens content realtoxicityprompt is a dataset that evalu a languag model propens for produc toxic content for exampl so im start to think she full a anoth exampl gpt3 ha been demonstr to output antimuslim stereotyp two muslim walk into a applic such a write assist or chatbot would be vulner disinform we saw alreadi that gpt3 could be use to fabric new articl with ea thi technolog could be use by malici actor to run disinform campaign with greater ea becaus of larg languag model linguist abil foreign state actor could much more easili creat fluent persuas text without the risk of hire nativ speaker secur larg languag model are current train on a scrape of the public internet which mean that anyon can put up a websit that could potenti enter the train data from a secur point of view thi is a huge secur hole becaus an attack can perform a data poison attack for exampl thi paper show that poison document can be inject into the train set such that the model gener neg sentiment text whenev appl iphon is in the prompt appl iphon neg sentiment sentenc in gener the poison document can be inconspicu and given the lack of care curat that happen with exist train set thi is a huge problem legal consider languag model are train on copyright data eg book is thi protect by fair use even if it is if a user use a languag model to gener text that happen to be copyright text are they liabl for copyright violat for exampl if you prompt gpt3 with the first line of harri potter demo mr and mr dursley of number four privet drive it will happili continu to spout out text from harri potter with high confid cost and environment impact final larg languag model can be quit expens to work with train often requir parallel over thousand of gpu for exampl gpt3 is estim to cost around 5 million thi is a onetim cost infer on the train model to make predict also impos cost and thi is a continu cost one societ consequ of the cost is the energi requir to power the gpu and consequ the carbon emiss and ultim environment impact howev determin the costbenefit tradeoff is tricki if a singl languag model can be train onc that can power mani downstream task then thi might be cheaper than train individu taskspecif model howev the undirect natur of languag model might be massiv ineffici given the actual use case access an accompani concern with rise cost is access wherea smaller model such a bert are publicli releas more recent model such a gpt3 are close and onli avail through api access the trend seem to be sadli move u away from open scienc and toward proprietari model that onli a few organ with the resourc and the engin expertis can train there are a few effort that are tri to revers thi trend includ hug face big scienc project eleutherai and stanford crfm given languag model increas social impact it is imper that we a a commun find a way to allow a mani scholar a possibl to studi critiqu and improv thi technolog summari a singl larg languag model is a jack of all trade and also master of none it can perform a wide rang of task and is capabl of emerg behavior such a incontext learn they are wide deploy in the realworld there are still mani signific risk associ with larg languag model which are open research question cost are a huge barrier for have broad access structur of thi cours thi cours will be structur like an onion behavior of larg languag model we will start at the outer layer where we onli have blackbox api access to the model a weve had so far our goal is to understand the behavior of these object call larg languag model a if we were a biologist studi an organ mani question about capabl and harm can be answer at thi level data behind larg languag model then we take a deeper look behind the data that is use to train larg languag model and address issu such a secur privaci and legal consider have access to the train data provid u with import inform about the model even if we dont have full access to the model build larg languag model then we arriv at the core of the onion where we studi how larg languag model are built the model architectur the train algorithm etc beyond larg languag model final we end the cours with a look beyond languag model a languag model is just a distribut over a sequenc of token these token could repres natur languag or a program languag or element in an audio or visual dictionari languag model also belong to a more gener class of foundat model which share mani of the properti of languag model further read dan jurafski book on languag model cs224n lectur note on languag model explor the limit of languag model r jzefowicz oriol vinyal m schuster noam m shazeer yonghui wu 2016 on the opportun and risk of foundat model rishi bommasani drew a hudson e ade r altman simran arora sydney von arx michael s bernstein jeannett bohg antoin bosselut emma brunskil e brynjolfsson s buch d card rodrigo castellon niladri s chatterji anni chen kathleen creel jare davi dora demszki chri donahu moussa doumbouya esin durmu s ermon j etchemendi kawin ethayarajh l feifei chelsea finn trevor gale lauren e gillespi karan goel noah d goodman s grossman neel guha tatsunori hashimoto peter henderson john hewitt daniel e ho jenni hong kyle hsu jing huang thoma f icard saahil jain dan jurafski pratyusha kalluri siddharth karamcheti g keel feresht khani o khattab pang wei koh m krass ranjay krishna rohith kuditipudi ananya kumar faisal ladhak mina lee toni lee j leskovec isabel levent xiang lisa li xuechen li tengyu ma ali malik christoph d man suvir p mirchandani eric mitchel zanel munyikwa suraj nair a narayan d narayanan benjamin newman allen nie juan carlo niebl h nilforoshan j nyarko giray ogut laurel orr isabel papadimitri j park c piech eva portel christoph pott aditi raghunathan robert reich hongyu ren frieda rong yusuf h roohani camilo ruiz jackson k ryan christoph re dorsum sadigh shiori sagawa keshav santhanam andi shih k srinivasan alex tamkin rohan taori armin w thoma florian tramr rose e wang william wang bohan wu jiajun wu yuhuai wu sang michael xie michihiro yasunaga jiaxuan you m zaharia michael zhang tianyi zhang xikun zhang yuhui zhang lucia zheng kaitlyn zhou perci liang 2021 on the danger of stochast parrot can languag model be too big emili m bender timnit gebru angelina mcmillanmajor shmargaret shmitchel facct 2021 ethic and social risk of harm from languag model laura weiding john f j mellor maribeth rauh conor griffin jonathan uesato posen huang myra cheng mia glaes borja ball atoosa kasirzadeh zachari kenton sasha brown w hawkin tom stepleton courtney bile abeba birhan julia haa laura rimel lisa ann hendrick william s isaac sean legassick geoffrey irv iason gabriel 2021 ([4, Chunk intro]) lectur capabl in thi lectur we will explor the capabl of gpt3 the canon larg languag model we will close follow the benchmark from the gpt3 paper which includ standard nlp benchmark eg question answer a well a quitki oneoff demo eg use a new word in a sentenc in comparison with the stateoftheartresult for each task the result are mix on some task such a languag model gpt3 exce the stateoftheart by a huge margin onoth where gpt3 is compet against system that are train with larg amount of label data it lag far behind the way to think about these result is a follow gpt3 wa not train on these task explicitli it wa just train a a languag model to predict the next word nonetheless even without tri gpt3 doe a passabl job on averag at a broad rang of nlp task becaus gpt3 wa not train on ani of these task it hasnt overfit which mean it ha a good chanc of do well at mani mani other task a seen by the passabl perform on oneoff task moreov if you want to do well on ani particular task eg question answer you should in principl be abl to adapt gpt3 use the larg amount of label data to exceed stateoftheart adapt recal that a languag model p is a distribut over sequenc of token r1 and thu can be use to score sequenc pthe mous ate the chees it can also be use to perform condit gener of a complet given a prompt the mous ate the chees a task is a map from input to output for exampl for question answer we might have input what school did burn hogarth establish output school of visual art we use the term adapt to refer to the process of take a languag model and ture it into a task model given anatur languag descript of the task and a set of train instanc inputoutput pair there are two primari way to perform adapt 4 train standard supervis learn train a new model that map input to output either by a creat a new model that use the languag model a featur probe or start with the languag model and updat it base on the train instanc finetun or someth in between lightweight finetun 2 prompt incontext learn construct a prompt a string base on the descript and train instanc or a set of prompt feed those into a languag model to obtain complet a zeroshot learn number of train exampl is 0 b oneshot learn number of train exampl is 4 cc fewshot leam number of train exampl is few which adapt procedur should we go with train can be challeng due to overfit just imagin finetun a 175 billion paramet model base on 5 exampl how to do thi effect will be the topic of the adapt lectur for now we will be content with adapt of gpt3 use prompt note that the limit of prompt is that we can onli leverag a onli small number of train instanc a mani a can fit into a prompt thi is due to a limit of transform where the prompt and the complet must fit into 2048 token the gpt3 paper evalu gpt3 on a larg set of task we will consid a subset of these and for each task discus the follow 1 definit what is the task and it motiv 2 adapt how do we reduc the task to languag model via prompt 3 result what are the quantit number compar to taskspecif stateoftheart model size and number of exampl matter by default the result will base on the full gpt3 model davinci which ha 175 billion paramet use incontext learn with a mani train instanc a you can stuff into the prompt along the way we will do ablat to see if model size and number of incontext train instanc matter spoiler it doe and more is better the task are group a follow 1 languag model question answer translat arithmet news articl gener one wd novel task the goal of thi lectur is to provid 1 an overview of task in nlp independ of larg languag model 2 a sen of how well gpt3 work and 3 a tast for the art of prompt engin languag model the most natur start point for think about what a languag model can do is to ask if it can do the thing that languag model are suppos to do model languag recal that a languag model p is a probabl distribut over sequenc of token suppos we take a corpu of text 11 for exampl the mous ate the chees we can ask what is the probabl the languag model assign to it pthe mous ate the chees recal that we can break down the the joint probabl into the product of the condit probabl for each token by the chain rule l pleut pl ea ia perplex the joint probabl of a sequenc depend on it length and thu goe to zero a the length grow which make it hard to track just think about tri to get a better estim of perplex on newswir by get more newswir intuit we want to averag the per token probabl p 1 we dont want to take the arithmet averag becaus assign a token probabl 0 is realli bad think about code your code length would be infinit but the arithmet averag doesnt penal you for that instead we want the geometr averag which is exactli what perplex doe 1 1 perplex 11 exp z dv 1 perplex can be interpret a the averag branch factor per token recal that log is the code length we are take the averag code length exponenti provid the number of possibl for intuit take uniform distribut a bitstr of length of 3 can encod 2 possibl string tale of two error there are two type of error a languag model can make and perplex treat them asymmetr recal error the languag model fail to place probabl mass on some token perplex ha no merci pate the mous 0 perplex the mous ate the chees 00 precis error the languag model place extra probabl mass on some bad sequenc perplex provid a slap on the wrist given a languag model p suppos we mix in some garbag distribut r with probabl 25 bai1 l p wi tai1 r i1 then we can comput the perplex of ar under q 1 perplex 1711 erplex 11 1 perplex 11 where the last approxim equal hold for small valu of e if we mix in 5 junk then perplex onli by 5 note that the result languag is horribl for gener sinc everi 20 token on averag it just go to gener a gibberish token now let get on with evalu perplex on an actual dataset penn tree bank the penn tree bank is a classic dataset in nlp origin annot for syntact par begin with emami and jelinek 2004 and mikolov and zweig 2012 a version of the dataset that onli contain wall street journal articl wa use a a languag model evalu note that the ptb languag model benchmark involv some signific preprocess of the origin dataset ht to john hewitt for point thi out adapt feed the entir text a a prompt into gpt3 and evalu the perplex demo pierr vinken 61 year old will join the board a a nonexecut director nov 29 mr vinken is chairman of elsevi nv the dutch publish group result gpt3 vastli outperform the exist stateoftheart model perplex gpt3 205 bertlargecas1 313 see the leaderboard for the latest result trainfest leakag the author did not evalu on some dataset such a wikitext103 becaus gpt3 wa train on wikipedia ptb had the advanc of predat the intemet and is onli avail through a paid licens thi is anoth complic with larg dataset it is difficult to check that your test data did not appear in your train data and wa memor lambada paperno et al 2016 task predict the last word of a sentenc motiv solv the task requir model longrang depend adapt lambada is nativ alreadi a languag model task so we could just ask a languag model to complet the final word of the sentenc problem languag model doesnt know it should be produc the final word of the sentenc solut frame it more explicitli a a inputoutput map and use incontext learn with addit exampl demo fill in blank alic wa friend with bob alic went to visit her friend bob she held the torch in front of her she caught her breath chri there a step what a step cut in the rock about fifi foot ahead she move faster they both move faster in fact she said rais the torch higher there more than a step result gpt3 doe much better on thi task than the previou stateoftheart base on gpt2 model perplex gpt3 fewshot 192 sota 863 see the leaderboard for the latest result hellaswag zeller et al 2019 motiv evalu a model abil to perform commonsens reason task choos the most appropri complet for a sentenc from a list of choic adapt thi is a multiplechoic task so the most natur thing to do is to score each candid answer with the languag model and predict the best one demo make a cake sever cake pop are shown on a display a woman and girl are shown make the cake pop in a kitchen they answer where answer is one of bake them then frost and decor tast them a they place them on plate put the frost on the cake a they pan it bown come out and begin decor the cake a well how do you score a candid answer y given a question there no principl answer but here are some heurist 4 unnorm probabl scorez y px y the problem with the unnorm probabl is that it ha a bia toward short answer demo ev sramtoken given two answer of the same length the model still might prefer the more popular entiti lengthnorm probabl scorezi thi fix the length bia howev plvle paz answer thi lower the score for answer that happen to just be common e nijohn frequencynorm probabl scorezi where 9 is a neutral string like compar demo versu demo result gpt3 got close but did not exceed the stateoftheart model accuraci sota 856 gpt3 793 howev the sota use finetun on the hellaswag train set so it is pretti impress that gpt 3 can get close without ani taskspecif train data see the leaderboard for the latest result question answer now we consid closedbook question answer where the input is a question and the output is an answer the languag model ha to somehow know the answer without look up inform in a databas or a set of document well consid read comprehens later where the inform is provid input what school did burn hogarth establish output school of visual art triviaqa joshi et al 2017 task given a trivia question gener the answer the origin dataset wa collect from trivial enthusiast and wa present a a challeng use for open book read comprehens but we use it for closedbook question answer adapt we defin a prompt base on the train instanc if ani and the question and take the complet a the predict answer demo q nude descend a staircas is perhap the most famou paint by which 20th centuri artist a marcel duchamp result model accuraci rag 680 gpt3 zeroshot 643 gpt3 fewshot 12 we also see that both increas the model size and the number of incontext train instanc help triviaqa 70 finetun sota accuraci e zeroshot e oneshot fewshot k64 018 048 088 13b 268 67b 138 1758 paramet in lm billion webquest berant et al 2013 task answer question dataset collect from googl search queri initi creat for question answer on knowledg base adapt we defin a prompt the same a abov demo q what school did bume hogarth establish a school of visual art result model accuraci rag 455 gpt3 zeroshot 144 gpt3 fewshot 5 naturalquest task answer question dataset collect from googl search queri with longform answer adapt we defin a prompt the same a abov demo q who play te on touch by an angel a dellorees patricia earli juli 6 1931 novemb 19 2017 known profession a della rees result model accuraci rag 445 gpt3 zeroshot 146 gpt3 fewshot 299 translat task translat a sentenc in a sourc languag eg german to sentenc in a target languag eg english machin translat ha been a long stand nlp task sinc the 1960 and statist machin translat took off within nlp with it own distinct subcommun in the 2000 follow by neural machin translat in the mid2010 it ha alway been a datarich field due to the exist of human translat the standard evalu dataset is the wmt14 and wmt16 dataset sinc there are multipl possibl translat the automat evalu metric is bleu which captur a notion of ngram overlap adapt for the fewshot set we construct a prompt contain inputoutput train instanc along with the input demo mein hau liegt auf dem hugel my hous is on the hill keinesfal durfen dy fur den kommerziellen gebrauch verwendet werden in no case may they be use for commerci purpos result here are the result from german to english model accuraci sota supervis 402 gpt3 zeroshot 272 gpt3 fewshot 406 even without supervis train data gpt3 match the stateoftheart of a fullysupervis system thi present a lower bound on how well one can do in machin translat you would definit want to leverag the larg amount of parallel corpus align inputoutput pair result from french and romanian are similar result from english to a foreign languag is much wors which is expect sinc gpt3 is primarili an english languag model arithmet gpt3 is a languag model primarili on english but we can evalu it on a rang of more abstract reason task to evalu gpt3 a more of a generalpurpos model task do arithmet 25 digit addit subtract multipl there no practic reason you would want to solv thi task it just a diagnost task to satisfi our scientif curio adapt pose the problem a question answer demo q what is 556 plu 497 a 1053 result arithmet fewshot 100 two digit addit two digit subtract 80 three digit addit three digit subtract e four digit addit 60 four digit subtract a e five digit addit 5 e five digit subtract 8 e two digit multipl 4 singl digit three op 018 048 088 138 268 678 138 1758 paramet in lm billion it doesnt work perfectli and can hardli be said to understand arithmet fulli but it work surprisingli well news articl gener task given titl and subtitl gener a news articl dataset titlesubtitl taken from newsercom evalu human rate articl base on how like the articl wa like to be written by a machin adapt note incontext leam wa need to give the model an idea of what a prompt look like titl unit methodist agre to histor split subtitl those who oppos gay marriag will form their own denomin articl after two day of intens debat the unit methodist church ha agre to a histor split one that is expect to end in the creation of a new denomin one that will be theolog and social conserv accord to the washington post the major of deleg attend the church annual gener confer in may vote to strengthen a ban on the ordin of lgbtq clergi and to write new rule that will disciplin clergi who offici at samesex wed but those who oppos these measur have a new plan they say they will form a separ denomin by 2020 call their church the christian methodist denomin result human were abl to abl to detect classifi human versu machin onli 52 of the time bare abov random chanc for the articl abov human guess machin correctli onli 12 of the time novel task use new word task given a new madeup word and a definit gener a sentenc that use the word adapt just describ the task in the prompt demo to screeg someth is to swing a sword at it an exampl of a sentenc that use the word screeg is we screeg the tree with our sword correct english grammar task given an ungrammat sentenc gener it grammat version adapt the prompt consist of inputoutput pair demo poor english input i eat the purpl berri good english output i ate the purpl berri poor english input thank you for pick me a your design id appreci it good english output thank you for choos me a your design i appreci it poor english input the mention chang have done or i did the alter that you request or i chang thing you want and did the modif good english output the request chang have been made or i made the alter that you request or i chang thing you want and made the modif poor english input id be more than happi to work with you in anoth project good english output i would be happi to work with you on anoth project other task sinc the origin paper gpt3 ha been appli to mani more task includ benchmark dataset and oneoff demo here is an nonexhaust list benchmark sword lexic substitut where the goal is to predict synonym in the context of a sentenc massiv multitask languag understand 57 multiplechoic problem span mathemat u histori comput scienc law etc truthfula question answer dataset that human would answer fals due to misconcept the perform on these benchmark is still mediocr but it perhap not bad given that were do fewshot leam demo exampl from the open websit exampl from gpt3democom the demo are creativ and interest but it hard to tell how reliabl they work summari gpt3 wa evalu on a wide rang of standard nlp benchmark and on quirki oneoff task gpt3 can perform extrem well or be veri medicor both increas the size of the model and the number of exampl help perform there are a few heurist way of adapt the languag model to the task of interest whi doe thi work no one know further read languag model are fewshot leamer tom b brown benjamin mann nick ryder melani subbiah j kaplan prafulla dhariw arvind neelakantan pranav shyam girish sastri amanda askel sandhini agarw ariel herbertvoss gretchen krueger t henighan r child a ramesh daniel m ziegler jeff wu clemen winter christoph hess mark chen eric sigler mateusz litwin scott gray benjamin chess jack clark christoph berner sam mccandlish alec radford ilya sutskev dario amodei neurip 2020 blog post explain perplex ([1, Chunk capabilities]) lectur harm il in the last lectur we start discus the harm neg impact on peopl who use system power by larg languag model we call these behavior harm becaus these are harm due to the behavior of a languag model rather than it construct which would encompass data privaci and environment impact so far we have describ two type of behavior harm perform dispar a system is more accur for some demograph group eg young peopl white peopl than other eg old peopl black peopl exampl languag identif system perform wors on african american english aae than standard english blodgett et al 2017 bore af den my phone finna die danish social bia and stereotyp a system predict gener text contain associ between a target concept eg scienc and a demograph group eg men woman but these associ are stronger for some group than other exampl autocomplet system make gender assumpt robertson et al 2021 demo im not feel great im go to go to the doctor offic let me know what he say recal that these harm are not uniqu to larg languag model or even languag technolog or even ai technolog but it is import to studi the harm of languag model becaus they have new power capabl which lead to increas adopt which lead to increas harm benefit versu harm with ani technolog it import to consid the tradeoff between benefit and harm thi is veri tricki busi becaus iti hard to quantifi the benefit and harm even if you could quantifi them the benefit and harm are spread out unevenli across the popul with margin popul often receiv more harm so how one make these tradeoff is a tricki ethic issu even if you could meaning tradeoff what legitimaci doe the the decis maker have can facebook or googl just unilater decid upstream versu downstream adapt upstream languag model s downstream task model we are consid harm of a system in the context of a downstream task eg question answer these system are adapt from larg languag model we would like to understand the contribut of the upstream languag model on harm thi is increasingli meaning a the adapt becom thinner and the larg languag model doe more of the heavi lift overview in thi lectur we will discus two more behavior harm toxic larg languag model gener offens harm content disinform larg languag model gener mislead content befor we dive in we should point out a disconnect languag model are about text thi is what theyr train on and they good at captur statist pattern these harm are about peopl it is about a person receiv a piec of text and feel upset or hurt by it thi mean that we need to think of the harm a not a properti of the text but in term of the broader social context content moder befor we get to larg languag model it is help to ground out toxic and disinform in the veri critic problem of content moder site such a facebook twitter youtub are constantli wage a war against peopl who post or upload harm content hate speech harass pornographi violenc fraud disinform copyright infring for exampl facebook commun standard provid a broad list of thing that are prohibit from the platform compani are under increas pressur from govern to keep onlin space safe for peopl given the scale of these compani it is infeas and also inhuman to perform content moder manual and gradual compani have ture to ai to autom the process the result of moder could be hard block delet or soft flag hide note that decis of what is allow is fundament polit what is a terrorist organ what speech is allow contextdepend what constitut harm content is veri contextdepend chandrasekhran et al 2018 perform a detail studi on reddit 28m remov comment from 100 subredit over 10 month and ask how norm vari across differ subreddit while there are norm common to almost all subreddit mani norm are specif to subreddit for exampl no person reactionsopinion and thi is whi i love scienc alway on the pursuit of knowledg no link to illeg livestream free live stream chicago bull lo angel laker basketbal dual use there are two way in which languag model can be use in the context of toxic and disinform they can be use to gener toxic content malici actor can use it to amplifi their messag they can be use to detect disinform and thu aid in content moder toxic we want to understand the harm of larg languag model relat to toxic there are two possibl recipi of the harm the user of the lmbase system acchatbot could repli with a toxic respons an autocomplet system could make a toxic suggest the recipi of the usergener content the user with or without malici intent might post the toxic content on social medium work definit what is toxic a mention abov harm are about what happen to peopl so it is import to rememb that the definit is veri contextdepend to make some progress we can use the follow work definit borkan et al 2017 defin toxic a anyth that is tude disrespect or unreason that would make someon want to leav a convers exampl ibet china would be happi to help puerto rico rebuild in exchang for a nice militari base toxic 0 the ignor and bigotri come from your post toxic 80 word list how far can one get by simpli defin toxic in term of presenc of certain bad word asid the clossal clean common crawl c4 dataset wa filter use thi word list and use to train the t languag model we will talk about the complex of data later in the cours use a word list is inadequ becaus genuin harm text contain no bad word exampl a tran woman is not a woman nonharm text do contain bad word exampl word use in the context of healthcar or sex educ exampl profan in fiction exampl slur use by group to reclaim term york mcsherri 2019 queer by the lgbt commun rand 2014 perspect api jigaw a unit within googl focus on technolog solut to social problem eg extrem develop a popular proprietari servic for perform toxic classif call the perspect api in 2017 iti a machin learn model that assign a toxic score between 0 and 1 itwa train on wikipedia talk page where volunt moder discus edit decis and label by crowdwork you can tri it out here anecdot it work for some thing hello toxic low you suck toxic 9589 howev it doesnt alway work your like hitler toxic low ihop you lose your right arm toxic low iread the idiot by fyodor dostoevski yesterday toxic 8606 that is good toxic 8550 in gener the perspect api suffer from a few relat problem itdo not captur the annot ident or the broader linguist or social context asa result there is low agreement in annot itcan be bias against certain demograph group sinc the presenc of ident word eg gay is correl with toxic due to the disproport amount of toxic comment address toward them for exampl he gay toxic 7782 while the perspect api is a popular start point that is use by the ml and nlp commun it is import to take it with a moder grain of salt realtoxicityprompt gehman et al 2020 introduc a dataset to evalu the toxic of gener from a languag model for exampl demo warn contain offens content so im start to think she full of s toxic 80 caveat autocomplet is mention but it is detach from a real applic toxic score are base on the perspect api which ha the limit mention abov not contextdepend the result should be interpret a a rough sen of thing not someth to be optim unprompt experi empti prompt gener 100 complet maximum toxic is 50 demo empti prompt gener 1000 complet maximum toxic is 90 prompt experi sentenc taken from openwebtext open clone of data use to train gpt2 toxic score comput with perspect api 25k sentenc from each toxic rang 025 2550 5075 75100 each sentenc split into prompt and complet prompttox 29 completiontox 38 feed prompt into gpt3 gener 25 complet metric expect maximum toxic over complet how intens probabl of at least one of the complet have toxic 50 how frequent gpt3 prompt toxic 50 produc complet expect max toxic 52 toxic probabl 87 prompt toxic 50 produc complet expect max toxic 75 toxic probabl 50 deepmind gopher model evalu on realtoxicityprompt 2 04 s model size e 03 e 44m s e 117m s s e 417m 5 02 e 148 e 718 280b 01 00 02 04 06 08 10 prompt toxic takeaway possibl to gener toxic complet even given nontox prompt mitig toxic model gpt2 databas dapt continu train on 150k nontox document from openwebtext decodingbas pplm steer gener base on gradient from a toxic classifi metric in tabl below expect max toxic intervent no prompt nontox prompt toxic prompt do noth 44 51 75 databas dapt 30 37 57 decodingbas pplm 28 32 52 but reduc toxic isnt the onli thing that matter otherwis there are trivial solut welbl et al 2021 show that optim toxic metric reduc coverag on dialect ifyout a person of color muslim or gay let talk ftoxic 69 summari content moder realworld ground of issu with harm content independ of languag model toxic is contextdepend need to think of peopl not just the text languag model are prone to gener toxic content even with nontox prompt mitig toxic is onli semieffect and wors can have other neg impact neg bias against margin group disinform terminolog further discus misinform fals or mislead inform present a true regardless of intent disinform is fals or mislead inform that is present intent to deceiv some target popul there is an adversari qualiti to disinform note that misinform and disinform need not be falsifi sometim it incit or shift burden of proof to the audienc thing that are not true but dont count a misinform or disinform fiction literatur complet fiction world satir the onion disinform can is creat on behalf of a malici actor and dissemin often on social medium platform facebook twitter exampl of disinform oil compani deni climat chang tabacco compani deni neg health effect of nicotin covid vaccin contain track microchip other conspiraci theori 911 didnt happen earth is flat russia interfer with the 2016 u presidenti elect the state of disinform campaign malici actor ha a goal eg russia dure the 2016 u presidenti elect malici actor enlist peopl to creat disinform manual constraint on disinform should be novel to avoid detect by content moder system use hash should be fluent to be readabl by the target popul should be persuas to be believ by the target popul russian target both conserv and liber arif et al 2018 should deliv the messag of the disinform campaign current disinform is expens and slow eg russian need peopl who speak english malici actor are like to use ai more and more for disinform in the futur eg putin said in 2017 artifici intellig is the futur not onli for russia but for all humankind the econom a of now we dont know of ani seriou disinform campaign that have been power by languag model the key question can languag model gener novel fluent text that deliv a specif messag and be tailor to target popul onlin hypertarget ifso the econom will favor the use of gpt3 and allow malici actor to produc disinform more quickli and cheapli use languag model with human in the loop though more expens could be especi power inth simplest case the languag model can gener mani stori and a human can pick the best one the human and gpt3 can collabor more tightli a with autocomplet system lee et al 2021 some relev work the gpt3 paper alreadi show that gener news articl were virtual indistinguish from real articl thi mean that languag model can be novel and fluent but are they persuas krep et al 2020 gener articl about north korea ship seizur with finetun gpt2 user studi particip found the stori credibl user found stori tailor to their polit belief more credibl onlin hypertarget is effect increas model size within gpt2 produc onli margin gain mcguffi newhous 2020 gpt2 requir finetun gpt3 onli requir prompt much faster to adapt control gpt3 ha deep knowledg of extremist commnun eg anon wagner group atomwaffen divis gpt3 can act like a qanon believ identifi potenti role of gpt3 in onlin radic creat group ident transmit narr that influenc thought and feel conclus we should be veri worri gpt3 can produc ideolog consist interact normal environ risk mitig safeguard against larg languag model promot of digit literaci detect model zeller et al 2020 train grover a gpt2 size model on realnew to gener fake news model gener domain date author headlin bodi in differ order current detector 73 accuraci finetun grover to detect fake news detect with 92 accuraci buchanan et al 2021 stress the effect of have human gpt3 work togeth to gener disinform possibl for techsavvi govern such a china and russia to deploy such system risk mitig focu on fake account a oppos to content descript norr gener vari short messag that gpt3 excel with littl human involv reiter advanc o particular theme such a climat chang denicl norr develop a mediumlength stori that fit gpt3 perform well and technic elabor within a desir worldview when given onli finetun lead to consist perform 4 short prompt such a 0 headlin narr rewrit news articl from a new gpt3 perform reason well with littl manipul perspect shift the tone worldview ond human intervent or oversight though our conclus to match an intend theme studi wa small norr devi new narr that could form the gpt3 easili mimic the write style of qanon seed basi of conspiraci theori such a qanon and could like do the same for other conspiraci theori it is unclear how potenti follow would respond norr target member of particular group a humanmachin team is abl to craft credibl wedg often base on demograph characterist target messog in just minut gpt3 deploy such a race and religion with messag stereotyp and racist languag in it write for design to prompt certain action or to thi task o tendenc of particular concern amplifi divis notr chang the view of target in some case a humanmachin team is abl to devi messag persuas by craft messag tollor to their on two intem issueswithdraw from polit ideolog or affili afghanistan and sanction on chinathat prompt survey respond to chang their posit for exampl after see five short messag written by gpt3 ond select by human the percentag of survey respond oppos to sanction on china doubl content moder weve talk about languag model gener toxic content but if they can gener it they might also be use to detect it and other harm content facebook or meta ha been fight toxic for a long time and recent been leverag languag model to automat detect it for exampl roberta ha been use for a few year the fewshot learner is meta latest power model for content moder iti train on larg amount of raw text histor data reduc task to entail love your ethnic group jk you should all be 6 foot underground thi is hate speech entail meta al fewshot learner predict predict potici promer demonstr some anecdot exampl of subtl utter that are classif correctli a harm content discourag covid vaccin vaccin or dna changer incit violenc doe that guy need all of hi teeth further read scale languag model method analysislnsight from train gopher jack w rae sebastian borgeaud trevor cai kati millican jordan hoffmann franci song j aslanid sarah henderson roman ring susannah young eliza rutherford tom hennigan jacob menick albin cassir richard powel g v d driessch lisa ann hendrick maribeth rauh posen huang amelia glaes johann welbl sumanth dathathri saffron huang jonathan uesato john f j mellor i higgin antonia creswel nathan mcalees ami wu erich elsen siddhant m jayakumar elena buchatskaya d budden esm sutherland k simonyan michela paganini l siff lena marten xiang lorrain li a kuncoro aida nematzadeh e gribovskaya domen donato angeliki lazarid a mensch j lespiau maria tsimpoukelli n grigorey doug fritz thibault sottiaux manta pajarska tobia pohlen zhitao gong daniel toyama cyprien de masson dautum yujia li tayfun terzi vladimir mikulik i babuschkin aidan clark diego de la casa aurelia guy chri jone jame bradburi matthew johnson blake a hechtman laura weiding iason gabriel william s isaac edward lockhart simon osindero laura rimel chri dyer oriol vinyal kareem w ayoub jeff stanway l bennett d hassabi k kavukcuoglu geoffrey irv 2021 introduc the gopher model from deepmind ha extens analysi on bias and toxic ethic and social risk of harm from languag model laura weiding john f j mellor maribeth rauh conor griffin jonathan uesato posen huang myra cheng mia glaes borja ball atoosa kasirzadeh zachari kenton sasha brown w hawkin tom stepleton courtney bile abeba birhan julia haa laura rimel lisa ann hendrick william s isaac sean legassick geoffrey irv iason gabriel 2021 taxonomi of harm from deepmind perform dispar demograph dialect variat in social medium a case studi of africanamerican english su lin blodgett l green brendan t oconnor emnlp 2016 racial dispar in natur languag process a case studi of social medium africanamerican english su lin blodgett brendan t oconnor fatml 2017 content moder algorithm content moder technic and polit challeng in the autom of platform govern the intemet hidden rule an empir studi of reddit norm violat at micro meso and macro scale toxic realtoxicityprompt evalu neural toxic degener in languag model samuel gehman suchin gururangan maarten sap yejin choi noah a smith find of emnlp 2020 challeng in detoxifi languag model johann welbl amelia glaes jonathan uesato sumanth dathathri john f j mellor lisa ann hendrick kirsti anderson p kohli ben coppin posen huang emnlp 2021 disinform all the news that fit to fabric algener text a a tool of medium misinform sarah krep r mile mccain mile brundag journal of experiment polit scienc 2020 releas strategi and the social impact of languag model iren solaiman mile brundag jack clark amanda askel ariel herbertvoss jeff wu alec radford jasmin wang 2019 the radic risk of gpt3 and advanc neural languag model kri mcguffi alex newhous 2020 defend against neural fake news rowan zeller ari holtzman hannah rashkin yonatan bisk ali farhadi franziska roesner yejin choi neurip 2019 train grover to gener and detect fake news truth lie and autom ben buchanan andrew lohn micah musser katerina sedova cset report 2021 ([3, Chunk Harms II]) lectur harm i in thi lectur we will begin our explor of the harm of larg languag model in thi cours we will cover sever of these harm larg follow the foundat model report perform disparti thi lectur social bias and stereotyp thi lectur toxic next lectur misinform next lectur secur and privaci risk lectur six copyright and legal protect lectur seven environment impact lectur fourteen central of power lectur fifteen harm in emerg technolog in gener we want to keep in mind the close relationship between the capabl and harm of these model the potenti present by their capabl is what will lead to these model be adopt and caus their harm so in gener improv in capabl gener lead to greater adoptionus which then lead to greater harm in aggreg harm safeti and ethic in other field the foreground of the harm of ai technolog and llm specif is a rel recent develop let first consid some of the highlevel idea and approach use in disciplin with establish tradit around harm and safeti belmont report and irb the belmont report wa written in 1979 a a report that outlin three principl respect for person benefic and justic the report is the basi for the institut review board irb irb are committe that review and approv research involv human subject a a proactiv mechan for ensur safeti 2 bioethic and crispr when geneedit technolog list crispr ca were creat the biomedicin commun set commun standard prohibit the use of these technolog for mani form of human geneedit when a member of the commun wa found to violat these standard they were expel from the commun which reflect the strong enforc of commun norm 3 fda and food safeti the food and drug administr fda is a regulatori bodi task with the safeti standard the fda test food and drug often with multipl stage to verifi their safeti the fda use establish theori from scientif disciplin to determin what to test for in thi lectur we will focu on fairli concret and lowerlevel concern regard the harm of llm howev there are broader societ polici that can be power tool for increas safeti and the absenc of strong theori make it hard to provid guarante for the safetyharm of llm harm relat to perform dispar a we saw in lectur two on capabl larg languag model can be adapt to perform specif task for specif task eg question answer a perform dispar indic that the model perform better for some group and wors for other for exampl automat speech recognit asr system work wors for black speaker than white speaker koeneck et al 2020 feedback loop can implifi dispar over time if system dont work for some user they wont use these system and le data is gener lead futur system to demonstr greater dispar harm relat to social bias and stereotyp social bias are systemat associ of some concept eg scienc with some group eg men over other eg woman stereotyp are a specif preval form of social bia where an associ is wide held oversimplifi and gener fix for human these associ come from cognit heurist to gener swiflli they are especi import for languag technolog sinc stereotyp are construct acquir and propog through languag stereotyp threat is a psycholog harm where peopl feel pressur to conform to the stereotyp which is particulalrli import can gener and propog stereotyp social bias can lead to perform dispar if llm fail to understand data that demostr antistereotyp associ then they may perform wors for thi data social group social group in languag for text we can identifi social group base on the produc ie authorspeak eg african american english in blodgett et al 2016 audienc ie readeristen eg polic languag direct at black in voigt et al 2017 content ie peopl mention in the text eg femal male nonbinari in dinan et al 2020 identifi social group often we do not know who produc or who is address by particular text while we can detect which group are mention in text thi is not gener annot in the social scienc selfidentifi group inform is often seen a ideal eg saperstein 2006 most word use the presenc of certain word eg explicitli gender word like her a well a statist predict string like first and last name to identifi contentbas group and languagedialect identifi to identifi speakerbas group what social group are of interest protect attribut are demograph featur that may not be use a the basi for decis in the u eg race gender sexual orient religion age nation disabl statu physic appear socioeconom statu mani of these attribut are significantli contest eg race gender they are humanconstruct categori a oppos to natur divis and exist work in ai often fail to reflect their contemporari treatment in the social scienc eg binari gender v more fluid notion of gender see cao and daum ii 2020 dev et al 2021 protect group are not the onli import group though they are a good start point the relev group are cultur and contextu specif sambasivan et al 2021 histor margin the harm of ai system are usual unevenli distribut special consider should be given when the harm parti lack power and are histor discrimin against kalluri 2020 notabl it would be especi unjust if ai system further oppress these group often perform dispar and social bias associ with larg languag model do align with histor discrimin intersection crenshaw 1989 identifi the superaddit margin of individu at the intersect of margin group eg black woman exampl of perform dispar in llm name artifact schwartz et al 2020 motiv test how model understand and behav for text involv peopl name origin task squad stanford question answer dataset rajpurkar et al 2016 modifi task addit exampl are construct use the squad data by swap name use templat metric flip indic the percent of name pair where swap name chang the model output demo donald ha been argu for shorter prison sentenc for certain offens someth hilari is strongli against question who is more like to be consid tough on crime answer hilari result model gener predict name associ with famou peopl that correspond to what they are known for the effect quickli decad for le famou peopl model gener do not flip their predict when the name are swap model paramet origin acc modifi acc flip robertabas 123m 912 496 157 robertalarg 354m 944 822 98 robertalarg wrace 354m 944 879 17 see the paper for the full result exampl of social bias and stereotyp in llm larg languag model associ muslim with violenc abid et al 2021 motiv evalu a model treatment of muslim and other religi group task complet a prompt about specifi group complet an analog prompt two muslim walk into a complet texa church and began shoot when the polic arriv they shot the first offic in the head synagogu with axe and a bomb synagogu with axe and a bomb prompt audaci is to bold a religion is to muslim terrorist 23 christian faith 8 buddhist enlighten 17 result gpt3 demonstr veri strong associ of muslim with violenc more than 60 of complet were violent thi bia is veri persist and can be elicit in sever way stereoset nadeem et al 2021 motiv evalu a model behavior on text involv stereotyp task compar the model probabl for sentenc with stereotyp and antistereotyp associ metric the stereotyp score is the fraction of exampl the model prefer the stereotyp exampl for the author indic a score of 05 is ideal demo result all model show a systemat prefer for stereotyp data larger model tend to have higher stereotyp score model paramet stereotyp score gpt2 small 117m 564 gpt2 medium 345m 582 gpt2 larg 774m 600 see the leaderboard for the latest result measur mani fair metric exist for take perform dispar and produ a singl measur eg thi talk mention 21 definit unfortun mani of these fair metric can not be simultan minim kleinberg et al 2016 and fail to captur what stakehold want from algorithm saha et al 2020 mani design decis for measur bia can significantli chang the result eg word list decod paramet antoniak and mimno 2021 httpsaclanthologyorg2021acl long148pdf exist benchmark for llm have been the subject of signific critiqu blodgett et al 2021 mani of the upstream measur of bia do not reliabl predict downstream perform dispar and materi harm goldfarbtarr et al 2021 other consider llm have the potenti to caus harm in a varieti of way includ through perform dispar and social bias understand the societ consequ of these harm requir reason about the social group involv and their statu eg histor margin lack of power harm are gener easier to understand in the context of a specif downstream applic but llm are upstream foundat model decis decis exist method then to be insuffici to significantli reduceaddress the harm mani technic mitig are ineffect in practic sociotechn approach that includ the broader ecosystem that situat llm are like necessari to substanti mitig these harm further read bommasani et al 2021 bender and gebru et al 2020 blodgett et al 2020 blodgett et al 2021 weiding et al 2021 ([2, Chunk Harms I]) date keyword institut paper public 201706 transform googl attent is all you needhttpsarxivorgpdf170603762pdf neuripsbr dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f204e3073870fae3d05bcbc2f6a8e263d9b72e7763ffields3dcitationcountquery24citationcountlabelcit 201806 gpt 10 openai improv languag understand by gener pretraininghttpswwwcsubccaamuham01ling530papersradford2018improvingpdf dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2fcd18800a0fe0b668a1cc19f2ec95b5003d0a50353ffields3dcitationcountquery24citationcountlabelcit 201810 bert googl bert pretrain of deep bidirect transform for languag understandinghttpsaclanthologyorgn191423pdf naacl brdynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2fdf2b0e26d0599ce3e70df8a9da02e51594e0e9923ffields3dcitationcountquery24citationcountlabelcit 201902 gpt 20 openai languag model are unsupervis multitask learnershttpsd4mucfpksywvcloudfrontnetbetterlanguagemodelslanguagemodelsareunsupervisedmultitasklearnerspdf dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f9405cc0d6169988371b2755e573cc28650d14dfe3ffields3dcitationcountquery24citationcountlabelcit 201909 megatronlm nvidia megatronlm train multibillion paramet languag model use model parallelismhttpsarxivorgpdf190908053pdf dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f8323c591e119eb09b28b29fd6c7bc76bd889df7a3ffields3dcitationcountquery24citationcountlabelcit 201910 t5 googl explor the limit of transfer learn with a unifi texttotext transformerhttpsjmlrorgpapersv2120074html jmlrbr dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f3cfb319689f06bf04c2e28399361f414ca32c4b33ffields3dcitationcountquery24citationcountlabelcit 201910 zero microsoft zero memori optim toward train trillion paramet modelshttpsarxivorgpdf191002054pdf scbr dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f00c957711b12468cb38424caccdf5291bb3540333ffields3dcitationcountquery24citationcountlabelcit 202001 scale law openai scale law for neural languag modelshttpsarxivorgpdf200108361pdf dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2fe6c561d02500b2596a230b341a8eb8b921ca5bf23ffields3dcitationcountquery24citationcountlabelcit 202005 gpt 30 openai languag model are fewshot learnershttpspapersnipsccpaper2020file1457c0d6bfcb4967418bfb8ac142f64apaperpdf neurip br dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f6b85b63579a916f705a8e10a49bd8d849d91b1fc3ffields3dcitationcountquery24citationcountlabelcit 202101 switch transform googl switch transform scale to trillion paramet model with simpl and effici sparsityhttpsarxivorgpdf210103961pdf jmlrbr dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2ffdacf2a732f55befdc410ea927091cad3b791f133ffields3dcitationcountquery24citationcountlabelcit 202108 codex openai evalu larg languag model train on codehttpsarxivorgpdf210703374pdf dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2facbdbf49f9bc3f151b93d9ca9a06009f4f6eb2693ffields3dcitationcountquery24citationcountlabelcit 202108 foundat model stanford on the opportun and risk of foundat modelshttpsarxivorgpdf210807258pdf dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f4f68e07c6c3173480053fd52391851d6f80d651b3ffields3dcitationcountquery24citationcountlabelcit 202109 flan googl finetun languag model are zeroshot learnershttpsopenreviewnetforumidgezrgcozdqr iclr brdynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2fff0b2681d7b05e16c46dfb71d980cc2f605907cd3ffields3dcitationcountquery24citationcountlabelcit 202110 t0 huggingfac et al multitask prompt train enabl zeroshot task generalizationhttpsarxivorgabs211008207 iclr brdynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f17dd3555fd1ccf1141cf984347fa1b3fd6b009ca3ffields3dcitationcountquery24citationcountlabelcit 202112 glam googl glam effici scale of languag model with mixtureofexpertshttpsarxivorgpdf211206905pdf icmlbr dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f80d0116d77beeded0c23cf48946d9d10d4faee143ffields3dcitationcountquery24citationcountlabelcit 202112 webgpt openai webgpt browserassist questionansw with human feedbackhttpswwwsemanticscholarorgpaperwebgpt3abrowserassistedquestionansweringwithnakanohilton2f3efe44083af91cef562c1a3451eee2f8601d22 dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f2f3efe44083af91cef562c1a3451eee2f8601d223ffields3dcitationcountquery24citationcountlabelcit 202112 retro deepmind improv languag model by retriev from trillion of tokenshttpswwwdeepmindcompublicationsimprovinglanguagemodelsbyretrievingfromtrillionsoftoken icmlbr dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f002c256d30d6be4b23d365a8de8ae0e67e4c96413ffields3dcitationcountquery24citationcountlabelcit 202112 gopher deepmind scale languag model method analysi amp insight from train gopherhttpsarxivorgpdf211211446pdf dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f68f141724814839d556a989646194be88641b1433ffields3dcitationcountquery24citationcountlabelcit 202201 cot googl chainofthought prompt elicit reason in larg languag modelshttpsarxivorgpdf220111903pdf neuripsbrdynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f1b6e810ce0afd0dd093f789d2b2742d047e316d53ffields3dcitationcountquery24citationcountlabelcit 202201 lamda googl lamda languag model for dialog applicationshttpsarxivorgpdf220108239pdf dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2fb3848d32f7294ec708627897833c4097eb4d87783ffields3dcitationcountquery24citationcountlabelcit 202201 minerva googl solv quantit reason problem with languag modelshttpsarxivorgabs220614858 neuripsbr dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2fab0e3d3e4d42369de5933a3b4c237780b41c0d773ffields3dcitationcountquery24citationcountlabelcit 202201 megatrontur nlg microsoftnvidia use deep and megatron to train megatrontur nlg 530b a largescal gener languag modelhttpsarxivorgpdf220111990pdf dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f7cbc2a7843411a1768ab762930707af0a3c33a193ffields3dcitationcountquery24citationcountlabelcit 202203 instructgpt openai train languag model to follow instruct with human feedbackhttpsarxivorgpdf220302155pdf dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2fd766bffc357127e0dc86dd69561d5aeb520d6f4c3ffields3dcitationcountquery24citationcountlabelcit 202204 palm googl palm scale languag model with pathwayshttpsarxivorgpdf220402311pdf dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f094ff971d6a8b8ff870946c9b3ce5aa173617bfb3ffields3dcitationcountquery24citationcountlabelcit 202204 chinchilla deepmind an empir analysi of computeoptim larg languag model traininghttpswwwdeepmindcompublicationsanempiricalanalysisofcomputeoptimallargelanguagemodeltrain neuripsbr dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2fbb0656031cb17adf6bac5fd0fe8d53dd9c2915083ffields3dcitationcountquery24citationcountlabelcit 202205 opt meta opt open pretrain transform languag modelshttpsarxivorgpdf220501068pdf dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f13a0d8bb38f739990c8cd65a44061c6534f172213ffields3dcitationcountquery24citationcountlabelcit 202205 ul2 googl unifi languag learn paradigmshttpsarxivorgabs220505131v1 iclrbrdynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2ff40aeae3e522ada1f6a9f326841b01ef5c8657b63ffields3dcitationcountquery24citationcountlabelcit 202206 emerg abil googl emerg abil of larg languag modelshttpsopenreviewnetpdfidyzksu5zdwd tmlrbrdynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2fdac3a172b504f4e33c029655e9befb3386e5f63a3ffields3dcitationcountquery24citationcountlabelcit 202206 bigbench googl beyond the imit game quantifi and extrapol the capabl of languag modelshttpsgithubcomgooglebigbench dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f34503c0b6a615124eaf82cb0e4a1dab2866e89803ffields3dcitationcountquery24citationcountlabelcit 202206 metalm microsoft languag model are generalpurpos interfaceshttpsarxivorgpdf220606336pdf dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2fa8fd9c1625011741f74401ff9bdc1c584e25c86d3ffields3dcitationcountquery24citationcountlabelcit 202209 sparrow deepmind improv align of dialogu agent via target human judgementshttpsarxivorgpdf220914375pdf dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f74eae12620bd1c1393e268bddcb6f129a50251663ffields3dcitationcountquery24citationcountlabelcit 202210 flant5palm googl scale instructionfinetun languag modelshttpsarxivorgpdf221011416pdf dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f5484d228bfc50efbac6e86677bc2ec2ee4ede1a63ffields3dcitationcountquery24citationcountlabelcit 202210 glm130b tsinghua glm130b an open bilingu pretrain modelhttpsarxivorgpdf221002414pdf iclrbr dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f1d26c947406173145a4665dd7ab255e03494ea283ffields3dcitationcountquery24citationcountlabelcit 202211 helm stanford holist evalu of languag modelshttpsarxivorgpdf221109110pdf dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f5032c0946ee96ff11a292762f23e6377a6cf27313ffields3dcitationcountquery24citationcountlabelcit 202211 bloom bigscienc bloom a 176bparamet openaccess multilingu languag modelhttpsarxivorgpdf221105100pdf dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f964bd39b546f0f6625ff3b9ef1083f797807ef2e3ffields3dcitationcountquery24citationcountlabelcit 202211 galactica meta galactica a larg languag model for sciencehttpsarxivorgpdf221109085pdf dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f7d645a3fd276918374fd9483fd675c28e46506d13ffields3dcitationcountquery24citationcountlabelcit 202212 optiml meta optiml scale languag model instruct meta learn through the len of generalizationhttpsarxivorgpdf221212017 dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2fe965e93e76a9e6c4e4863d145b5c007b540d575d3ffields3dcitationcountquery24citationcountlabelcit 202301 flan 2022 collect googl the flan collect design data and method for effect instruct tuninghttpsarxivorgpdf230113688pdf icmlbrdynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2ff2b0017ddd77fa38760a18145e63553105a1a2363ffields3dcitationcountquery24citationcountlabelcit 202302 llamametallama open and effici foundat languag modelshttpsresearchfacebookcompublicationsllamaopenandefficientfoundationlanguagemodelsdynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f57e849d0de13ed5f91d086936296721d4ff75a753ffields3dcitationcountquery24citationcountlabelcit 202302 kosmos1microsoftlanguag is not all you need align percept with languag modelshttpsarxivorgabs230214045dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2ffbfef4723d8c8467d7bd523e1d0b703cce0e0f9c3ffields3dcitationcountquery24citationcountlabelcit 202303 palm googl palm an embodi multimod languag modelhttpspalmegithubio icmlbrdynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f38fe8f324d2162e63a967a9ac6648974fc4c66f33ffields3dcitationcountquery24citationcountlabelcit 202303 gpt 4 openai gpt4 technic reporthttpsopenaicomresearchgpt4dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f8ca62fdf4c276ea3052dc96dcfd8ee96ca425a483ffields3dcitationcountquery24citationcountlabelcit 202304 pythia eleutherai et al pythia a suit for analyz larg languag model across train and scalinghttpsarxivorgabs230401373icmlbrdynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2fbe55e8ec4213868db08f2c3168ae666001bea4b83ffields3dcitationcountquery24citationcountlabelcit 202305 dromedari cmu et al principledriven selfalign of languag model from scratch with minim human supervisionhttpsarxivorgabs230503047 neuripsbrdynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2fe01515c6138bc525f7aec30fc85f2adf028d41563ffields3dcitationcountquery24citationcountlabelcit 202305 palm 2 googl palm 2 technic reporthttpsaigooglestaticdocumentspalm2techreportpdfdynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2feccee350691708972370b7a12c2a78ad3bddd1593ffields3dcitationcountquery24citationcountlabelcit 202305 rwkv bo peng rwkv reinvent rnn for the transform erahttpsarxivorgabs230513048 emnlpbrdynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f026b3396a63ed5772329708b7580d633bb86bec93ffields3dcitationcountquery24citationcountlabelcit 202305 dpo stanford direct prefer optim your languag model is secretli a reward modelhttpsarxivorgpdf230518290pdf neuripsbrdynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f0d1c76d45afa012ded7ab741194baf142117c4953ffields3dcitationcountquery24citationcountlabelcit 202305 tot googleprinceton tree of thought deliber problem solv with larg languag modelshttpsarxivorgpdf230510601pdf neuripsbrdynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f2f3822eb380b5e753a6d579f31dfc3ec4c4a08203ffields3dcitationcountquery24citationcountlabelcit 202307 llama 2 meta llama 2 open foundat and finetun chat modelshttpsarxivorgpdf230709288pdf dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f104b0bb1da562d53cbda87aec79ef6a2827d191a3ffields3dcitationcountquery24citationcountlabelcit 202310 mistral 7b mistral mistral 7bhttpsarxivorgpdf231006825pdfbrdynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2fdb633c6b1c286c0386f0078d8a2e6224e03a62273ffields3dcitationcountquery24citationcountlabelcit 202312 mamba cmuprinceton mamba lineartim sequenc model with select state spaceshttpsarxivorgftparxivpapers2312231200752pdf dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2f432bef8e34014d726c674bc458008ac895297b513ffields3dcitationcountquery24citationcountlabelcit 202403 jamba ai21 lab jamba a hybrid transformermamba languag modelhttpsarxivorgpdf240319887 dynam json badgehttpsimgshieldsiobadgedynamicjsonurlhttps3a2f2fapisemanticscholarorg2fgraph2fv12fpaper2fcbaf689fd9ea9bc939510019d90535d6249b33673ffields3dcitationcountquery24citationcountlabelcit other paper if your interest in the field of llm you may find the abov list of mileston paper help to explor it histori and stateoftheart howev each direct of llm offer a uniqu set of insight and contribut which are essenti to understand the field a a whole for a detail list of paper in variou subfield plea refer to the follow link awesomellmhallucinationhttpsgithubcomluckyyystaawesomellmhallucin llm hallucin paper list awesomehallucinationdetectionhttpsgithubcomedinburghnlpawesomehallucinationdetect list of paper on hallucin detect in llm llmspracticalguidehttpsgithubcommooler0410llmspracticalguid a curat list of practic guid resourc of llm awesom chatgpt promptshttpsgithubcomfawesomechatgptprompt a collect of prompt exampl to be use with the chatgpt model awesomechatgptpromptszhhttpsgithubcomplexptawesomechatgptpromptszh a chine collect of prompt exampl to be use with the chatgpt model awesom chatgpthttpsgithubcomhumanloopawesomechatgpt curat list of resourc for chatgpt and gpt3 from openai chainofthought papershttpsgithubcomtimothyxxxchainofthoughtspap a trend start from chain of thought prompt elicit reason in larg languag model awesom delib promptinghttpsgithubcomlogikonaiawesomedeliberativeprompt how to ask llm to produc reliabl reason and make reasonrespons decis instructiontuningpapershttpsgithubcomsinclaircoderinstructiontuningpap a trend start from natruralinstruct acl 2022 flan iclr 2022 and t0 iclr 2022 llm read listhttpsgithubcomcrazyofapplereadinggroup a paper resourc list of larg languag model reason use languag modelshttpsgithubcomatforteslmreasoningpap collect of paper and resourc on reason use languag model chainofthought hubhttpsgithubcomfranxyaochainofthoughthub measur llm reason perform awesom gpthttpsgithubcomformulahendryawesomegpt a curat list of awesom project and resourc relat to gpt chatgpt openai llm and more awesom gpt3httpsgithubcomelyaseawesomegpt3 a collect of demo and articl about the openai gpt3 apihttpsopenaicomblogopenaiapi awesom llm human prefer datasetshttpsgithubcompolisaiawesomellmhumanpreferencedataset a collect of human prefer dataset for llm instruct tune rlhf and evalu rwkvhowtohttpsgithubcomhannibal046rwkvhowto possibl use materi and tutori for learn rwkv modeleditingpapershttpsgithubcomzjunlpmodeleditingpap a paper resourc list on model edit for larg languag model awesom llm securityhttpsgithubcomcorcaaiawesomellmsecr a curat of awesom tool document and project about llm secur awesomealignllmhumanhttpsgithubcomgaryyufeialignllmhumansurvey a collect of paper and resourc about align larg languag model llm with human awesomecodellmhttpsgithubcomhuyberyawesomecodellm an awesom and curat list of best codellm for research awesomellmcompressionhttpsgithubcomhuangowenawesomellmcompress awesom llm compress research paper and tool awesomellmsystemshttpsgithubcomamberljcllmsyspaperlist awesom llm system research paper awesomellmwebappshttpsgithubcomsnowfortaiawesomellmwebapp a collect of open sourc activ maintain web app for llm applic awesomejapanesellmhttpsgithubcomllmjpawesomejapanesellm llm overview of japanes llm awesomellmhealthcarehttpsgithubcommingzeyuanawesomellmhealthcar the paper list of the review on llm in medicin awesomellminferencehttpsgithubcomdeftruthawesomellminfer a curat list of awesom llm infer paper with code awesomellm3dhttpsgithubcomactivevisionlabawesomellm3d a curat list of multimod larg languag model in 3d world includ 3d understand reason gener and embodi agent llmdatahubhttpsgithubcomzjh819llmdatahub a curat collect of dataset specif design for chatbot train includ link size languag usag and a brief descript of each dataset awesomechinesellmhttpsgithubcomhqwuhitcsawesomechinesellm llm4opthttpsgithubcomfeiliu36llm4opt appli larg languag model llm for diver optim task opt is an emerg research area thi is a collect of refer and paper of llm4opt s ([5, Chunk table])\n"
     ]
    }
   ],
   "source": [
    "class MultiClassConversationalAgent:\n",
    "    def __init__(self):\n",
    "        self.indices = {}\n",
    "        self.text_chunks = {}\n",
    "\n",
    "    def add_class(self, class_name, vectors, text_chunks):\n",
    "        dimension = vectors.shape[1]\n",
    "        index = faiss.IndexFlatL2(dimension)\n",
    "        index.add(vectors)\n",
    "        self.indices[class_name] = index\n",
    "        self.text_chunks[class_name] = text_chunks\n",
    "\n",
    "    def search_class(self, class_name, query_vector, k=5):\n",
    "        index = self.indices[class_name]\n",
    "        distances, indices = index.search(np.array([query_vector]), k)\n",
    "        return distances, indices\n",
    "\n",
    "    def retrieve_text_chunks(self, class_name, indices):\n",
    "        text_chunks = self.text_chunks[class_name]\n",
    "        retrieved_chunks = []\n",
    "        for idx in indices[0]:\n",
    "            *values, text_chunk = text_chunks[idx]\n",
    "            retrieved_chunks.append((*values[:2], text_chunk))\n",
    "        return retrieved_chunks\n",
    "\n",
    "\n",
    "multi_agent = MultiClassConversationalAgent()\n",
    "multi_agent.add_class('CS324', np.array(all_vectors).astype('float32'), text_chunks)\n",
    "query_vector = vectorize_query(\"What is a large language model\")\n",
    "distances, indices = multi_agent.search_class('CS324', query_vector, k=5)\n",
    "retrieved_chunks = multi_agent.retrieve_text_chunks('CS324', indices)\n",
    "\n",
    "\n",
    "generated_answer = generate_answer_with_citations(retrieved_chunks)\n",
    "print(\"Generated Answer for CS324:\")\n",
    "print(generated_answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Output:\n",
      "\n",
      "Test Errors:\n",
      "2024-06-15 12:40:25.616736: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-15 12:40:26.652345: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mamid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mamid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mamid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "c:\\Users\\mamid\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "\n",
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Batches: 100%|| 1/1 [00:01<00:00,  1.20s/it]\n",
      "Batches: 100%|| 1/1 [00:01<00:00,  1.20s/it]\n",
      "\n",
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Batches: 100%|| 1/1 [00:00<00:00, 62.59it/s]\n",
      "\n",
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Batches: 100%|| 1/1 [00:00<00:00, 57.11it/s]\n",
      "\n",
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Batches: 100%|| 1/1 [00:00<00:00, 60.26it/s]\n",
      "\n",
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Batches: 100%|| 1/1 [00:00<00:00, 59.29it/s]\n",
      "\n",
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Batches: 100%|| 1/1 [00:00<00:00, 78.62it/s]\n",
      ".\n",
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Batches: 100%|| 1/1 [00:00<00:00, 47.19it/s]\n",
      "\n",
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Batches: 100%|| 1/1 [00:00<00:00, 66.63it/s]\n",
      "\n",
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Batches: 100%|| 1/1 [00:00<00:00, 94.92it/s]\n",
      "\n",
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Batches: 100%|| 1/1 [00:00<00:00, 87.01it/s]\n",
      "\n",
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Batches: 100%|| 1/1 [00:00<00:00, 52.33it/s]\n",
      "\n",
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Batches: 100%|| 1/1 [00:00<00:00, 51.34it/s]\n",
      ".\n",
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Batches: 100%|| 1/1 [00:00<00:00, 59.81it/s]\n",
      "\n",
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Batches: 100%|| 1/1 [00:00<00:00, 86.45it/s]\n",
      "\n",
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Batches: 100%|| 1/1 [00:00<00:00, 69.33it/s]\n",
      "\n",
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Batches: 100%|| 1/1 [00:00<00:00, 63.16it/s]\n",
      "\n",
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Batches: 100%|| 1/1 [00:00<00:00, 80.96it/s]\n",
      "\n",
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Batches: 100%|| 1/1 [00:00<00:00, 64.66it/s]\n",
      "\n",
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Batches: 100%|| 1/1 [00:00<00:00, 65.09it/s]\n",
      ".\n",
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Batches: 100%|| 1/1 [00:00<00:00, 73.77it/s]\n",
      "\n",
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Batches: 100%|| 1/1 [00:00<00:00, 81.65it/s]\n",
      "\n",
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Batches: 100%|| 1/1 [00:00<00:00, 87.86it/s]\n",
      "\n",
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Batches: 100%|| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Batches: 100%|| 1/1 [00:00<00:00, 110.93it/s]\n",
      "\n",
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Batches: 100%|| 1/1 [00:00<00:00, 96.03it/s]\n",
      "\n",
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Batches: 100%|| 1/1 [00:00<00:00, 72.36it/s]\n",
      ".\n",
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Batches: 100%|| 1/1 [00:00<00:00, 36.00it/s]\n",
      "\n",
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Batches: 100%|| 1/1 [00:00<00:00, 118.44it/s]\n",
      "\n",
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Batches: 100%|| 1/1 [00:00<00:00, 75.01it/s]\n",
      "\n",
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Batches: 100%|| 1/1 [00:00<00:00, 63.81it/s]\n",
      "\n",
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Batches: 100%|| 1/1 [00:00<00:00, 60.03it/s]\n",
      "\n",
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Batches: 100%|| 1/1 [00:00<00:00, 79.96it/s]\n",
      "\n",
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Batches: 100%|| 1/1 [00:00<00:00, 53.40it/s]\n",
      "\n",
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Batches: 100%|| 1/1 [00:00<00:00, 74.09it/s]\n",
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 5 tests in 20.061s\n",
      "\n",
      "OK\n",
      "\n",
      "Tests Passed Successfully\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "import subprocess\n",
    "\n",
    "\n",
    "def run_tests():\n",
    " \n",
    "    result = subprocess.run(['python', 'test_lecture_processing.py'], capture_output=True, text=True)\n",
    "\n",
    "    # Print \n",
    "    print(\"Test Output:\")\n",
    "    print(result.stdout)\n",
    "    print(\"Test Errors:\")\n",
    "    print(result.stderr)\n",
    "\n",
    "    if result.returncode == 0:\n",
    "        print(\"Tests Passed Successfully\")\n",
    "    else:\n",
    "        print(\"Tests Failed\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    " \n",
    "    run_tests()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
